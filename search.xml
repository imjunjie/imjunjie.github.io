<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[（全平台）中文-Sublime Text 3207激活]]></title>
    <url>%2Fquan-ping-tai-zhong-wen-sublime-text-3207-ji-huo.html</url>
    <content type="text"><![CDATA[有朋友反馈说安装不了Package Control研究了一下，之前Host的列表把ST3的全都屏蔽了仔细想了想有点智障，只用屏蔽license server就行已更新，新的host就没问题了。感谢@eui620的提醒 我没在win下开发的习惯…抱歉没发现WinHex不能保存超过200K的文件用这个在线编辑器吧，啥平台都行。https://hexed.it 1. 下载软件官网: 点我下载网盘地址见底部 2. 安装软件这个我就不多BB了。安装完请勿打开SublimeText3。（若已打开关了就是） 破解”&gt;3. 破解3207版本基本杜绝了共享license key的方法所以我们要修改验证license时的trigger因官方采用revoke illegal licenses的方式，即使当时显示激活成功，联网验证时便会凉凉。 所以我们还要采用hosts屏蔽法复制以下地址直接粘贴到相应系统的hosts文件内1234127.0.0.1 license.sublimehq.com127.0.0.1 www.sublimetext.com50.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io 3.1 修改trigger3.1.1 Win 利用WinHex(网盘会有)或其他HexEditor打开软件根目录下的sublime_text.exe 搜索16进制 97 94 0D 00 改为 00 00 00 00 保存 3.1.2 Mac 拷出/Applications/Sublime Text.app/Contents/MacOS/Sublime Text 其实就是 应用程序 文件夹下找到SublimeText应用，然后右键-&gt;显示包内容，然后打开/Contents/MacOS/ 然后找到 Sublime Text 这个文件 拷出来 利用0xED(网盘会有)或者其他HexEditor打开它 搜索16进制 97 94 0D 00 改为 00 00 00 00 如果实在不会修改网盘里有修改好的现成的 保存 打开终端，切换到当前目录 然后键入chmod 755 Sublime Text 替换掉/Applications/Sublime Text.app/Contents/MacOS/Sublime Text 完事儿 3.1.3 Linux基本同Mac操作 找个16进制编辑器打开软件根目录下的Sublime Text 搜索16进制 97 94 0D 00 改为 00 00 00 00 保存 打开终端，切换到当前目录 然后键入chmod 755 Sublime Text 完事儿 3.2 修改host3.2.1 WinWindows的hosts文件在：系统盘:/windows/system32/drivers/etc/hosts12Tips: Win下的权限获取可能有点复杂，不如先拷到桌面，编辑完替换回去。在最后一行插入 1234127.0.0.1 license.sublimehq.com127.0.0.1 www.sublimetext.com50.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io 3.2.2 Mac 打开Terminal(终端) 输入 sudo nano /Private/etc/hosts 回车 输入密码后回车 在最后一行插入 1234127.0.0.1 license.sublimehq.com127.0.0.1 www.sublimetext.com50.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io 按下Control + X，输入Y确定修改，确认保存路径后敲击回车 3.2.3 Linux同Mac 4. 激活打开Sublime Text 3选择Help -&gt; Enter License输入12345678910111213----- BEGIN LICENSE -----TwitterInc200 User LicenseEA7E-8900071D77F72E 390CDD93 4DCBA022 FAF6079061AA12C0 A37081C5 D0316412 4584D13694D7F7D4 95BC8C1C 527DA828 560BB037D1EDDD8C AE7B379F 50C9D69D B35179EF2FE898C4 8E4277A8 555CE714 E1FB0E43D5D52613 C3D12E98 BC49967F 7652EED29D2D2E61 67610860 6D338B72 5CF95C69E36B85CC 84991F19 7575D828 470A92AB------ END LICENSE ------ 选择Use license 5.大功告成 6.中文化不知道该不该写，好多人觉得哇人家破解版带个汉化猴赛雷其实在Package Control就有 Rexdf 翻译的插件 按下command + shift + p(win或linux为 ctrl + shift + p) 输入 install 选择 Install Package Control 等待提示安装完成 完成后按下command + shift + p(win或linux为 ctrl + shift + p) 输入 install 选择 Package Control: Install Package 输入 localization 选择 ChineseLocalizitions 等待提示安装完成 大功告成，如需切换语言选择help -&gt; Languages 7.附件附件什么附件？？我这破等级还想发附件？？？卑微，蚂蚁花卑，葡萄美酒夜光卑…..百度网盘 密码:lksz 问答（发表于 2019-4-16 08:08，by hjner）我测试，说注册码不对…..XP 下测试的。 第二次重装，同样出现 ：plugin_host has exited unexpectedly,plugin functionality won’t be available until Sublime Text has been restarted 不关闭程序，先设置好HOST文件，屏蔽服务器，然后，直接 输入注册码，反而通过！之前重启程序在输入注册码，则无效。 谢谢，成功！ 安装中文时候，出现installing package control,出现提示，访问https://packagecontrol.io/installation 手动安装， 但是网站进不去…只有以后再试试了。 (发表于 2019-5-7 16:36 by 花了19元)127.0.0.1 license.sublimehq.com127.0.0.1 www.sublimetext.com50.116.34.243 sublime.wbond.net50.116.34.243 packagecontrol.io 激活成功，但是这样修改hosts，插件安装不了。 解决办法(https://www.jianshu.com/p/23b823d6e786)：点击 Preferences &gt; Package Settings &gt; Package Control &gt; Settings - User添加配置“channels”: [“https://raw.githubusercontent.com/HBLong/channel_v3_daily/master/channe]]></content>
  </entry>
  <entry>
    <title><![CDATA[Win10如何自定义右键菜单-修改注册表（图文）]]></title>
    <url>%2Fwin10-ru-he-zi-ding-yi-you-jian-cai-dan-xiu-gai-zhu-ce-biao-tu-wen.html</url>
    <content type="text"><![CDATA[理论介绍&emsp;&emsp;我研究这个是因为发现右键菜单在安装了一下软件后，越来越臃肿，有用的没用的菜单项都被塞进去了，于是自己动手给菜单瘦个身。 &emsp;&emsp;这里首先警告一句：下面操作全部涉及到修改注册表，看见不认识，不确定的注册表项，别手欠看见空项或者自以为无用的注册表项，就瞎乱删。最好是有一定操作注册表的基础在跟着本文操作，至少要知道怎么备份和恢复注册表。手欠的孩子都请自己准备好恢复或重装系统，本文的经过作者本人亲自实践无误，但不保证文中描述完全正确或适用于所有版本的win10操作系统。如果在按照本文说明操作时，发生了系统崩溃，死机，或其他任何可修复/不可修复的系统问题，你可以顺着网线来打我啊，然而我也救不了你。 &emsp;&emsp;首先，所有的右键菜单项，几乎都可以在注册表中设置。按 Win + R 打开“ 运行…”窗口，输入 regedit ，按回车键打开。注意：注册表编辑器是需要管理员权限的。 打开注册表，根项展开有5个子项，如上图所示。右键菜单的项目都包含在第一子项 HKEY_CALSS_ROOT 中。展开该项，第一个子项一般是 * ，这个统配符表示一切后缀的文件都通用。也就是说，这个子项中的一切右键菜单项，没有特别说明，会出现每一个文件的右键菜单中。 展开这一子项，在其内部，所有的右键菜单分为两部分存储（我也懒得去搞清楚这两块区域有什么不同），见下图： 用红线圈起来的两个注册表键，就是放置了右键菜单的地方，看看有哪些是自己安装的软件带来的，看名字挑着没用的就能删除了。这里特别提醒一句，看见键名称是一串序列号的，请仔细核对后，确认不是系统项再删除。用这种长串数字当名字的键，如果里面空空如也，那很有可能是系统项。 然后是文件夹，文件夹分为两类菜单，一类是鼠标指向一个文件夹图标时，点击右键出来的菜单；第二类菜单时鼠标在已经打开的文件夹窗口的空白处，点击右键弹出的菜单。如下图所示，第一类菜单的注册表项直接在 Directory 下，shell和shellex\ContextMenuHandlers 里面；第二类菜单则在子项 Background 里面。 哦，对了，还有比较特殊的桌面菜单。在桌面空白处点击右键，弹出的菜单在 DesktopBackground 项里面： 是的，细心的人应该已经发现了，这里的菜单项不全。是的，不全，然而我也不知道其他的在哪里，懒得找…… 然后还有一些，比如：驱动器（就是C盘、光驱，之类那些，带着卷标的），在 Drive 项里面；文件夹还有一些在 Folder 项里面；字体文件的在 fontfile 项里面；等等…… 英文好的同学可以自行发挥了。 上面讲的是如何找到一些项，然后就能删除里面多余的菜单项。下面将一些添加项的方法： 以python文件为例（*.py），python如见有两个大分支：2.x系列和3.x系列。那么有时候我们的机器上会同时安装这两个python的运行环境，这时候想要快速的用python解释器打开某个*.py文件，要么就是命令行，要么就是频繁更改打开方式，要么就是来回挪动环境变量的前后顺序……好吧，我不废话了，下面开始动手添加右键菜单。 首先，还是找到包含python脚本文件的右键菜单项的注册表键，完整的路径是 Computer\HKEY_CLASSES_ROOT\pysFile ，如下图。这里可以看到，有3个子项。一眼可以看到右键菜单的藏身之处： 一般安装python时，附带的菜单项倒在 Shell 子键里面，展开，把一串什么 runwithidle 之类的统统干掉，然后我们来加入自己的项。右键点击 Shell ，然后选择 新建 ，然后选择 键： 简单点的话，不做附加设置，这个键的名字就会是右键菜单项的显示名字，如下图所示：之后，如果更改这个键的默认值，就会更改菜单的显示名字：只有一个键，是不能让这个菜单项真正生效的，这时如果点击这个菜单项，就会收到系统发出的错误警告。下面来添加点击这个菜单项所触发的命令：在新建的键里面（图里面的 MieHaHa键），再新建一个键，命名为 command，一般大小写都行，但是我还是建议全小写吧。然后更改这个键的默认值，双击(Default)（中文操作系统这里应该是默认），会弹出修改框，把值修改为你的python.exe所在完整路径+参数就可以了，比如我的python36安装在D:\Environment\Python36\python.exe, 那么我这里就要输入 &quot;D:\Environment\Python36\python.exe&quot; &quot;%1&quot; %*。这里简单解释一下，这里的值，就相当与是命令行里敲的命令。因为是点击文件弹出的菜单， %1 就是被点击的py文件的完整路径。 有了这个菜单项，就能使用这一项直接用python运行脚本文件了。然而，这也太简陋了，看好多程序都用dll文件，把自己的菜单项折叠成了一个子菜单组，简洁又方便。在WIN10里，其实不用dll，只用注册表，也能自己制作一个折叠的子菜单组，比如上图（图8）的 Run With 项就是我自己写的一个菜单组。下图直接上键的树： 除了最内层两个 command 和 最外层的 runwith 其余的键都没有值。 runwith 里需要新建两个 字符串的值：一个命名为 MUIVerb，值为 &amp;Run With，也就是这个菜单组的名称，注意要以 &amp; 开头，这个字符不会被显示；第二个值，命名为Subcommands，没有值。如下图： 将 Sublime Text 添加到系统右键菜单栏的方法Sublime Text 是一个代码编辑器（Sublime Text 2是收费软件，但可以无限期试用），也是HTML和散文先进的文本编辑器。Sublime Text是由程序员Jon Skinner于2008年1月份所开发出来，它最初被设计为一个具有丰富扩展功能的Vim。Sublime Text具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。（摘自百度百科） 咳咳，废话少说，切入正题：这么好的编译器在打开需要编译的文件时却存在一个非常让人头疼的问题：不能右键菜单选择使用 Sublime text 打开！！！ 下面为大家介绍一种通过修改系统注册表的方法将 Sublime text 添加到右键菜单中： 首先使用 Win + R 打开运行，输入 regedit 按下回车键进入注册表；运行后进入注册表界面： 依次展开 HKEY_CLASSES_ROOT -&gt; * -&gt; shell，右键 shell 新建一个项并命名为：Open With Sublime Text 如图： 双击选中新建项 Open With Sublime Text ，在右边展示栏空白处右键新建字符串值，数值名称填 lcon ，数值数据填 E:\Sublime Text3\sublime_text.exe,0 （注意：路径请改成你安装Sublime Text 的路径喔） 右键新建的 Open With Sublime Text 新建一个项，命名为 Command （注意：请必须命名为 Command 喔），并双击选中 Command ，在右边的展示栏中将默认项的数值数据修改为：E:\Sublime Text 3\sublime_text.exe “%1” （注意：1、将我的路径改成你安装 Sublime Text 的路径，2、”必须要写喔）进行到这里，右键菜单的 Open With Sublime Text 选项就实现啦！！！ sublime text 添加到鼠标右键功能Sublime Text是一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件，不仅具有华丽的界面，还支持插件扩展机制，用她来写代码，绝对是一种享受。如何把sublime text添加到鼠标右键，以方便我们使用呢？ 方法与步骤1.在Windows系统中，下载并安装sublime text3 软件，（可以到sublime text3 的官方网站下载），如下图，可以按不同的系统选择下载安装包。 2.下载完成后，双击安装包文件进行安装，安装比较简单，按照提示点击“下一步”，最后完成安装，如下图所示。 3.sublime text 添加到鼠标右键功能：把以下内容复制并保存到文件，重命名为：sublime_addright.reg，然后双击就可以了。（注意：需要把下面代码中的Sublime的安装目录（标粗部分），替换成自已实际的Sublime安装目录）123456789101112131415161718192021Windows Registry Editor Version 5.00[HKEY_CLASSES_ROOT\*\shell\SublimeText3]@="用 SublimeText3 打开""Icon"="C:\\Program Files\\Sublime Text 3\\sublime_text.exe,0"[HKEY_CLASSES_ROOT\*\shell\SublimeText3\command]@="C:\\Program Files\\Sublime Text 3\\sublime_text.exe %1"[HKEY_CLASSES_ROOT\Directory\shell\SublimeText3]@="用 SublimeText3 打开""Icon"="C:\\Program Files\\Sublime Text 3\\sublime_text.exe,0"[HKEY_CLASSES_ROOT\Directory\shell\SublimeText3\command]@="C:\\Program Files\\Sublime Text 3\\sublime_text.exe %1" 其中，@=”用 SublimeText3 打开” 引号中的内容为出现在鼠标右键菜单中的文字内容。4.双击文件sublime_addright.reg 完成后，鼠标选中要编辑的文件，点击鼠标右键，弹出菜单，其中就会出现刚才添加的“用 SublimeText3 打开”选项，如下图所示。 将Sublime Text 添加到鼠标右键的方法步骤: win+R 打开运行，并输入regedit。 在左侧依次打开HKEY_CLASSES_ROOT*\shell 在shell下新建“Sublime Text”项，在右侧窗口的“默认”键值栏内输入“用Sublime Text打开”。项的名称和键值可以任意，最好是和程序关联起来。其中键值将显示在右键菜单中。 在“用Sublime Text打开”下再新建Command项，在右侧窗口的“默认”键值栏内输入Sublime Text程序所在的路径,在路径后添加 %1。%1表示要打开的文件参数。 关闭注册表窗口，立即生效。（如图） 注意事项： 1. 第四步时的Command无法自定义。必须输入Command才可以。 2. 输入程序路径时注意为以下格式：例. d:\sub\sub.exe %1 &emsp;&emsp;万事不要太依赖别人，自己动手才能丰衣足食。这个鼠标右键也可以使用第三方程序来调用添加，而且这个改注册表不止方便，还可以举一反三，做的更多。&emsp;&emsp;附上下载链接这个中文不会乱码: 链接：https://pan.baidu.com/s/1jH9KD8a 密码：p6du 参考文献 Win10如何自定义右键菜单-修改注册表（图文 将 Sublime Text 添加到系统右键菜单栏的方法 sublime text 添加到鼠标右键功能 将Sublime Text 添加到鼠标右键的方法 win10右键菜单修改，任意位置打开cmd命令行程序]]></content>
  </entry>
  <entry>
    <title><![CDATA[Semi-supervised learning with GANs]]></title>
    <url>%2Fsemi-supervised-learning-with-gans.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;In this post I will cover a partial re-implementation of a recent paper on manifold regularization (Lecouat et al., 2018) for semi-supervised learning with Generative Adversarial Networks (Goodfellow et al., 2014). I will attempt to re-implement their main contribution, rather than getting all the hyperparameter details just right. Also, for the sake of demonstration, time constraints and simplicity, I will consider the MNIST dataset rather than the CIFAR10 or SVHN datasets as done in the paper. Ultimately, this post aims at bridging the gap between the theory and implementation for GANs in the semi-supervised learning setting. The code that comes with this post can be found here. Generative Adversarial Networks&emsp;&emsp;Let’s quickly go over Generative Adversarial Networks (GAN). In terms of the current pace within the AI/ML community, they have been around for a while (just about 4 years), so you might already be familiar with them. The ‘vanilla’ GAN procedure is to train a generator to generate images that are realistic and capable of fooling a discriminator. The generator generates the images by means of a deep neural network that takes in a noise vector z.&emsp;&emsp;The discriminator (which is a deep neural network as well) is fed with the generated images, but also with some real data. Its job is to say whether each image is either real (coming from the dataset) or fake (coming from the generator), which in terms of implementation comes down to binary classification. The image below summarizes the vanilla GAN setup. Semi-supervised learning&emsp;&emsp;Semi-supervised learning problems concern a mix of labeled and unlabeled data. Leveraging the information in both the labeled and unlabeled data to eventually improve the performance on unseen labeled data is an interesting and more challenging problem than merely doing supervised learning on a large labeled dataset. In this case we might be limited to having only about 200 samples per class. So what should we do when only a small portion of the data is labeled?&emsp;&emsp;Note that adversarial training of vanilla GANs doesn’t require labeled data. At the same time, the deep neural network of the discriminator is able to learn powerful and robust abstractions of images by gradually becoming better at discriminating fake from real. Whatever it’s learning about unlabeled images will presumably also yield useful feature descriptors of labeled images. So how do we use the discriminator for both labeled and unlabeled data? Well, the discriminator is not necessarily limited to just telling fake from real. We could decide to train it to also classify the real data.&emsp;&emsp;A GAN with a classifying discriminator would be able to exploit both the unlabeled as well as the labeled data. The unlabeled data will be used to merely tell fake from real. The labeled data would be used to optimize the classification performance. In practice, this just means that the discriminator has a softmax output distribution for which we minimize the cross-entropy. Indeed, part of the training procedure is just doing supervised learning. The other part is about adversarial training. The image below summarizes the semi-supervised learning setup with a GAN. The implementation&emsp;&emsp;Let’s just head over to the implementation, since that might be the best way of understanding what’s happening. The snippet below prepares the data. It doesn’t really contain anything sophisticated. Basically, we take 400 samples per class and concatenate the resulting arrays as being our actual supervised subset. The unlabeled dataset consists of all train data (it also includes the labeled data, since we might as well use it anyway). As is customary for training GANs now, the output of the generator uses a hyperbolic tangent function, meaning its output is between -1 and +1. Therefore, we rescale the data to be in that range as well. Then, we create TensorFlow iterators so that we can efficiently go through the data later without having to struggle with feed dicts later on.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def prepare_input_pipeline(flags_obj): (train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data( "/home/jos/datasets/mnist/mnist.npz") def reshape_and_scale(x, img_shape=(-1, 28, 28, 1)): return x.reshape(img_shape).astype(np.float32) / 255. * 2.0 - 1.0 # Reshape data and rescale to [-1, 1] train_x = reshape_and_scale(train_x) test_x = reshape_and_scale(test_x) # Shuffle train data train_x_unlabeled, train_y_unlabeled = shuffle(train_x, train_y) # Select subset as supervised train_x_labeled, train_y_labeled = [], [] for i in range(flags_obj.num_classes): train_x_labeled.append( train_x_unlabeled[train_y_unlabeled == i][:flags_obj.num_labeled_examples]) train_y_labeled.append( train_y_unlabeled[train_y_unlabeled == i][:flags_obj.num_labeled_examples]) train_x_labeled = np.concatenate(train_x_labeled) train_y_labeled = np.concatenate(train_y_labeled) with tf.name_scope("InputPipeline"): def train_pipeline(data, shuffle_buffer_size): return tf.data.Dataset.from_tensor_slices(data)\ .cache()\ .shuffle(buffer_size=shuffle_buffer_size)\ .batch(flags_obj.batch_size)\ .repeat()\ .make_one_shot_iterator() # Setup pipeline for labeled data train_ds_lab = train_pipeline( (train_x_labeled, train_y_labeled.astype(np.int64)), flags_obj.num_labeled_examples * flags_obj.num_classes) images_lab, labels_lab = train_ds_lab.get_next() # Setup pipeline for unlabeled data train_ds_unl = train_pipeline( (train_x_unlabeled, train_y_unlabeled.astype(np.int64)), len(train_x_labeled)) images_unl, labels_unl = train_ds_unl.get_next() # Setup another pipeline that also uses the unlabeled data, so that we use a different # batch for computing the discriminator loss and the generator loss train_x_unlabeled, train_y_unlabeled = shuffle(train_x_unlabeled, train_y_unlabeled) train_ds_unl2 = train_pipeline( (train_x_unlabeled, train_y_unlabeled.astype(np.int64)), len(train_x_labeled)) images_unl2, labels_unl2 = train_ds_unl2.get_next() # Setup pipeline for test data test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y.astype(np.int64)))\ .cache()\ .batch(flags_obj.batch_size)\ .repeat()\ .make_one_shot_iterator() images_test, labels_test = test_ds.get_next() return (images_lab, labels_lab), (images_unl, labels_unl), (images_unl2, labels_unl2), \ (images_test, labels_test) &emsp;&emsp;Next up is to define the discriminator network. I have deviated quite a bit from the architecture in the paper. I’m going to play safe here and just use Keras layers to construct the model. Actually, this enables us to very conveniently reuse all weights for different input tensors, which will prove to be useful later on. In short, the discriminator’s architecture uses 3 convolutions with 5x5 kernels and strides of 2x2, 2x2 and 1x1 respectively. Each convolution is followed by a leaky ReLU activation and a dropout layer with a dropout rate of 0.3. The flattened output of this stack of convolutions will be used as the feature layer.&emsp;&emsp;The feature layer can be used for a feature matching loss (rather than a sigmoid cross-entropy loss as in vanilla GANs), which has proven to yield a more reliable training process. The part of the network up to this feature layer is defined in _define_tail in the snippet below. The _define_head method defines the rest of the network. The ‘head’ of the network introduces only one additional fully connected layer with 10 outputs, that correspond to the logits of the class labels. Other than that, there are some methods to make the interface of a Discriminator instance behave similar to that of a tf.keras.models.Sequential instance.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Discriminator: def __init__(self): """The discriminator network. Split up in a 'tail' and 'head' network, so that we can easily get the """ self.tail = self._define_tail() self.head = self._define_head() def _define_tail(self, name="Discriminator"): """Defines the network until the intermediate layer that can be used for feature-matching loss.""" feature_model = models.Sequential(name=name) def conv2d_dropout(filters, strides, index=0): # Adds a convolution followed by a Dropout layer suffix = str(index) feature_model.add(layers.Conv2D( filters=filters, strides=strides, name="Conv&#123;&#125;".format(suffix), padding='same', kernel_size=5, activation=tf.nn.leaky_relu)) feature_model.add(layers.Dropout(name="Dropout&#123;&#125;".format(suffix), rate=0.3)) # Three blocks of convs and dropouts. They all have 5x5 kernels, leaky ReLU and 0.3 # dropout rate. conv2d_dropout(filters=32, strides=2, index=0) conv2d_dropout(filters=64, strides=2, index=1) conv2d_dropout(filters=64, strides=1, index=2) # Flatten it and build logits layer feature_model.add(layers.Flatten(name="Flatten")) return feature_model def _define_head(self): # Defines the remaining layers after the 'tail' head_model = models.Sequential(name="DiscriminatorHead") head_model.add(layers.Dense(units=10, activation=None, name="Logits")) return head_model @property def trainable_variables(self): # Return both tail's parameters a well as those of the head return self.tail.trainable_variables + self.head.trainable_variables def __call__(self, x, *args, **kwargs): # By adding this, the code below can treat a Discriminator instance as a # tf.keras.models.Sequential instance features = self.tail(x, *args, **kwargs) return self.head(features, *args, **kwargs), features &emsp;&emsp;The generator’s architecture also uses 5x5 kernels. Many implementations of DCGAN-like architectures use transposed convolutions (sometimes wrongfully referred to as ‘deconvolutions’). I have decided to give the upsampling-convolution alternative a try. This should alleviate the issue of the checkerboard pattern that sometimes appears in generated images. Other than that, there are ReLU nonlinearities, and a first layer to go from the 100-dimensional noise to a (rather awkwardly shaped) 7x7x64 spatial representation.123456789101112131415161718192021def define_generator(): model = models.Sequential(name="Generator") def conv2d_block(filters, upsample=True, activation=tf.nn.relu, index=0): if upsample: model.add(layers.UpSampling2D(name="UpSampling" + str(index), size=(2, 2))) model.add(layers.Conv2D( filters=filters, kernel_size=5, padding='same', name="Conv2D" + str(index), activation=activation)) # From flat noise to spatial model.add(layers.Dense(7 * 7 * 64, activation=tf.nn.relu, name="NoiseToSpatial")) model.add(layers.Reshape((7, 7, 64))) # Four blocks of convolutions, 2 that upsample and convolve, and 2 more that # just convolve conv2d_block(filters=128, upsample=True, index=0) conv2d_block(filters=64, upsample=True, index=1) conv2d_block(filters=64, upsample=False, index=2) conv2d_block(filters=1, upsample=False, activation=tf.nn.tanh, index=3) return model &emsp;&emsp;I have tried to make this model work with what TensorFlow’s Keras layers have to offer so that the code would be easy to digest (and to implement of course). This also means that I have deviated from the architectures in the paper (e.g. I’m not using weight normalization). Because of this experimental approach, I have also experienced just how sensitive the training setup is to small variations in network architectures and parameters. There are plenty of neat GAN ‘hacks’ listed here which I definitely found insightful. Putting it together&emsp;&emsp;Let’s do the forward computations now so that we see how all of the above comes together. This consists of setting up the input pipeline, noise vector, generator and discriminator. The snippet below does all of this. Note that when define_generator returns the Sequential instance, we can just use it as a functor to obtain the output of it for the noise tensor given by z.&emsp;&emsp;The discriminator will do a lot more. It will take (i) the ‘fake’ images coming from the generator, (ii) a batch of unlabeled images and finally (iii) a batch of labeled images (both with and without dropout to also report the train accuracy). We can just repetitively call the Discriminator instance to build the graph for each of those outputs. Keras will make sure that the variables are reused in all cases. To turn off dropout for the labeled training data, we have to pass training=False explicitly.1234567891011121314151617181920212223(images_lab, labels_lab), (images_unl, labels_unl), (images_unl2, labels_unl2), \ (images_test, labels_test) = prepare_input_pipeline(flags_obj)with tf.name_scope("BatchSize"): batch_size_tensor = tf.shape(images_lab)[0]# Get the noise vectorsz, z_perturbed = define_noise(batch_size_tensor, flags_obj)# Generate images from noise vectorwith tf.name_scope("Generator"): g_model = define_generator() images_fake = g_model(z) images_fake_perturbed = g_model(z_perturbed)# Discriminate between real and fake, and try to classify the labeled datawith tf.name_scope("Discriminator") as discriminator_scope: d_model = Discriminator() logits_fake, features_fake = d_model(images_fake, training=True) logits_fake_perturbed, _ = d_model(images_fake_perturbed, training=True) logits_real_unl, features_real_unl = d_model(images_unl, training=True) logits_real_lab, features_real_lab = d_model(images_lab, training=True) logits_train, _ = d_model(images_lab, training=False) The discriminator’s loss&emsp;&emsp;Recall that the discriminator will be doing more than just separating fake from real. It also classifies the labeled data. For this, we define a supervised loss which takes the softmax output. In terms of implementation, this means that we feed the unnormalized logits to tf.nn.sparse_cross_entropy_with_logits.&emsp;&emsp;Defining the loss for the unsupervised part is where things get a little bit more involved. Because the softmax distribution is overparameterized, we can fix the unnormalized logit at 0 for an image to be fake (i.e. coming from the generator). If we do so, the probability of it being real just turns into: p(x)=\frac{Z(x)}{Z(x)+exp(l_{fake})} = \frac{Z(x)}{Z(x)+1}&emsp;&emsp;where Z(x) is the sum of the unnormalized probabilities. Note that we currently only have the logits. Ultimately, we want to use the log-probability of the fake class to define our loss function. This can now be achieved by computing the whole expression in log-space: log(p(x)) = log(Z(x)) - log(1+Z(x)) = logsumexp(l_1,...,l_K) - softplus(logsumexp(l_1,...,l_K))&emsp;&emsp;Where the lower case l with subscripts denote the individual logits. Divisions become subtractions and sums can be computed by the logsumexp function. Finally, we have used the definition of the softplus function: softplus(x) = log(1+x)&emsp;&emsp;In general, if you have the log-representation of a probability, it is numerically safer to keep things in log-space for as long as you can, since we are able to represent much smaller numbers in that case.&emsp;&emsp;We’re not there yet. Generative adversarial training asks us to ascend the gradient of: log(D(x)) + log(1-D(G(z)))&emsp;&emsp;So whenever we call tf.train.AdamOptimizer.minimize we should descent: -log(D(x)) - log(1-D(G(z))) = -log(\frac{Z(x)}{1+Z(x)})-log(1-\frac{Z(G(z))}{1+Z(G(z))})&emsp;&emsp;The first term on the right-hand side of the equation can be written: softplus(logsumexp(l^{(x)}_1,...,l^{(x)}_K)) - logsumexp(l^{(x)}_1,...,l^{(x)}_K)&emsp;&emsp;The second term of the right-hand side can be written as: -log(1-\frac{Z(G(z))}{1+Z(G(z))}) = -log(\frac{1}{1+Z(G(z))}) = softplus(logsumexp(l^{G(z)}_1,...,l^{G(z)}_K))&emsp;&emsp;So that finally, we arrive at the following loss: softplus(logsumexp(l^{(x)}_1,...,l^{(x)}_K)) - logsumexp(l^{(x)}_1,...,l^{(x)}_K) + softplus(logsumexp(l^{G(z)}_1,...,l^{G(z)}_K))123456789101112131415# Set the discriminator losseswith tf.name_scope("DiscriminatorLoss"): # Supervised loss, just cross-entropy. This normalizes p(y|x) where 1 &lt;= y &lt;= K loss_supervised = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits( labels=labels_lab, logits=logits_real_lab)) # Sum of unnormalized log probabilities logits_sum_real = tf.reduce_logsumexp(logits_real_unl, axis=1) logits_sum_fake = tf.reduce_logsumexp(logits_fake, axis=1) loss_unsupervised = 0.5 * ( tf.negative(tf.reduce_mean(logits_sum_real)) + tf.reduce_mean(tf.nn.softplus(logits_sum_real)) + tf.reduce_mean(tf.nn.softplus(logits_sum_fake))) loss_d = loss_supervised + loss_unsupervised Optimizing the discriminator&emsp;&emsp;Let’s setup the operations for actually updating the parameters of the discriminator. We will just reside to the Adam optimizer. While tweaking the parameters before I wrote this post, I figured I might slow down the discriminator by setting its learning rate at 0.1 times that of the generator. After that my results got much better, so I decided to leave it there for now. Notice also that we can very easily select the subset of variables corresponding to the discriminator by exploiting the encapsulation offered by Keras.12345# Configure discriminator training opswith tf.name_scope("Train") as train_scope: optimizer = tf.train.AdamOptimizer(flags_obj.lr * 0.1) optimize_d = optimizer.minimize(loss_d, var_list=d_model.trainable_variables) train_accuracy_op = accuracy(logits_train, labels_lab) Adding some control flow to the graph&emsp;&emsp;After we have the new weights for the discriminator, we want the generator’s update to be aware of the updated weights. TensorFlow will not guarantee that the updated weights will actually be used even if we were to redeclare the forward computation after defining the minimization operations for the discriminator. We can still force this by using tf.control_dependencies. Any operation defined in the scope of this context manager will depend on the evaluation of the ones that are passed to context manager at instantiation. In other words, our generator’s update that we define later on will be guaranteed to compute the gradients using the updated weights of the discriminator.12345with tf.name_scope(discriminator_scope): with tf.control_dependencies([optimize_d]): # Build a second time, so that new variables are used logits_fake, features_fake = d_model(images_fake, training=True) logits_real_unl, features_real_unl = d_model(images_unl2, training=True) The generator’s loss and updates&emsp;&emsp;In this implementation, the generator tries to minimize the L2 distance of the average features of the generated images vs. the average features of the real images. This feature-matching loss (Salimans et al., 2016) has proven to be more stable for training GANs than directly trying to optimize the discriminator’s probability for observing real data. It is straightforward to implement. While we’re at it, let’s also define the update operations for the generator. Notice that the learning rate of this optimizer is 10 times that of the discriminator.1234567891011# Set the generator loss and the actual train opwith tf.name_scope("GeneratorLoss"): feature_mean_real = tf.reduce_mean(features_real_unl, axis=0) feature_mean_fake = tf.reduce_mean(features_fake, axis=0) # L1 distance of features is the loss for the generator loss_g = tf.reduce_mean(tf.abs(feature_mean_real - feature_mean_fake))with tf.name_scope(train_scope): optimizer = tf.train.AdamOptimizer(flags_obj.lr, beta1=0.5) train_op = optimizer.minimize(loss_g, var_list=g_model.trainable_variables) Adding manifold regularization&emsp;&emsp;Lecouat et. al (2018) propose to add manifold regularization to the feature-matching GAN training procedure of Salimans et al. (2016). The regularization forces the discriminator to yield similar logits (unnormalized log probabilities) for nearby points in the latent space in which z resides. It can be implemented by generating a second perturbed version of z and computing the generator’s and discriminator’s outputs once more with this slightly altered vector. &emsp;&emsp;This means that the noise generation code looks as follows:12345678def define_noise(batch_size_tensor, flags_obj): # Setup noise vector with tf.name_scope("LatentNoiseVector"): z = tfd.Normal(loc=0.0, scale=flags_obj.stddev).sample( sample_shape=(batch_size_tensor, flags_obj.z_dim_size)) z_perturbed = z + tfd.Normal(loc=0.0, scale=flags_obj.stddev).sample( sample_shape=(batch_size_tensor, flags_obj.z_dim_size)) * 1e-5 return z, z_perturbed &emsp;&emsp;The discriminator’s loss will be updated as follows (note the 3 extra lines at the bottom):1234567891011121314151617# Set the discriminator losseswith tf.name_scope("DiscriminatorLoss"): # Supervised loss, just cross-entropy. This normalizes p(y|x) where 1 &lt;= y &lt;= K loss_supervised = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits( labels=labels_lab, logits=logits_real_lab)) # Sum of unnormalized log probabilities logits_sum_real = tf.reduce_logsumexp(logits_real_unl, axis=1) logits_sum_fake = tf.reduce_logsumexp(logits_fake, axis=1) loss_unsupervised = 0.5 * ( tf.negative(tf.reduce_mean(logits_sum_real)) + tf.reduce_mean(tf.nn.softplus(logits_sum_real)) + tf.reduce_mean(tf.nn.softplus(logits_sum_fake))) loss_d = loss_supervised + loss_unsupervised if flags_obj.man_reg: loss_d += 1e-3 * tf.nn.l2_loss(logits_fake - logits_fake_perturbed) \ / tf.to_float(batch_size_tensor) Classification performance&emsp;&emsp;So how does it really perform? I have provided a few plots below. There are many things I might try to squeeze out additional performance (for instance, just training for longer, using a learning rate schedule, implementing weight normalization), but the main purpose of writing this post was to get to know a relatively simple yet powerful semi-supervised learning approach. After 100 epochs of training, the mean test accuracy approaches 98.9 percent.&emsp;&emsp;The full script can be found here. Thanks for reading! Reference Semi-supervised learning with GANs]]></content>
  </entry>
  <entry>
    <title><![CDATA[Semi-Supervised Learning and GANs]]></title>
    <url>%2Fsemi-supervised-learning-and-gans.html</url>
    <content type="text"><![CDATA[&emsp;&emsp;Vincent Van Gogh painted this beautiful art: ‘The Starry Night’ in 1889 and today my GAN model (I like to call it GAN Gogh :P) painted some MNIST digits with only 20% labeled data!! How could it achieve this remarkable feat? … Let’s find out IntroductionWhat is semi-supervised learning?&emsp;&emsp;Most deep learning classifiers require a large amount of labeled samples to generalize well, but getting such data is an expensive and difficult process. To deal with this limitation Semi-supervised learning is presented, which is a class of techniques that make use of a morsel of labeled data along with a large amount of unlabeled data.Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data can produce considerable improvement in learning accuracy. GANs have shown a lot of potential in semi-supervised learning where the classifier can obtain a good performance with very few labeled data. Background on GANs&emsp;&emsp;GANs are members of deep generative models. They are particularly interesting because they don’t explicitly represent a probability distribution over the space where the data lies. Instead, they provide some way of interacting less directly with this probability distribution by drawing samples from it.The basic idea of GAN is to set up a game between two players: A generator G: Takes random noise z as input and outputs an image x. Its parameters are tuned to get a high score from the discriminator on fake images that it generates. A discriminator D: Takes an image x as input and outputs a score which reflects its confidence that it is a real image. Its parameters are tuned to have a high score when it is fed by a real image, and a low score when a fake image is fed from the generator. &emsp;&emsp;I suggest you to go through this and this for more details on their working and optimisation objectives. Now, let us turn the wheels a little and talk about one of the most prominent applications of GANs, semi-supervised learning. Intuition&emsp;&emsp;The vanilla architecture of discriminator has only one output neuron for classifying the R/F probabilities. We train both the networks simultaneously and discard the discriminator after the training as it was used only for improving the generator.&emsp;&emsp;For the semi-supervised task, in addition to R/F neuron, the discriminator will now have 10 more neurons for classification of MNIST digits. Also, this time their roles change and we can discard the generator after training, whose only objective was to generate unlabeled data to improve the discriminator’s performance.&emsp;&emsp;Now the discriminator is turned into an 11-class classifier with 1 neuron (R/F neuron) representing the fake data output and the other 10 representing real data with classes. The following has to be kept in mind: To assert R/F neuron output label = 0, when real unsupervised data from dataset is fed To assert R/F neuron output label= 1, when fake unsupervised data from generator is fed To assert R/F output label = 0 and corresponding label output = 1, when real supervised data is fed This combination of different sources of data will help the discriminator classify more accurately than, if it had been only provided with a portion of labeled data. ArchitectureNow it’s time to get our hands dirty with some code :DThe Discriminator&emsp;&emsp;The architecture followed is similar to the one proposed in DCGAN paper. We use strided convolutions for reducing the dimensions of the feature-vectors rather than any pooling layers and apply a series of leaky_relu, dropout and BN for all layers to stabilize the learning. BN is dropped for input layer and last layer (for the purpose of feature matching). In the end, we perform Global Average Pooling to take the average over the spatial dimensions of the feature vectors. This squashes the tensor dimensions to a single value. After flattening the features, a dense layer of 11 classes is added with softmax activation for multi-class output.1234567891011121314151617181920212223242526272829303132333435def discriminator(x, dropout_rate = 0., is_training = True, reuse = False): # input x -&gt; n+1 classes with tf.variable_scope('Discriminator', reuse = reuse): # x = ?*64*64*1 #Layer 1 conv1 = tf.layers.conv2d(x, 128, kernel_size = [4,4], strides = [2,2], padding = 'same', activation = tf.nn.leaky_relu, name = 'conv1') # ?*32*32*128 #No batch-norm for input layer dropout1 = tf.nn.dropout(conv1, dropout_rate) #Layer2 conv2 = tf.layers.conv2d(dropout1, 256, kernel_size = [4,4], strides = [2,2], padding = 'same', activation = tf.nn.leaky_relu, name = 'conv2') # ?*16*16*256 batch2 = tf.layers.batch_normalization(conv2, training = is_training) dropout2 = tf.nn.dropout(batch2, dropout_rate) #Layer3 conv3 = tf.layers.conv2d(dropout2, 512, kernel_size = [4,4], strides = [4,4], padding = 'same', activation = tf.nn.leaky_relu, name = 'conv3') # ?*4*4*512 batch3 = tf.layers.batch_normalization(conv3, training = is_training) dropout3 = tf.nn.dropout(batch3, dropout_rate) # Layer 4 conv4 = tf.layers.conv2d(dropout3, 1024, kernel_size=[3,3], strides=[1,1], padding='valid',activation = tf.nn.leaky_relu, name='conv4') # ?*2*2*1024 # No batch-norm as this layer's op will be used in feature matching loss # No dropout as feature matching needs to be definite on logits # Layer 5 # Note: Applying Global average pooling flatten = tf.reduce_mean(conv4, axis = [1,2]) logits_D = tf.layers.dense(flatten, (1 + num_classes)) out_D = tf.nn.softmax(logits_D) return flatten,logits_D,out_D The Generator&emsp;&emsp;The generator architecture is designed to mirror the discriminator’s spatial outputs. Fractional strided convolutions are used to increase the spatial dimension of the representation. An input of 4-D tensor of noise z is fed which undergoes a series of transposed convolutions, relu, BN(except at output layer) and dropout operations. Finally tanh activation maps the output image in range (-1,1).123456789101112131415161718192021222324252627282930def generator(z, dropout_rate = 0., is_training = True, reuse = False): # input latent z -&gt; image x with tf.variable_scope('Generator', reuse = reuse): #Layer 1 deconv1 = tf.layers.conv2d_transpose(z, 512, kernel_size = [4,4], strides = [1,1], padding = 'valid', activation = tf.nn.relu, name = 'deconv1') # ?*4*4*512 batch1 = tf.layers.batch_normalization(deconv1, training = is_training) dropout1 = tf.nn.dropout(batch1, dropout_rate) #Layer 2 deconv2 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4], strides = [4,4], padding = 'same', activation = tf.nn.relu, name = 'deconv2')# ?*16*16*256 batch2 = tf.layers.batch_normalization(deconv2, training = is_training) dropout2 = tf.nn.dropout(batch2, dropout_rate) #Layer 3 deconv3 = tf.layers.conv2d_transpose(dropout2, 128, kernel_size = [4,4], strides = [2,2], padding = 'same', activation = tf.nn.relu, name = 'deconv3')# ?*32*32*256 batch3 = tf.layers.batch_normalization(deconv3, training = is_training) dropout3 = tf.nn.dropout(batch3, dropout_rate) #Output layer deconv4 = tf.layers.conv2d_transpose(dropout3, 1, kernel_size = [4,4], strides = [2,2], padding = 'same', activation = None, name = 'deconv4')# ?*64*64*1 out = tf.nn.tanh(deconv4) return out Model Loss&emsp;&emsp;We start by preparing an extended label for the whole batch by appending actual label to zeros. This is done to assert the R/F neuron output to 0 when the labeled data is fed. The discriminator loss for unlabeled data can be thought of as a binary sigmoid loss by asserting R/F neuron output to 1 for fake images and 0 for real images.12345678910111213141516171819 ### Discriminator loss #### Supervised loss -&gt; which class the real data belongs to temp = tf.nn.softmax_cross_entropy_with_logits_v2(logits = D_real_logit, labels = extended_label) # Labeled_mask and temp are of same size = batch_size where temp is softmax cross_entropy calculated over whole batchD_L_Supervised = tf.reduce_sum(tf.multiply(temp,labeled_mask)) / tf.reduce_sum(labeled_mask)# Multiplying temp with labeled_mask gives supervised loss on labeled_mask# data only, calculating mean by dividing by no of labeled samples# Unsupervised loss -&gt; R/F D_L_RealUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits = D_real_logit[:, 0], labels = tf.zeros_like(D_real_logit[:, 0], dtype=tf.float32)))D_L_FakeUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits = D_fake_logit[:, 0], labels = tf.ones_like(D_fake_logit[:, 0], dtype=tf.float32)))D_L = D_L_Supervised + D_L_RealUnsupervised + D_L_FakeUnsupervised &emsp;&emsp;Generator loss is a combination of fake_image loss which falsely wants to assert R/F neuron output to 0 and feature matching loss which penalizes the mean absolute error between the average value of some set of features on the training data and the average values of that set of features on the generated samples.123456789101112 ### Generator loss ### # G_L_1 -&gt; Fake data wanna be real G_L_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits = D_fake_logit[:, 0],labels = tf.zeros_like(D_fake_logit[:, 0], dtype=tf.float32)))# G_L_2 -&gt; Feature matchingdata_moments = tf.reduce_mean(D_real_features, axis = 0)sample_moments = tf.reduce_mean(D_fake_features, axis = 0)G_L_2 = tf.reduce_mean(tf.square(data_moments-sample_moments))G_L = G_L_1 + G_L_2 Training&emsp;&emsp;The training images are resized from [batch_size, 28 ,28 , 1] to [batch_size, 64, 64, 1] to fit the generator/discriminator architectures. Losses, accuracies and generated samples are calculated and are observed to improve over each epoch.1234567891011121314151617181920212223242526272829303132333435363738for epoch in range(epochs): train_accuracies, train_D_losses, train_G_losses = [], [], [] for it in range(no_of_batches): batch = mnist_data.train.next_batch(batch_size, shuffle = False) # batch[0] has shape: batch_size*28*28*1 batch_reshaped = tf.image.resize_images(batch[0], [64, 64]).eval() # Reshaping the whole batch into batch_size*64*64*1 for disc/gen architecture batch_z = np.random.normal(0, 1, (batch_size, 1, 1, latent)) mask = get_labeled_mask(labeled_rate, batch_size) train_feed_dict = &#123;x : scale(batch_reshaped), z : batch_z, label : batch[1], labeled_mask : mask, dropout_rate : 0.7, is_training : True&#125; #The label provided in dict are one hot encoded in 10 classes D_optimizer.run(feed_dict = train_feed_dict) G_optimizer.run(feed_dict = train_feed_dict) train_D_loss = D_L.eval(feed_dict = train_feed_dict) train_G_loss = G_L.eval(feed_dict = train_feed_dict) train_accuracy = accuracy.eval(feed_dict = train_feed_dict) train_D_losses.append(train_D_loss) train_G_losses.append(train_G_loss) train_accuracies.append(train_accuracy) tr_GL = np.mean(train_G_losses) tr_DL = np.mean(train_D_losses) tr_acc = np.mean(train_accuracies) print ('After epoch: '+ str(epoch+1) + ' Generator loss: ' + str(tr_GL) + ' Discriminator loss: ' + str(tr_DL) + ' Accuracy: ' + str(tr_acc)) gen_samples = fake_data.eval(feed_dict = &#123;z : np.random.normal(0, 1, (25, 1, 1, latent)), dropout_rate : 0.7, is_training : False&#125;) # Dont train batch-norm while plotting =&gt; is_training = False test_images = tf.image.resize_images(gen_samples, [64, 64]).eval() show_result(test_images, (epoch + 1), show = True, save = False, path = '') Conclusion&emsp;&emsp;The training was done for 5 epochs and 20% labeled_rate due to restricted GPU access. For better results more training epochs with lesser labeled_rate is advised. The complete code notebook can be found here.&emsp;&emsp;Unsupervised learning is considered as a lacuna in the field of AGI. To bridge this gap, GANs are considered as a potential solution for learning complex tasks with low labeled data. With blooming new approaches in the domain of semi and unsupervised learning we can expect that this gap will lessen.&emsp;&emsp;I would be remiss not to mention my inspiration from this beautiful blog, this implementation along with the assistance of my colleague working on similar projects.Until next time!! Kz Reference Semi-Supervised Learning and GANs]]></content>
  </entry>
  <entry>
    <title><![CDATA[手把手教你用GAN实现半监督学习]]></title>
    <url>%2Fshou-ba-shou-jiao-ni-yong-gan-shi-xian-ban-jian-du-xue-xi.html</url>
    <content type="text"><![CDATA[引言&emsp;&emsp;本文主要介绍如何在tensorflow上仅使用200个带标签的mnist图像，实现在一万张测试图片上99%的测试精度，原理在于使用GAN做半监督学习。前文主要介绍一些原理部分，后文详细介绍代码及其实现原理。前文介绍比较简单，有基础的同学请略过直接看第二部分，文章末尾给出了代码GitHub链接。对GAN不了解的同学可以查看微信公众号：机器学习算法全栈工程师 的GAN入门文章。1本博客中的代码最终以GitHub中的代码为准，GitHub链接在文章底部，另外，本文已投稿至微信公众号：机器学习算法全栈工程师，欢迎关注此公众号 1.监督，无监督，半监督学习介绍&emsp;&emsp;在正式介绍实现半监督学习之前，我在这里首先介绍一下监督学习（supervised learning），半监督学习（semi-supervised learning）和无监督学习（unsupervised learning）的区别。 监督学习是指在训练集中包含训练数据的标签（label），比如类别标签，位置标签等等。最普遍使用标签学习的是分类任务，对于分类任务，输入给网络训练样本（samples）的一些特征（feature）以及此样本对应的标签（label），通过神经网络拟合的方法，神经网络可以在特征和标签之间找到一个合适的映射关系（mapping），这样当训练完成后，输入给网络没有label的样本，神经网络可以通过这一个映射关系猜出它属于哪一类。典型机器学习的监督学习的例子是KNN和SVM。目前机器视觉领域的急速发展离不开监督学习。 而无监督学习的训练事先没有训练标签，直接输入给算法一些数据，算法会努力学习数据的共同点，寻找样本之间的规律性。无监督学习是很典型的学习，人的学习有时候就是基于无监督的，比如我并不懂音乐，但是我听了上百首歌曲后，我可以根据我听的结果将音乐分为摇滚乐（记为0类）、民谣（记为1类）、纯音乐（记为2类）等等，事实上，我并不知道具体是哪一类，所以将它们记为0，1，2三类。典型的无监督学习方法是聚类算法，比如k-means。 东方快车电影里面大侦探有过一个台词，人们的话只有对与错，没有中间地带，最后经过一系列事件后他找到了对与错之间的betweeness。在监督学习和无监督学习之间，同样存在着中间地带——半监督学习。半监督学习简单来说就是将无监督学习和监督学习相结合，一部分包含了监督学习一部分包含了无监督学习，比如给一个分类任务，此分类任务的训练集中有精确标签的数据非常少，但是包含了大量的没有标注的数据，如果直接用监督学习的方法去做的话，效果不一定很好，有标注的训练数据太少很容易导致过拟合，而且大量的无标注的数据都没有充分的利用，最常见的例子是在医学图像的分析检测任务中，医学图像本身就不容易获得，要获得精标注的图像就需要有经验的医生去一个一个标注，显然他们并没有那么多的时间。这时候就是半监督学习的用武之地了，半监督学习很适合用在标签数据少，训练数据又比较多的情况。常见的半监督学习方法主要有：&emsp;&emsp;1.Self training &emsp;&emsp;2.Generative model &emsp;&emsp;3.S3VMs &emsp;&emsp;4.Graph-Based AIgorithems &emsp;&emsp;5.Multiview AIgorithems &emsp;&emsp;接下来我会结合Improved Techniques for Training GANs这篇论文详细介绍如何使用目前最火的生成对抗模型GAN去实现半监督学习，也即是半监督学习的第二种方法，并给出详细的代码解释，对理论不是很熟悉的同学可以直接看代码。另外注明：我只复现了论文半监督学习的部分，之前也有人复现了此部分，但是我感觉他对原文有很大的曲解，他使用了所有的标签去帮助生成，并不在分类上，不太符合半监督学习的本质，而且代码很复杂，感兴趣的可以去GitHub上搜ssgan,希望能帮助你。 2. Improved Techniques for Training GANs&emsp;&emsp;GAN是无监督学习的代表，它可以不断学习模拟数据的分布进而生成和训练数据相似分布的样本，在训练过程不需要标签，GAN在无监督学习领域，生成领域，半监督学习领域以及强化学习领域都有广泛的应用。但是GAN存在很多的训练不稳定等等的问题，作者good fellow在2016年放出了Improved Techniques for Training GANs，对GAN训练不稳定的问题做了一些解释和经验上的解决方案，并给出了和半监督学习结合的方法。&emsp;&emsp;从平衡点角度解释GAN的不稳定性来说，GAN的纳什均衡点是一个鞍点，并不是一个局部最小值点，基于梯度的方法主要是寻找高维空间中的极小值点，因此使用梯度训练的方法很难使GAN收敛到平衡点。为此，为了进一部分缓解这个问题，goodfellow联合提出了一些改进方案，主要有： Feature matching, Minibatch discrimination weight Historical averaging (相当于一个正则化的方式) One-sided label smoothing Virtual batch normalization 后来发现Feature matching在半监督学习上表现良好，mini-batch discrimination表现很差。 3. semi-supervised GAN&emsp;&emsp;对于一个普通的分类器来说，假设对MNIST分类，一共有10类数据，分别是0-9，分类器模型以数据x作为输入，输出一个K=10维的向量，经过softmax后计算出分类概率最大的那个类别。在监督学习领域，往往是通过最小化类别标签和预测分布 的交叉熵来实现最好的结果。&emsp;&emsp;但是将GAN用在半监督学习领域的时候需要做一些改变，生成器不做改变，仍然负责从输入噪声数据中生成图像，判别器D不在是一个简单的真假分类（二分类）器，假设输入数据有K类，D就是K+1的分类器，多出的那一类是判别输入是否是生成器G生成的图像。网络的流程图见下图：&emsp;&emsp;网络结构确定了之后就是损失函数的设计部分，借助GAN我们就可以从无标签数据中学习，只要知道输入数据是真实数据，那就可以通过最大化\(logP_{model}(y\in{1,2,…,K}|x)\)来实现，上述式子可解释为不管输入的是哪一类真的图片（不是生成器G生成的假图片），只要最大化输出它是真图像的概率就可以了，不需要具体分出是哪一类。由于GAN的生成器的参与，训练数据中有一半都是生成的假数据。&emsp;&emsp;下面给出判别器D的损失函数设计，D损失函数包括两个部分，一个是监督学习损失，一个是半监督学习损失，具体公式如下： L=L_{supervised} + L_{unsupervised}其中：对于无监督学习来说，只需要输出真假就可以了，不需要确定是哪一类，因此我们令 D(x)=1-logP_{model}(y\in{1,2,...,K}|x)其中\( P_{model} \)表示判别是假图像的概率，那么D(x)就代表了输出是真图像的概率，那么无监督学习的损失函数就可以表示为 L_{unsupervised} = -\{E_{x\sim Pdata(x)}logD(x) + E_{z\sim noise}log(1-D(G(z)))\}&emsp;&emsp;这不就是GAN的损失函数嘛！好了，到这里得出结论，在半监督学习中，判别器的分类要多分一类，多出的这一类表示的是生成器生成的假图像这一类，另外判别器的损失函数不仅包括了监督损失函数而且还有无监督的损失函数，在训练过程中同时最小化这两者。损失函数介绍完毕，接下来介绍代码实现部分。 4.代码实现及解读&emsp;&emsp;注：完整代码的GitHub连接在文章底部。这里只截取关键部分做介绍 &emsp;&emsp;在代码中，我使用feature matching，one side label smoothing方式，并没有使用论文中介绍的Historical averaging,而是只对判别器D使用了简单的l2正则化，防止过拟合，另外论文中介绍的Minibatch discrimination, Virtual batch normalization等等都没有使用，主要是这两者在半监督学习中表现不是很好，但是如果想获得好的生成结果还是很有用的。&emsp;&emsp;首先介绍网络结构部分，因为是在mnist数据集比较简单，所以随便搭了一个判别器和生成器，具体如下：12345678910111213141516171819202122def discriminator(self, name, inputs, reuse): l = tf.shape(inputs)[0] inputs = tf.reshape(inputs, (l,self.img_size,self.img_size,self.dim)) with tf.variable_scope(name,reuse=reuse): out = [] output = conv2d('d_con1',inputs,5, 64, stride=2, padding='SAME') #14*14 output1 = lrelu(self.bn('d_bn1',output)) out.append(output1) # output1 = tf.contrib.keras.layers.GaussianNoise output = conv2d('d_con2', output1, 3, 64*2, stride=2, padding='SAME')#7*7 output2 = lrelu(self.bn('d_bn2', output)) out.append(output2) output = conv2d('d_con3', output2, 3, 64*4, stride=1, padding='VALID')#5*5 output3 = lrelu(self.bn('d_bn3', output)) out.append(output3) output = conv2d('d_con4', output3, 3, 64*4, stride=2, padding='VALID')#2*2 output4 = lrelu(self.bn('d_bn4', output)) out.append(output4) output = tf.reshape(output4, [l, 2*2*64*4])# 2*2*64*4 output = fc('d_fc', output, self.num_class) # output = tf.nn.softmax(output) return output, out &emsp;&emsp;其中conv2d()是卷积操作，参数依次是，层的名字，输入tensor，卷积核大小，输出通道数，步长，padding。判别器中每一层都加了归一化层，这里使用最简单的归一化，函数如下所示，另外每一层的激活函数使用leakyrelu。判别器D最终返回两个值，第一个是计算的logits，另外一个是一个列表，列表的每一个元素代表判别器每一层的输出，为接下来实现feature matching做准备。&emsp;&emsp;生成器结构如下所示：其最后一层激活函数使用tanh12345678910111213141516171819def generator(self,name, noise, reuse): with tf.variable_scope(name,reuse=reuse): l = self.batch_size output = fc('g_dc', noise, 2*2*64) output = tf.reshape(output, [-1, 2, 2, 64]) output = tf.nn.relu(self.bn('g_bn1',output)) output = deconv2d('g_dcon1',output,5,outshape=[l, 4, 4, 64*4]) output = tf.nn.relu(self.bn('g_bn2',output)) output = deconv2d('g_dcon2', output, 5, outshape=[l, 8, 8, 64 * 2]) output = tf.nn.relu(self.bn('g_bn3', output)) output = deconv2d('g_dcon3', output, 5, outshape=[l, 16, 16,64 * 1]) output = tf.nn.relu(self.bn('g_bn4', output)) output = deconv2d('g_dcon4', output, 5, outshape=[l, 32, 32, self.dim]) output = tf.image.resize_images(output, (28, 28)) # output = tf.nn.relu(self.bn('g_bn4', output)) return tf.nn.tanh(output) 网络结构是根据DCGAN的结构改的，所以网络简要介绍到这里。 接下来介绍网络初始化方面：首先在train.py里建立一个Train的类，并做一些初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215#coding:utf-8from glob import globfrom PIL import Imageimport matplotlib.pyplot as pltimport scipy.misc as scmfrom vlib.layers import *import tensorflow as tfimport numpy as npfrom vlib.load_data import *import osimport vlib.plot as plotimport vlib.my_extract as dataloadimport vlib.save_images as save_imgimport timefrom tensorflow.examples.tutorials.mnist import input_data #as mnist_datamnist = input_data.read_data_sets('data/', one_hot=True)# temp = 0.89class Train(object): def __init__(self, sess, args): #sess=tf.Session() self.sess = sess self.img_size = 28 # the size of image self.trainable = True self.batch_size = 50 # must be even number self.lr = 0.0002 self.mm = 0.5 # momentum term for adam self.z_dim = 128 # the dimension of noise z self.EPOCH = 50 # the number of max epoch self.LAMBDA = 0.1 # parameter of WGAN-GP self.model = args.model # 'DCGAN' or 'WGAN' self.dim = 1 # RGB is different with gray pic self.num_class = 11 self.load_model = args.load_model self.build_model() # initializer def build_model(self): # build placeholders self.x=tf.placeholder(tf.float32,shape=[self.batch_size,self.img_size*self.img_size*self.dim],name='real_img') self.z = tf.placeholder(tf.float32, shape=[self.batch_size, self.z_dim], name='noise') self.label = tf.placeholder(tf.float32, shape=[self.batch_size, self.num_class - 1], name='label') self.flag = tf.placeholder(tf.float32, shape=[], name='flag') self.flag2 = tf.placeholder(tf.float32, shape=[], name='flag2') # define the network self.G_img = self.generator('gen', self.z, reuse=False) d_logits_r, layer_out_r = self.discriminator('dis', self.x, reuse=False) d_logits_f, layer_out_f = self.discriminator('dis', self.G_img, reuse=True) d_regular = tf.add_n(tf.get_collection('regularizer', 'dis'), 'loss') # D regular loss # caculate the unsupervised loss un_label_r = tf.concat([tf.ones_like(self.label), tf.zeros(shape=(self.batch_size, 1))], axis=1) un_label_f = tf.concat([tf.zeros_like(self.label), tf.ones(shape=(self.batch_size, 1))], axis=1) logits_r, logits_f = tf.nn.softmax(d_logits_r), tf.nn.softmax(d_logits_f) d_loss_r = -tf.log(tf.reduce_sum(logits_r[:, :-1])/tf.reduce_sum(logits_r[:,:])) d_loss_f = -tf.log(tf.reduce_sum(logits_f[:, -1])/tf.reduce_sum(logits_f[:,:])) # d_loss_r = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=un_label_r*0.9, logits=d_logits_r)) # d_loss_f = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=un_label_f*0.9, logits=d_logits_f)) # feature match f_match = tf.constant(0., dtype=tf.float32) for i in range(4): f_match += tf.reduce_mean(tf.multiply(layer_out_f[i]-layer_out_r[i], layer_out_f[i]-layer_out_r[i])) # caculate the supervised loss s_label = tf.concat([self.label, tf.zeros(shape=(self.batch_size,1))], axis=1) s_l_r = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=s_label*0.9, logits=d_logits_r)) s_l_f = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=un_label_f*0.9, logits=d_logits_f)) # same as d_loss_f self.d_l_1, self.d_l_2 = d_loss_r + d_loss_f, s_l_r self.d_loss = d_loss_r + d_loss_f + s_l_r*self.flag*10 + d_regular self.g_loss = d_loss_f + 0.01*f_match all_vars = tf.global_variables() g_vars = [v for v in all_vars if 'gen' in v.name] d_vars = [v for v in all_vars if 'dis' in v.name] for v in all_vars: print v if self.model == 'DCGAN': self.opt_d = tf.train.AdamOptimizer(self.lr, beta1=self.mm).minimize(self.d_loss, var_list=d_vars) self.opt_g = tf.train.AdamOptimizer(self.lr, beta1=self.mm).minimize(self.g_loss, var_list=g_vars) elif self.model == 'WGAN_GP': self.opt_d = tf.train.AdamOptimizer(1e-5, beta1=0.5, beta2=0.9).minimize(self.d_loss, var_list=d_vars) self.opt_g = tf.train.AdamOptimizer(1e-5, beta1=0.5, beta2=0.9).minimize(self.g_loss, var_list=g_vars) else: print ('model can only be "DCGAN","WGAN_GP" !') return # test test_logits, _ = self.discriminator('dis', self.x, reuse=True) test_logits = tf.nn.softmax(test_logits) temp = tf.reshape(test_logits[:, -1],shape=[self.batch_size, 1]) for i in range(10): temp = tf.concat([temp, tf.reshape(test_logits[:, -1],shape=[self.batch_size, 1])], axis=1) test_logits -= temp self.prediction = tf.nn.in_top_k(test_logits, tf.argmax(s_label, axis=1), 1) self.saver = tf.train.Saver() if not self.load_model: init = tf.global_variables_initializer() self.sess.run(init) elif self.load_model: self.saver.restore(self.sess, os.getcwd()+'/model_saved/model.ckpt') print 'model load done' self.sess.graph.finalize() def train(self): if not os.path.exists('model_saved'): os.mkdir('model_saved') if not os.path.exists('gen_picture'): os.mkdir('gen_picture') noise = np.random.normal(-1, 1, [self.batch_size, 128]) temp = 0.80 print 'training' for epoch in range(self.EPOCH): # iters = int(156191//self.batch_size) iters = 50000//self.batch_size flag2 = 1 # if epoch&gt;10 else 0 for idx in range(iters): start_t = time.time() flag = 1 if idx &lt; 4 else 0 # set we use 2*batch_size=200 train data labeled. batchx, batchl = mnist.train.next_batch(self.batch_size) # batchx, batchl = self.sess.run([batchx, batchl]) g_opt = [self.opt_g, self.g_loss] d_opt = [self.opt_d, self.d_loss, self.d_l_1, self.d_l_2] feed = &#123;self.x:batchx, self.z:noise, self.label:batchl, self.flag:flag, self.flag2:flag2&#125; # update the Discrimater k times _, loss_d, d1,d2 = self.sess.run(d_opt, feed_dict=feed) # update the Generator one time _, loss_g = self.sess.run(g_opt, feed_dict=feed) print ("[%3f][epoch:%2d/%2d][iter:%4d/%4d],loss_d:%5f,loss_g:%4f, d1:%4f, d2:%4f"% (time.time()-start_t, epoch, self.EPOCH,idx,iters, loss_d, loss_g,d1,d2)), 'flag:',flag plot.plot('d_loss', loss_d) plot.plot('g_loss', loss_g) if ((idx+1) % 100) == 0: # flush plot picture per 1000 iters plot.flush() plot.tick() if (idx+1)%500==0: print ('images saving............') img = self.sess.run(self.G_img, feed_dict=feed) save_img.save_images(img, os.getcwd()+'/gen_picture/'+'sample&#123;&#125;_&#123;&#125;.jpg'\ .format(epoch, (idx+1)/500)) print 'images save done' test_acc = self.test() plot.plot('test acc', test_acc) plot.flush() plot.tick() print 'test acc:&#123;&#125;'.format(test_acc), 'temp:%3f'%(temp) if test_acc &gt; temp: print ('model saving..............') path = os.getcwd() + '/model_saved' save_path = os.path.join(path, "model.ckpt") self.saver.save(self.sess, save_path=save_path) print ('model saved...............') temp = test_acc# output = conv2d('Z_cona&#123;&#125;'.format(i), output, 3, 64, stride=1, padding='SAME') def generator(self,name, noise, reuse): with tf.variable_scope(name,reuse=reuse): l = self.batch_size output = fc('g_dc', noise, 2*2*64) output = tf.reshape(output, [-1, 2, 2, 64]) output = tf.nn.relu(self.bn('g_bn1',output)) output = deconv2d('g_dcon1',output,5,outshape=[l, 4, 4, 64*4]) output = tf.nn.relu(self.bn('g_bn2',output)) output = deconv2d('g_dcon2', output, 5, outshape=[l, 8, 8, 64 * 2]) output = tf.nn.relu(self.bn('g_bn3', output)) output = deconv2d('g_dcon3', output, 5, outshape=[l, 16, 16,64 * 1]) output = tf.nn.relu(self.bn('g_bn4', output)) output = deconv2d('g_dcon4', output, 5, outshape=[l, 32, 32, self.dim]) output = tf.image.resize_images(output, (28, 28)) # output = tf.nn.relu(self.bn('g_bn4', output)) return tf.nn.tanh(output) def discriminator(self, name, inputs, reuse): l = tf.shape(inputs)[0] inputs = tf.reshape(inputs, (l,self.img_size,self.img_size,self.dim)) with tf.variable_scope(name,reuse=reuse): out = [] output = conv2d('d_con1',inputs,5, 64, stride=2, padding='SAME') #14*14 output1 = lrelu(self.bn('d_bn1',output)) out.append(output1) # output1 = tf.contrib.keras.layers.GaussianNoise output = conv2d('d_con2', output1, 3, 64*2, stride=2, padding='SAME')#7*7 output2 = lrelu(self.bn('d_bn2', output)) out.append(output2) output = conv2d('d_con3', output2, 3, 64*4, stride=1, padding='VALID')#5*5 output3 = lrelu(self.bn('d_bn3', output)) out.append(output3) output = conv2d('d_con4', output3, 3, 64*4, stride=2, padding='VALID')#2*2 output4 = lrelu(self.bn('d_bn4', output)) out.append(output4) output = tf.reshape(output4, [l, 2*2*64*4])# 2*2*64*4 output = fc('d_fc', output, self.num_class) # output = tf.nn.softmax(output) return output, out def bn(self, name, input): val = tf.contrib.layers.batch_norm(input, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=True, scope=name) return val # def get_loss(self, logits, layer_out): def test(self): count = 0. print 'testing................' for i in range(10000//self.batch_size): testx, textl = mnist.test.next_batch(self.batch_size) prediction = self.sess.run(self.prediction, feed_dict=&#123;self.x:testx, self.label:textl&#125;) count += np.sum(prediction) return count/10000. &emsp;&emsp;args是传进来的参数，主要包括三个， 一个是args.model，选择DCGAN模式还是WGAN-GP模式，二者的不同主要在于损失函数不同和优化器的学习率不同，其他都一样。 第二个参数是args.trainable，训练还是测试，训练时为True，测试是False。 Loadmodel表示是否选择加载训练好的权重。 &emsp;&emsp;Build_model函数里面主要包括了网络训练前的准备工作，主要包括损失函数的设计和优化器的设计。下文将详细做出介绍，尤其是损失函数部分。&emsp;&emsp;首先，建立了五个placeholder，flag表示两个标志位，只有0-1两种情况，注意到我num_class是11，也就是做11分类，但是lable的placeholder中shape是(batchsize,10)。为了方便，我将生成器的生成结果和真实数据X级联在一起作为判别器的输入，输出再把他它们结果split分开。&emsp;&emsp;d_regular 表示正则化，这里我将判别器中所有的weights做了l2正则。&emsp;&emsp;监督学习的损失函数使用常见的交叉熵损失函数，对生成器生成的图像的label的one_hot型为：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]&emsp;&emsp;将原始的label扩展到(batchsize，11)后再和生成器生成的假数据的label在第一维度concat到一起得到batchl，另外乘以0.9，做单边标签平滑（one side smoothing），由此计算得到监督学习的损失函数值s_l,。 生成器G的损失函数&emsp;&emsp;生成器G的损失函数包括两部分，一个是来自GAN训练的部分，另外一个是feature matching , 论文中提到的feature matching意思是特征匹配，主要思想是希望生成器生成的假数据输入到判别器，经过判别器每一层计算的结果和将真实数据X输入到判别器，判别器每一层的结果尽可能的相似，公式如下： \|E_{x\sim Pdata(x)}f(x) - E_{z\sim Pz(z)}f(G(z))\|^2_2&emsp;&emsp;其中\(f(x)\)是D的每一层的输出。Feature matching 是指导G进行训练，所以我将他放在了G的损失函数里。 分类器D的损失函数：相比较G的损失函数，D的损失函数就比较麻烦了接下来介绍无监督学习的损失函数实现：在前面介绍的无监督学习的损失函数中，有一部分和GAN的损失函数很相似，所以在代码中我们使用了&emsp;&emsp;无监督学习的时候没有标签的指导，此时判别器或者称为分类器D无法正确对输入进行分类，此时只要求D能够区分真假就可以了，由此我们得到了无监督学习的损失un_s，直观上也很好理解，假设输入给判别器D真图像，它结果经过softmax后输出类似下面表格的形式，其中前十个黄色区域表示对0-9的分类概率，最后一个灰色的表示对假图像的分类概率，由于无监督学习中判别器D并不知道具体是哪一类数据，所以干脆D的损失函数最小化输出假图像的概率就可以了，当输入为生成器生成的假图像时，只要最小化D输出为真图像的概率，由此我们得到了un_s.。但是此时有一个问题，即是有监督学习的时候不就没有用了吗，因为这时候应该使用s_l.为了解决这个问题，我使用了一个标志位flag作为控制他们之间的使用，具体代码： flag*s\_i + (1-flag)*un\_s&emsp;&emsp;有标签的时候flag是1，表示使用s_l,无监督的时候flag是0，表示使用无监督损失函数。此时已经完成了判别器D损失函数的一部分设计，剩下的一部分和GAN中的D的损失一样，在代码中我给出了两种损失函数，一个是原始GAN的交叉熵损失函数，和DCGAN使用的一样，另外一个是improved wgan论文中使用的损失函数，但是在做了对比之后，我强烈建议使用DCGAN来做，improved wgan的损失函数虽然在生成结果的优化上有很大帮助，但是并不适合半监督学习中。 训练接下来就是训练部分：&emsp;&emsp;此时可能有一个疑问，我们是如何实现只使用200带标签的数据训练的，答案就在flag这个标志位里，在训练部分代码中，当迭代次数小于200的时候，flag=1, 此时表示使用s_l作为损失函数的一部分，当flag=0的时候，un_s起作用而s_l并没有起作用，这时，即使我们feed了正确的标签数据，但是s_l不起作用，就相当于没有使用标签。&emsp;&emsp;flag的作用本来是使用他控制feature matching是否工作的，因为这部分损失相当的大，后来发现影响不大，暂时就放在这里了。 测试12345678def test(self): count = 0. print 'testing................' for i in range(10000//self.batch_size): testx, textl = mnist.test.next_batch(self.batch_size) prediction = self.sess.run(self.prediction, feed_dict=&#123;self.x:testx, self.label:textl&#125;) count += np.sum(prediction) return count/10000. 测试精度结果变化图 本文实验代码使用GAN实现半监督学习代码https://github.com/LDOUBLEV/semi-supervised-GAN如果感觉有用的话，欢迎star， fork 备注&emsp;&emsp;详细代码请以github中为准，另关于结果不理想的问题，可能和之前做的迁移学习有关，下面是最近跑出来的结果，最好的精度是0.95，这个问题有时间会慢慢解决。另：链接中的模型精度是很高的，可以直接调用 参考文献 手把手教你用GAN实现半监督学习]]></content>
  </entry>
  <entry>
    <title><![CDATA[训练TensorFlow识别手写数字]]></title>
    <url>%2Fxun-lian-tensorflow-shi-bie-shou-xie-shu-zi.html</url>
    <content type="text"><![CDATA[TensorFlowSharp安装和使用入门(posted @ 2017-11-25 21:31)Tensorflow是一个人工智能框架。TensorflowSharp是对Tensorflow C语言版接口的封装，便于C#开发人员在项目中使用Tensorflow。 一、使用方法TensorflowSharp的使用很简单，首先使用NuGet安装TensorflowSharp包，然后新建C#控制台程序，输入下面代码，运行即可。123456789101112131415161718192021222324// 创建图var g = new TFGraph();// 定义常量var a = g.Const(2);var b = g.Const(3);// 加法和乘法运算var add = g.Add(a, b);var mul = g.Mul(a, b);// 创建会话var sess = new TFSession(g);// 计算加法var result1 = sess.GetRunner().Run(add).GetValue();Console.WriteLine("a+b=&#123;0&#125;", result1);// 计算乘法var result2 = sess.GetRunner().Run(mul).GetValue();Console.WriteLine("a*b=&#123;0&#125;", result2);// 关闭会话sess.CloseSession(); 运行后输出结果：12a+b=5a*b=6 二、注意事项 国内目前无法访问Tensorflow官网，但是可以访问谷歌提供的Tensorflow官网镜像。 国内使用NuGet安装TensorflowSharp很容易失败，可以直接从Nuget官网下载，然后改后缀名zip，解压后手工安装。 TensorflowSharp项目使用的.net版本必须高于4.6.1，本教程使用的版本是4.7.0，可以在属性选项卡中设置。 TensorflowSharp项目必须使用64位CPU，需要在属性选项卡生成中，去掉首选32位的勾选。 手动安装TensorflowSharp，处理要引用TensorFlowSharp.dll，还要将libtensorflow.dll复制到每个项目的输出目录。 三、相关网站Tensorflow教程：https://github.com/tengge1/learn-tensorflow-sharpTensorflow官网：http://www.tensorflow.orgGoogle Tensorflow镜像：https://tensorflow.google.cn/Tensorflow开源项目：https://github.com/tensorflow/tensorflowTensorflowSharp开源项目：https://github.com/migueldeicaza/TensorFlowSharpTensorflowSharp NuGet主页：https://www.nuget.org/packages/TensorFlowSharp/Tensorflow中文社区：http://www.tensorfly.cn/ 03 使用TensorFlow做计算题我们使用Tensorflow，计算((a+b)*c)^2/a，然后求平方根。看代码：12345678910111213141516171819202122232425262728import tensorflow as tf# 输入储存容器a = tf.placeholder(tf.float16)b = tf.placeholder(tf.float16)c = tf.placeholder(tf.float16)# 计算d = tf.add(a, b) #加法e = tf.multiply(d, c) #乘法f = tf.pow(e, 2) #平方g = tf.divide(f, a) #除法h = tf.sqrt(g) #平方根# 会话sess = tf.Session()# 赋值feed_dict= &#123;a:1, b:2, c:3&#125;# 计算result = sess.run(h, feed_dict= feed_dict)# 关闭会话sess.close()# 输出结果print(result) 这里让a=1，b=2，c=3，如果输出9.0，证明运行成功。Tensorflow做计算的方法是，先把计算的式子构建一个图，然后把这个图和赋值在cpu上一起运行，计算速度比较快。 04 TensorFlow中的常量、变量和数据类型打开Python Shell，先输入import tensorflow as tf，然后可以执行以下命令。Tensorflow中的常量创建方法：1hello = tf.constant('Hello,world!', dtype=tf.string) 其中，’Hello,world!’是常量初始值；tf.string是常量类型，可以省略。常量和变量都可以去构建Tensorflow中的图。 Tensorflow中变量的创建方法：1a = tf.Variable(10, dtype=tf.int32) 其中，10是变量初始值，tf.int32是变量的类型。Tensorflow中，主要有以下几种数据类型。tf.int8：8位整数。tf.int16：16位整数。tf.int32：32位整数。tf.int64：64位整数。 tf.uint8：8位无符号整数。tf.uint16：16位无符号整数。 tf.float16：16位浮点数。tf.float32：32位浮点数。tf.float64：64位浮点数。tf.double：等同于tf.float64。 tf.string：字符串。 tf.bool：布尔型。 tf.complex64：64位复数。tf.complex128：128位复数。 参考文献 《张量的阶、形状、数据类型》 05 TensorFlow中变量的初始化打开Python Shell，输入import tensorflow as tf，然后可以执行以下代码。1、创建一个2*3的矩阵，并让所有元素的值为0.（类型为tf.float）1a = tf.zeros([2,3], dtype = tf.float32) 2、创建一个3*4的矩阵，并让所有元素的值为1.1b = tf.ones([3,4]) 3、创建一个1*10的矩阵，使用2来填充。（类型为tf.int32，可忽略）1c = tf.constant(2, dtype=tf.int32, shape=[1,10]) 4、创建一个1*10的矩阵，其中的元素符合正态分布，平均值是20，标准偏差是3.1d = tf.random_normal([1,10],mean = 20, stddev = 3) 上面所有的值都可以用来初始化变量。例如用0.01来填充一个1*2的矩阵来初始化一个叫bias的变量。1bias = tf.Variable(tf.zeros([1,2]) + 0.01) 如果你想查看这些量具体的值，可以在Session中执行它并输出。12sess = tf.Session()print(sess.run(d)) 这里，我得到了以下的值：[[ 22.44503784 18.19544983 17.89671898 17.67314911 19.45074844 18.6805439 18.56541443 16.59041977 22.11240005 19.12819099]]。它就是上面4我们创建的量的值。 参考文献 《Tensorflow学习笔记（3）》 06 使用TensorFlow拟合x与y之间的关系看代码：1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tfimport numpy as np#构造输入数据（我们用神经网络拟合x_data和y_data之间的关系）x_data = np.linspace(-1,1,300)[:, np.newaxis] #-1到1等分300份形成的二维矩阵noise = np.random.normal(0,0.05, x_data.shape) #噪音，形状同x_data在0-0.05符合正态分布的小数y_data = np.square(x_data)-0.5+noise #x_data平方，减0.05，再加噪音值#输入层（1个神经元）xs = tf.placeholder(tf.float32, [None, 1]) #占位符，None表示n*1维矩阵，其中n不确定ys = tf.placeholder(tf.float32, [None, 1]) #占位符，None表示n*1维矩阵，其中n不确定#隐层（10个神经元）W1 = tf.Variable(tf.random_normal([1,10])) #权重，1*10的矩阵，并用符合正态分布的随机数填充b1 = tf.Variable(tf.zeros([1,10])+0.1) #偏置，1*10的矩阵，使用0.1填充Wx_plus_b1 = tf.matmul(xs,W1) + b1 #矩阵xs和W1相乘，然后加上偏置output1 = tf.nn.relu(Wx_plus_b1) #激活函数使用tf.nn.relu#输出层（1个神经元）W2 = tf.Variable(tf.random_normal([10,1]))b2 = tf.Variable(tf.zeros([1,1])+0.1)Wx_plus_b2 = tf.matmul(output1,W2) + b2output2 = Wx_plus_b2#损失loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-output2),reduction_indices=[1])) #在第一维上，偏差平方后求和，再求平均值，来计算损失train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 使用梯度下降法，设置步长0.1，来最小化损失#初始化init = tf.global_variables_initializer() #初始化所有变量sess = tf.Session()sess.run(init) #变量初始化#训练for i in range(1000): #训练1000次 _,loss_value = sess.run([train_step,loss],feed_dict=&#123;xs:x_data,ys:y_data&#125;) #进行梯度下降运算，并计算每一步的损失 if(i%50==0): print(loss_value) # 每50步输出一次损失 输出：0.4053480.009544850.00689250.005519580.004714530.004252060.004003820.003818830.003674450.003533490.003413250.003304870.003211280.003134680.00306460.00300140.002948020.002901790.00286180.00282344可以看到，随机训练的进行，损失越来越小，证明拟合越来越好。 参考文献 《Tensorflow 自带可视化Tensorboard使用方法 附项目代码》 《tensorflow学习（六）：tensorflow中的tf.reduce_mean()这类函数》 07 训练TensorFlow识别手写数字打开Python Shell，输入以下代码：12345678910111213141516171819202122232425262728293031323334import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 获取数据（如果存在就读取，不存在就下载完再读取）mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)# 输入x = tf.placeholder("float", [None, 784]) #输入占位符（每张手写数字784个像素点）y_ = tf.placeholder("float", [None,10]) #输入占位符（这张手写数字具体代表的值，0-9对应矩阵的10个位置）# 计算分类softmax会将xW+b分成10类，对应0-9W = tf.Variable(tf.zeros([784,10])) #权重b = tf.Variable(tf.zeros([10])) #偏置y = tf.nn.softmax(tf.matmul(x,W) + b) # 输入矩阵x与权重矩阵W相乘，加上偏置矩阵b，然后求softmax（sigmoid函数升级版，可以分成多类）# 计算偏差和cross_entropy = -tf.reduce_sum(y_*tf.log(y))# 使用梯度下降法（步长0.01），来使偏差和最小train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)# 初始化变量init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for i in range(10): # 训练10次 batch_xs, batch_ys = mnist.train.next_batch(100) # 随机取100个手写数字图片 sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) # 执行梯度下降算法，输入值x：batch_xs，输入值y：batch_ys# 计算训练精度correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) #运行精度图，x和y_从测试手写图片中取值 执行该段代码，输出0.8002。训练10次得到80.02%的识别准确度，还是可以的。说明：由于网络原因，手写数字图片可能无法下载，可以直接下载本人做好的程序，里面已经包含了手写图片资源和py脚本。(链接已失效) 参考文献 07 训练TensorFlow识别手写数字 《面向机器学习初学者的 MNIST 初级教程》 10 TensorFlow中模型保存与读取我们的模型训练出来想给别人用，或者是我今天训练不完，明天想接着训练，怎么办？这就需要模型的保存与读取。看代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import tensorflow as tfimport numpy as npimport os#输入数据x_data = np.linspace(-1,1,300)[:, np.newaxis]noise = np.random.normal(0,0.05, x_data.shape)y_data = np.square(x_data)-0.5+noise#输入层xs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])#隐层W1 = tf.Variable(tf.random_normal([1,10]))b1 = tf.Variable(tf.zeros([1,10])+0.1)Wx_plus_b1 = tf.matmul(xs,W1) + b1output1 = tf.nn.relu(Wx_plus_b1)#输出层W2 = tf.Variable(tf.random_normal([10,1]))b2 = tf.Variable(tf.zeros([1,1])+0.1)Wx_plus_b2 = tf.matmul(output1,W2) + b2output2 = Wx_plus_b2#损失loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-output2),reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)#模型保存加载工具saver = tf.train.Saver()#判断模型保存路径是否存在，不存在就创建if not os.path.exists('tmp/'): os.mkdir('tmp/')#初始化sess = tf.Session()if os.path.exists('tmp/checkpoint'): #判断模型是否存在 saver.restore(sess, 'tmp/model.ckpt') #存在就从模型中恢复变量else: init = tf.global_variables_initializer() #不存在就初始化变量 sess.run(init)#训练for i in range(1000): _,loss_value = sess.run([train_step,loss], feed_dict=&#123;xs:x_data,ys:y_data&#125;) if(i%50==0): #每50次保存一次模型 save_path = saver.save(sess, 'tmp/model.ckpt') #保存模型到tmp/model.ckpt，注意一定要有一层文件夹，否则保存不成功！！！ print("模型保存：%s 当前训练损失：%s"%(save_path, loss_value)) 大家第一次训练得到： 模型保存：tmp/model.ckpt 当前训练损失：1.35421模型保存：tmp/model.ckpt 当前训练损失：0.011808模型保存：tmp/model.ckpt 当前训练损失：0.00916655模型保存：tmp/model.ckpt 当前训练损失：0.00690887模型保存：tmp/model.ckpt 当前训练损失：0.00575491模型保存：tmp/model.ckpt 当前训练损失：0.00526401模型保存：tmp/model.ckpt 当前训练损失：0.00498503模型保存：tmp/model.ckpt 当前训练损失：0.00478226模型保存：tmp/model.ckpt 当前训练损失：0.0046346模型保存：tmp/model.ckpt 当前训练损失：0.00454276模型保存：tmp/model.ckpt 当前训练损失：0.00446402模型保存：tmp/model.ckpt 当前训练损失：0.00436883模型保存：tmp/model.ckpt 当前训练损失：0.00427732模型保存：tmp/model.ckpt 当前训练损失：0.00418589模型保存：tmp/model.ckpt 当前训练损失：0.00409241模型保存：tmp/model.ckpt 当前训练损失：0.00400956模型保存：tmp/model.ckpt 当前训练损失：0.00392799模型保存：tmp/model.ckpt 当前训练损失：0.00383506模型保存：tmp/model.ckpt 当前训练损失：0.00373741模型保存：tmp/model.ckpt 当前训练损失：0.00366922 第二次继续训练，得到： 模型保存：tmp/model.ckpt 当前训练损失：0.00412003模型保存：tmp/model.ckpt 当前训练损失：0.00388735模型保存：tmp/model.ckpt 当前训练损失：0.00382827模型保存：tmp/model.ckpt 当前训练损失：0.00379988模型保存：tmp/model.ckpt 当前训练损失：0.00378107模型保存：tmp/model.ckpt 当前训练损失：0.003764模型保存：tmp/model.ckpt 当前训练损失：0.00375149模型保存：tmp/model.ckpt 当前训练损失：0.00374324模型保存：tmp/model.ckpt 当前训练损失：0.00373386模型保存：tmp/model.ckpt 当前训练损失：0.00372364模型保存：tmp/model.ckpt 当前训练损失：0.00371543模型保存：tmp/model.ckpt 当前训练损失：0.00370875模型保存：tmp/model.ckpt 当前训练损失：0.00370262模型保存：tmp/model.ckpt 当前训练损失：0.00369697模型保存：tmp/model.ckpt 当前训练损失：0.00369161模型保存：tmp/model.ckpt 当前训练损失：0.00368653模型保存：tmp/model.ckpt 当前训练损失：0.00368169模型保存：tmp/model.ckpt 当前训练损失：0.00367714模型保存：tmp/model.ckpt 当前训练损失：0.00367274模型保存：tmp/model.ckpt 当前训练损失：0.00366843可以看到，第二次训练是在第一次训练的基础上继续训练的。于是，我们可以把我们想要的模型保存下来，慢慢训练。 参考文献 《TensorFlow使用指南》：http://www.tensorfly.cn/tfdoc/tutorials/mnist_tf.html TensorFlow中模型保存与读取：https://www.cnblogs.com/tengge/p/6379893.html 11 使用TensorBoard显示图片首先，下载一张png格式的图片（注意：只支持png格式），命名为1.png。然后，打开PythonShell，输入以下代码：1234567891011121314151617181920212223import tensorflow as tf# 获取图片数据file = open('1.png', 'rb')data = file.read()file.close()# 图片处理image = tf.image.decode_png(data, channels=4)image = tf.expand_dims(image, 0)# 添加到日志中sess = tf.Session()writer = tf.summary.FileWriter('logs')summary_op = tf.summary.image("image1", image)# 运行并写入日志summary = sess.run(summary_op)writer.add_summary(summary)# 关闭writer.close()sess.close() &emsp;&emsp;然后，在相同目录打开cmd，输入tensorboard —logdir=logs，然后打开浏览器输入http://localhost:6006/。在Tensorboard的Images标签页，就可以看到我们的png图片了。 参考文献 11 使用TensorBoard显示图片 12 使用卷积神经网络识别手写数字看代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 下载训练和测试数据mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)# 创建sessionsess = tf.Session()# 占位符x = tf.placeholder(tf.float32, shape=[None, 784]) # 每张图片28*28，共784个像素y_ = tf.placeholder(tf.float32, shape=[None, 10]) # 输出为0-9共10个数字，其实就是把图片分为10类# 权重初始化def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) # 使用截尾正态分布的随机数初始化权重，标准偏差是0.1（噪音） return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape = shape) # 使用一个小正数初始化偏置，避免出现偏置总为0的情况 return tf.Variable(initial)# 卷积和集合def conv2d(x, W): # 计算2d卷积 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): # 计算最大集合 return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')# 第一层卷积W_conv1 = weight_variable([5, 5, 1, 32]) # 为每个5*5小块计算32个特征b_conv1 = bias_variable([32])x_image = tf.reshape(x, [-1, 28, 28, 1]) # 将图片像素转换为4维tensor，其中二三维是宽高，第四维是像素h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# 第二层卷积W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# 密集层W_fc1 = weight_variable([7 * 7 * 64, 1024]) # 创建1024个神经元对整个图片进行处理b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)# 退出（为了减少过度拟合，在读取层前面加退出层，仅训练时有效）keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# 读取层（最后我们加一个像softmax表达式那样的层）W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2# 预测类和损失函数cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)) # 计算偏差平均值train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # 每一步训练# 评估correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.global_variables_initializer())for i in range(1000): batch = mnist.train.next_batch(50) if i%10 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_: batch[1], keep_prob: 1.0&#125;, session = sess) # 每10次训练计算一次精度 print("步数 %d, 精度 %g"%(i, train_accuracy)) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;, session = sess)# 关闭sess.close() 执行上面的代码后输出： Extracting MNIST_data/train-images-idx3-ubyte.gzExtracting MNIST_data/train-labels-idx1-ubyte.gzExtracting MNIST_data/t10k-images-idx3-ubyte.gzExtracting MNIST_data/t10k-labels-idx1-ubyte.gz步数 0, 精度 0.12步数 10, 精度 0.34步数 20, 精度 0.52步数 30, 精度 0.56步数 40, 精度 0.6步数 50, 精度 0.74步数 60, 精度 0.74步数 70, 精度 0.78步数 80, 精度 0.82 ………. 步数 900, 精度 0.96步数 910, 精度 0.98步数 920, 精度 0.96步数 930, 精度 0.98步数 940, 精度 0.98步数 950, 精度 0.9步数 960, 精度 0.98步数 970, 精度 0.9步数 980, 精度 1步数 990, 精度 0.9&emsp;&emsp;可以看到，使用卷积神经网络训练1000次可以让精度达到95%以上，据说训练20000次精度可以达到99.2%以上。由于CPU不行，太耗时间，就不训练那么多了。大家可以跟使用softmax训练识别手写数字进行对比。《07 训练Tensorflow识别手写数字》 参考文献 12 使用卷积神经网络识别手写数字]]></content>
  </entry>
  <entry>
    <title><![CDATA[Windows系统下Tensorboard显示空白的问题]]></title>
    <url>%2Fwindows-xi-tong-xia-tensorboard-xian-shi-kong-bai-de-wen-ti.html</url>
    <content type="text"><![CDATA[Tensorboard显示空白，或者graphs中显示“No graph definition files were found”，在数据正确的前提下，最可能是路径的问题。Windows 下通过cmd启动tensorboard，采用如下两种方法可以避免路径造成的问题（假设文件在D盘的logs文件夹下）：1.文件夹之间使用 // 分割1&gt;tensorboard --logdir=D://logs 2.将路径直接切换到文件的上一级目录下12&gt;cd D:&gt;tensorboard --logdir=logs]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习常见处理方法]]></title>
    <url>%2Fji-qi-xue-xi-chang-jian-chu-li-fang-fa.html</url>
    <content type="text"><![CDATA[img_size = mnist.train.images[0].shape[0]1234567891011121314151617181920import numpy as npimport tensorflow as tfimport pickleimport matplotlib.pyplot as plt#%matplotlib inlineprint("TensorFlow Version: &#123;&#125;".format(tf.__version__))from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('./MNIST_data/')i = 10img = mnist.train.images[i]plt.imshow(img.reshape((28, 28)), cmap='Greys_r')print("Label: &#123;&#125;".format(mnist.train.labels[i]))img_size = mnist.train.images[10].shape[0]print(img_size)print(img.shape) 执行上述语句返回的结果为：Label: 0784(784,) hidden1 = tf.layers.dense(noise_img, n_units)12345678910111213141516171819202122def get_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01): """ 生成器 noise_img: 生成器的输入 n_units: 隐层单元个数 out_dim: 生成器输出tensor的size，这里应该为32*32=784 alpha: leaky ReLU系数 """ with tf.variable_scope("generator", reuse=reuse): # hidden layer hidden1 = tf.layers.dense(noise_img, n_units) # leaky ReLU hidden1 = tf.maximum(alpha * hidden1, hidden1) # dropout hidden1 = tf.layers.dropout(hidden1, rate=0.2) # logits &amp; outputs logits = tf.layers.dense(hidden1, out_dim) outputs = tf.tanh(logits) return logits, outputs 上述代码中的”hidden1 = tf.layers.dense(noise_img, n_units)“其中：&emsp;noise_img-表示输入层的单元个数，这里使用了占位符表示，但要表明单元大小，个数为None&emsp;n_units-表示第一个隐藏层中神经元个数，这里的函数，表示是全连接，将输入层的神经元全部与第一个隐藏层的神经元，进行全连接。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python 批量修改文件名]]></title>
    <url>%2Fpython-pi-liang-xiu-gai-wen-jian-ming.html</url>
    <content type="text"><![CDATA[Python 批量修改文件名原文: Python3 OS 文件/目录方法批量修改文件名python 对文件进行批量改名用到的是 os 模块中的 listdir 方法和 rename 方法。 os.listdir(dir) : 获取指定目录下的所有子目录和文件名 os.rename(原文件名，新文件名）: 对文件或目录改名 把混乱的文件名改成有序的文件名:1234567891011121314151617181920import ospath=input('请输入文件路径(结尾加上/)：') #获取该目录下所有文件，存入列表中f=os.listdir(path)n=0for i in f: #设置旧文件名（就是路径+文件名） oldname=path+f[n] #设置新文件名 newname=path+'a'+str(n+1)+'.JPG' #用os模块中的rename方法对文件改名 os.rename(oldname,newname) print(oldname,'======&gt;',newname) n+=1 参考地址Python 批量修改文件名Python endswith()方法 搜索内容：img_rgb = img_bgr[…, ::-1]opencv-python几何变换Python中cv2库和matplotlib库色彩空间排布不一致Python cv2.COLOR_BGR2RGB() Examplesopencv-python的格式转换 RGB与BGR互转[opencv-python的格式转换 RGB与BGR互转] tensorflow 加载部分变量变量:创建、初始化、保存和加载tensorflow 恢复部分参数、加载指定参数TensorFlow学习笔记(9):模型存储与加载北京大学 软件工程博士在读tensorflow 加载部分变量的实例讲解TensorFlow中对训练后的神经网络参数（权重、偏置）提取Keras 中构建神经网络的 5 个步骤08-保存和载入模型，使用Google的图像识别网络inception-v3进行图像识别.md TensorFlow-变量保存和恢复]]></content>
      <categories>
        <category>机器学习</category>
        <category>数据集处理</category>
      </categories>
      <tags>
        <tag>机器学习，数据集处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse作为Python的开发工具]]></title>
    <url>%2Feclipse-zuo-wei-python-de-kai-fa-gong-ju.html</url>
    <content type="text"><![CDATA[简介Eclipse是一款基于Java的可扩展开发平台。其官方下载中包括J2EE方向版本、Java方向版本、C/C++方向版本、移动应用方向版本等诸多版本。除此之外，Eclipse还可以通过安装插件的方式进行诸如Python、Android、PHP等语言的开发。 其中PyDev是一个功能强大的 Eclipse插件，使用户可用 Eclipse 来进行 Python 应用程序的开发和调试。PyDev 插件的出现方便了众多的 Python 开发人员，它提供了一些很好的功能，如：语法错误提示、源代码编辑助手、Quick Outline、Globals Browser、Hierarchy View、运行和调试等等。 本文将介绍eclipse的pydev插件的安装与配置。 安装准备配置Python的系统环境变量PATH=D:\Python34（即python的安装目录）显示python的版本，用来测试python的环境是否配置好 , 命令：1Python 安装python的第三方库12pip install pandaspip install PyMySql 用来测试python运行时会不会出错，没报错就说明程序可以运行。 安装教程1. 首先下载jdk，安装，配置环境变量； 2. 再下载eclipse，配置编译器； 3. 写一个helloworld；（具体过程请专门看教程。） 这样，基础环境就搭好了。下面安装PyDev，有两种安装方法，分别介绍之： 简易在线安装这是官网安装，下载安装很方便，但可能由于网络原因用时过长或失败。 官网安装过程如下： 安装Pydev Help - Install New Software(1)点击 - Add(2)输入 Name 和 Location，Name 随意，Location 为 http://pydev.org/updates ； 点击OK把【connect all update sites during install to find required software】的勾选去掉，否则在安装新插件时会联网寻找所有可能的更新站点搜索，导致安装时间不可预估，并可能导致安装失败。 确定后可以看到一个Pending过程，然后得到如下图所示的插件：勾选后，点击Next进行安装。 不过，由于网络的原因，这种方法安装PyDev极有可能失败，提示网络连接错误等。需要vpn。 eclipse和python关联：eclipse菜单 -&gt; Windows -&gt;Preferences -&gt; PyDev-&gt; Interpreters - Python Interpreter.点击New按钮,选择python.exe的路径(第1步安装Python的路径),打开后显示出一个包含很多复选框的窗口,点OK结束！ 离线安装通常官网安装并不如人意，你懂的… 推荐离线安装，步骤如下 下载PyDev离线安装包，我的云盘地址： http://pan.baidu.com/s/1pJ1HQKb 将解压后的features和plugins两个文件夹分别拷贝到Eclipse安装目录下的features和plugins目录中 PyDev的配置不论官网安装还是离线安装，都需要配置PyDev，步骤如下 启动Eclipse，打开window-&gt;Preferences 选择Interpreter-Python，然后选择New 点击【New】，添加一个系统里已有的Python解释器的位置。确定后会经过短暂的处理，得到它的Libraries、Buildins等。(我写的地址是提前安装好的annaconda。) 最后，单击Reference界面下的 OK ; 等待后即可在Eclipse中写Python了。 例子 实战前面就已经配置好了Python的开发环境，下面新建一个项目，来测试一下，确实可以运行。 点击【File】-【New】-【Other】，找到【PyDev】，选择【PyDev Project】，点击Next。取一个项目名称，比如helloPython，如下图所示： 点击【Finish】，完成项目创建。然后你会进入PyDev视图，进行Python开发。 右键项目的src目录，选择【New】-【PyDev Package】，创建一个Python包，此处也命名为helloPython。 再右键该package，【New】-【PyDev Module】（python的module 就是java的calss），此处也命名为helloPython。 （pydev提供了一些模板，这边暂选 Empty） 双击打开 helloPython .py，添加如下代码。1print'hello python!' 右键项目，选择【Run As】-【Python Run】，或Ctrl+F11运行项目。 此时，可以在下方的console窗口，看到项目的运行结果。 PyDev的注释快捷键个人认为，PyDev是很好用的python编辑器。使用起来的确得心应手，建议初学者多尝试用用。 下面从网上整理了一些PyDev的快捷键，和大家分享分享：123456789101112131415161718Ctrl+3 行注释Ctr+\ 去行注释Ctrl+Shift+3 去行注释Ctrl+4 块注释Ctrl+5 去块注释Ctrl+9 折叠全部Ctrl+0 展开全部Ctrl+- 折叠Ctrl+= 展开Ctrl+Shift+Up 上一函数Ctrl+Shift+Down 下一函数Ctrl+Shift+O 整理导入顺序 tensorflow中所安装的模块 Python安装cv2模块 1pip install opencv-python 安装 captcha 1pip install captcha 常见问题PEP 263 — Defining Python Source Code Encodings 参考文献Eclipse环境安装Python插件PyDev 其他网址使用captcha模块生成图形验证码python使用captcha模块生成图形验证码 【简书】1.CNN图片单标签分类（基于TensorFlow实现基础VGG16网络）【简书】2.CNN图片多标签分类（基于TensorFlow实现验证码识别OCR）【CSDN】2.CNN图片多标签分类（基于TensorFlow实现验证码识别OCR） Tensorflow制作并用CNN训练自己的数据集]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Eclipse</category>
      </categories>
      <tags>
        <tag>Marchine Learning；Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习知识点总结]]></title>
    <url>%2Ftensorflow-xue-xi-zhi-shi-dian-zong-jie.html</url>
    <content type="text"><![CDATA[tf.equal()函数tensorflow 中tf.equal()用法：1tf.equal(A, B，name=None) 对于单个变量，如果A==B,则返回true,否则返回false; 对于数组，会迭代的比较A[i]和B[i]，对应相等的则返回true,否则返回false&emsp;&emsp;是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，否则返回False，返回的值的矩阵维度和A是一样的&emsp;&emsp;equal，相等的意思。顾名思义，就是判断，A, B 是不是相等，它的判断方法不是整体判断，&emsp;&emsp;而是逐个元素进行判断，如果相等就是True，不相等，就是False。&emsp;&emsp;由于是逐个元素判断，所以A，B 的维度要一致。12345678import tensorflow as tfimport numpy as np A = [[1,3,4,5,6]]B = [[1,3,4,3,2]] with tf.Session() as sess: print(sess.run(tf.equal(A, B))) 输出：&emsp;&emsp;[[ True True True False False]] tf.cast()函数tf.cast(a, dtype=tf.float32)将a的类型转换成tf.float32 12345678import tensorflow as tfa = [1, 2, 3]b = [1, 5, 3]sess = tf.Session()with sess.as_default(): acc = sess.run(tf.equal(a, b)) print(acc) print(sess.run(tf.reduce_mean(tf.cast(acc, dtype=tf.float32)))) 运行结果如下图所示：12[True, False, True]0.6666667 代码解释：假设a是输入的y_hat, b是预测的y&emsp;&emsp;tf.equal(a, b)会比较每一个y和y_hat 是否相等&emsp;&emsp;tf.cast(acc, dtype=tf.float32)将比较的结果转换成tf.float32类型&emsp;&emsp;tf.reduce_mean()求所有tf.float32类型结果的均值 tf.argmax()函数tf.argmax(vector, 1)&emsp;&emsp;返回的是vector中的最大值的索引号，如果vector是一个向量，那就返回一个值，如果是一个矩阵，那就返回一个向量，这个向量的每一个维度都是相对应矩阵行的最大值元素的索引号。123456789import tensorflow as tfimport numpy as np A = [[1,3,4,5,6]]B = [[1,3,4], [2,4,1]] with tf.Session() as sess: print(sess.run(tf.argmax(A, 1))) print(sess.run(tf.argmax(B, 1))) 运行结果：123(tf14) zhangkf@Ubuntu2:~/lenet5$ python one.py [4][2 1] tf.equal()tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，否则返回False，返回的值的矩阵维度和A是一样的12345678import tensorflow as tfimport numpy as np A = [[1,3,4,5,6]]B = [[1,3,4,3,2]] with tf.Session() as sess: print(sess.run(tf.equal(A, B))) 运行结果：12(tf14) zhangkf@Ubuntu2:~/lenet5$ python one.py [[ True True True False False]] 12345678import tensorflow as tfimport numpy as np A = [[1,3,4,5,6],[1,2,3,4,5]]B = [[1,3,4,3,2],[2,2,3,4,3]] with tf.Session() as sess: print(sess.run(tf.equal(A, B))) 运行结果：123(tf14) zhangkf@Ubuntu2:~/lenet5$ python one.py [[ True True True False False] [False True True True False]] 二者结合起来在测试模型的准确率的时候，通常二者结合在一起。12correct_prediction=tf.equal(tf.getmax(y,1),tf.getmax(y_,1))accuracy=tf.reduce_mean(tf.cast(corrcet_prediction,tf.float32))#求平均获取准确率 tf.argmax 与 tf.arg_maxtf.argmax 与 tf.arg_max 用法相同，下面介绍 tf.argmax 用法 tf.argmax12345678910111213141516def argmax(input, axis=None, name=None, dimension=None, output_type=dtypes.int64)numpy.argmax(a, axis=None, out=None) 返回沿轴axis最大值的索引。 Parameters: input: array_like，数组axis : int, 可选，默认情况下，索引的是平铺的数组，否则沿指定的轴。 out : array, 可选 如果提供，结果以合适的形状和类型被插入到此数组中。 Returns: index_array : ndarray of ints 索引数组。它具有与a.shape相同的形状，其中axis被移除。 tf.argmax() 与 numpy.argmax() 方法的用法是一致的 axis = 0 的时候返回每一列最大值的位置索引axis = 1 的时候返回每一行最大值的位置索引axis = 2、3、4 …，即为多维张量时，同理推断 例子1234567891011121314151617&gt;&gt;&gt; a = np.arange(6).reshape(2,3)&gt;&gt;&gt; aarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.argmax(a)5&gt;&gt;&gt; np.argmax(a, axis=0) #0代表列array([1, 1, 1])&gt;&gt;&gt; np.argmax(a, axis=1) #1代表行array([2, 2])&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(6)&gt;&gt;&gt; b[1] = 5&gt;&gt;&gt; barray([0, 5, 2, 3, 4, 5])&gt;&gt;&gt; np.argmax(b) #只返回第一次出现的最大值的索引1 tf.arg_max()&emsp;&emsp;The following is the source code of argmax&emsp;&emsp;(from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py).12345678910# pylint: disable=redefined-builtin# TODO(aselle): deprecate arg_maxdef argmax(input, axis=None, name=None, dimension=None): if dimension is not None: if axis is not None: raise ValueError("Cannot specify both 'axis' and 'dimension'") axis = dimension elif axis is None: axis = 0 return gen_math_ops.arg_max(input, axis, name) &emsp;&emsp;As you can see, argmax is using arg_max inside. Also from the code, I recommend using argmax because arg_max could be deprecated soon. Python获取文件夹下的文件和子文件夹笔者小白在写代码的时候遇到的这样的问题，就是说需要根据文件夹的路径获取该文件夹下面的所有的文件和子文件夹。这里就介绍python的os模块中的两个函数：os.walk() 、os.listdir()。 os.walk()该函数的原型是：1os.walk(top, topdown=Ture, onerror=None, followlinks=False) 该函数没有返回值。12345参数 1. top -- 根目录下的每一个文件夹(包含它自己), 产生3-元组 (dirpath, dirnames, filenames)【文件夹路径, 文件夹名字, 文件名】。 2. topdown --可选，为True或者没有指定, 一个目录的的3-元组将比它的任何子文件夹的3-元组先产生 (目录自上而下)。如果topdown为 False, 一个目录的3-元组将比它的任何子文件夹的3-元组后产生 (目录自下而上)。 3. onerror -- 可选，是一个函数; 它调用时有一个参数, 一个OSError实例。报告这错误后，继续walk,或者抛出exception终止walk。 4. followlinks -- 设置为 true，则通过软链接访问目录。 函数执行之后得到一个三元tupple（dirpath， dirnames， filenams。 dirpath：string，是当前目录的路径； dirnames：list， 是当前路径下所有的子文件夹名字； filenames：list， 是当前路径下所有的非目录子文件的名字。要获取完整的路径，dirnames和filenames是不包含路径信息的，可以使用1os.path.join(dirpath, dirnames) 获得文件的完整路径。代码示例：123456789#!/usr/bin/python# -*- coding: UTF-8 -*-import osfor root, dirs, files in os.walk(".", topdown=False): for name in files: print(os.path.join(root, name)) for name in dirs: print(os.path.join(root, name)) 当需要特定类型的文件时，代码如下：1234567891011# -*- coding: utf-8 -*- import os def file_name(file_dir): L=[] for dirpath, dirnames, filenames in os.walk(file_dir): for file in filenames : if os.path.splitext(file)[1] == '.jpg': L.append(os.path.join(dirpath, file)) return L 其中os.path.splitext()函数将路径拆分为文件名+扩展名，例如os.path.splitext(“E:/lena.jpg”)将得到”E:/lena“+”.jpg”。 os.listdir()os.listdir()函数返回指定路径下的文件和文件夹列表。代码示例：123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*-import os, sys# 打开文件path = "/var/www/html/"dirs = os.listdir( path )# 输出所有文件和文件夹for file in dirs: print file python 遍历文件夹及子文件夹12345678910111213141516171819import osdef EnumPathFiles(path, callback): if not os.path.isdir(path): print('Error:"',path,'" is not a directory or does not exist.') return list_dirs = os.walk(path) for root, dirs, files in list_dirs: for d in dirs: EnumPathFiles(os.path.join(root, d), callback) for f in files: callback(root, f)def callbakc1(path, filename): print(path+'\\'+filename)if __name__ == '__main__': EnumPathFiles(r'D:\Projects\python', callbakc1) python 遍历目录(包括子目录)下所有文件1234567891011def list_all_files(rootdir): import os _files = [] list = os.listdir(rootdir) #列出文件夹下所有的目录与文件 for i in range(0,len(list)): path = os.path.join(rootdir,list[i]) if os.path.isdir(path): _files.extend(list_all_files(path)) if os.path.isfile(path): _files.append(path) return _files 123456789101112131415161718192021222324252627282930_fs = list_all_files('./资料')#将第一阶段的文件遍历出来_k = filter(lambda x:re.compile(r'stage2.txt').search(x),_fs)``` python 便利文件夹，返回所有文件夹及其子文件夹``` python # -*- coding: utf-8 -*-# File Name:aa.py# Author: biolxy# E-mail:biolxy@aliyun.com# Created Time:Wed 14 Mar 2018 07:06:03 PM PDT# python getDir.py /home/user/soft 返回/home/user/soft 下所有文件夹及子文件夹名字import osimport reimport sys fistr_dir = os.path.abspath(sys.argv[1])#print("fistr_dir is:\t&#123;&#125;".format(fistr_dir))def get_all_dir_from_dir(dir): # 遍历文件夹，输出文件夹及子文件夹 if os.path.exists(dir): path_dir = os.path.abspath(dir) for i in os.listdir(path_dir): path_i = os.path.join(path_dir, i) if os.path.isdir(path_i): print(path_i) get_all_dir_from_dir(path_i) get_all_dir_from_dir(fistr_dir) 1python getDir.py /home/user/soft 注意，如果你没有改文件夹的访问权限，改脚本无法显示该文件夹 来源：从网上下载了一个听书课程，里边是按照时间顺序放在文件夹中，一级目录嵌套一级目录，具体的mp3文件放在三级目录中，通过编写py函数，实现对目录中文件的遍历并拷贝出指定的文件，此处是mp3文件。 编程思路：人都是惰性了，刚想到这个问题，第一时间是简单的一层一层的遍历，但是问题是不知道多少层，同时希望程序有一定的健壮性。就想到了递归思想。 具体编程实现12345678910111213141516import osimport shutil file = "D:/BaiduNetdiskDownload/2017-10"todir = "C:/mp3File" def searchMp3(path): for item in os.listdir(path): subFile = path+"/"+item if os.path.isdir(subFile): searchMp3(subFile) else: if os.path.splitext(subFile)[1] ==".mp3": shutil.copy(subFile ,todir)if __name__ == '__main__': searchMp3(file) 假设FILE文件夹中有三个文件夹分别是file1,file2。file1中有文件夹sf1和sf2.file2中有sf3和sf4。sf1-4中分别各有一个MP3文件，分别是m1.mp3—-m4.mp3。 path是FILE路径。开始循环： subFile = path/file1 因为subFile也是文件，此时把 subFile作为path传递给searchFile函数，记住，此时第一次循环还没有结束。此时path路径下面也有两个子文件，此时，subFile = path/file1/sf1。当然sf1也是文件，把此时的subFile当做参数传给searchMp3函数，记住，此时path/file下面还没有遍历完成。把path/file1/sf1当做path，此时，文件中有一个m1.mp3文件。不是文件夹，执行else语句，判断文件后缀是mp3则移动都指定目录。此时开始执行subFile = path/file1/sf2。同样的执行else语句。此时开始执行 subFile = path/file2同样的遍历过程。最后把所有的指定文件都遍历出来。如果还不理解，可以自己画图，更好的理解整个过程。 tf.one_hot()tensorflow中tf.one_hot()函数的作用是将一个值化为一个概率分布的向量，一般用于分类问题。(参考地址)具体用法以及作用见以下代码：12345678910111213import numpy as npimport tensorflow as tf SIZE=6CLASS=8label1=tf.constant([0,1,2,3,4,5,6,7])sess1=tf.Session()print('label1:',sess1.run(label1))b = tf.one_hot(label1,CLASS,1,0)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(b) print('after one_hot',sess.run(b)) 最后的输出为：1234567891011label1: [0 1 2 3 4 5 6 7]after one_hot: [[1 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0] [0 0 1 0 0 0 0 0] [0 0 0 1 0 0 0 0] [0 0 0 0 1 0 0 0] [0 0 0 0 0 1 0 0] [0 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 1]] tf.one_hot（）使用(参考地址)tf.one_hot在看conditionGAN的时候注意到label的输入要把它转换成one-hot形式，再与噪声z进行tf.concat输入，之前看的时候忽略了，现在再看才算明白为什么。123456789tf.one_hot( indices,#输入，这里是一维的 depth,# one hot dimension. on_value=None,#output 默认1 off_value=None,#output 默认0 axis=None,#根据我的实验，默认为1 dtype=None, name=None) 代码示例：1234567891011121314151617181920import tensorflow as tfimport numpy as npz=np.random.randint(0,10,size=[10])y=tf.one_hot(z,10,on_value=1,off_value=None,axis=0)with tf.Session()as sess: print(z) print(sess.run(y))[5 7 7 0 5 5 2 0 0 0][[0 0 0 1 0 0 0 1 1 1] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [1 0 0 0 1 1 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 1 1 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]] 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python3# -*- coding: utf-8 -*-import tensorflow as tfimport numpy as npimport osos.environ["CUDA_VISIBLE_DEVICES"] = "2"z=np.random.randint(0,10,size=[10])y=tf.one_hot(z,10,on_value=1,off_value=None)y1=tf.one_hot(z,10,on_value=1,off_value=None,axis=1)with tf.Session()as sess: print(z) print(sess.run(y)) print("axis=1按行排", sess.run(y1)) [6 3 4 9 6 5 5 1 2 1][[0 0 0 0 0 0 1 0 0 0] [0 0 0 1 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1] [0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 1 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0 0]]axis=1按行排 [[0 0 0 0 0 0 1 0 0 0] [0 0 0 1 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 1] [0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 1 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0 0]] 感觉实际用的时候可以不传入axis值。可以看到经过one-hot的处理，输入的维度变成了10×depth，值也变成了0和1. 下面说在condition GAN中要输入标签信息y，怎样处理的。y是mnist的标签值，0和10之间的整数，尺寸为[BATCH],经过one-hot处理后维度变成了[BATCH，10]值也是0和1，此时再与噪声z按列（axis=1）连接，变成条件GAN的输入。因此one-hot操作是必须的，这个处理在infoGAN中将z，categorical latent code、continuous latent code连接在一起输入也要用到。123y = tf.one_hot(y, 10, name='label_onehot')z = tf.random_uniform([BATCH, 100], -1, 1, name='z_train')tf.concat([z, y], 1) 【参考3】tensorflow中将label索引转换成one-hot形式12345678import tensorflow as tfindex=[0,1,2,3]one_hot=tf.one_hot(index,5) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(one_hot)) 12345[[0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.]]]]></content>
      <categories>
        <category>机器学习</category>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Python的校园网自动登录]]></title>
    <url>%2Fji-yu-python-de-xiao-yuan-wang-zi-dong-deng-lu.html</url>
    <content type="text"><![CDATA[搞了几天终于把这个搞出来了，代码不到50行，要是别的网站很好模拟，但是这个稍微有点麻烦，抓包工具用了很多，httpfox没有抓全，wireshark抓的又太多不好分析，最后还是使用神器fiddle才把它搞出来。 工具 火狐浏览器+firedebug插件，debug插件可在浏览器附加组件中添加，其他浏览器也可以，只要有可以监控浏览器的网络行为插件即可。（notes:这里没有使用到firedebug，而是使用火狐浏览器自带的审查工具） Python+requests包 步骤打开火狐浏览器，输入网址”http://211.85.192.115/srun_portal_pc.php?ac_id=1&amp;“，打开校园网登录页面，如下：登录账号和密码，显示登录成功右键单击【审查元素】-【网络】构造请求头12345678910111213141516171819202122# 构造头部信息 注意Cookie可能十分重要，而且Cookie会有过期时间（我们学校过期时间是1个月），过期之后，可能需要复制新的Cookie替换。post_header = &#123; #'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0', 'Accept': '*/*', #'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3', 'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2', 'Accept-Encoding': 'gzip, deflate', #'Content-Type': 'application/x-www-form-urlencoded', 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'X-Requested-With': 'XMLHttpRequest', #'Origin': 'http://wlrz.fudan.edu.cn', 'Referer': 'http://211.85.192.115/srun_portal_pc.php?ac_id=1&amp;', #'Content-Length': '112', 'Content-Length': '104', #'Cookie': 'login=YUtl4F5w2GWDfWUA8O0nj3eCm7TrrFp%252FtbchCKzbO84IQczKx%252Fyc5mYGG7s6FQxsyiZbjwUIcJ2ECcqXWO%252BzwX85KrsMq0MDW7tX1eoOzS00eusx19E0245ORqeeZHVwBzEd1DGI%253D', 'Cookie': 'login=请自己获得，这里不提供', #'Host': 'wlrz.fudan.edu.cn', 'Host': '211.85.192.115', 'Connection': 'keep-alive',&#125; 构造发送数据123456789101112131415161718192021#此处根据自己校园网Form Data中发送的数据进行更改action = 'login'username = '学号'#password = '加密的密码'password = '密码xxx'ac_id = '1'save_me = '0'ajax = '1'#user_ip = '127.131.1.1'post_data = &#123; 'action': action, 'username': username, 'password': password,# 'password': base64.b64encode(password.encode()).decode(),# 'password': base64.b64encode(password.encode()), 'ac_id': ac_id,# 'user_ip': user_ip, 'save_me': save_me, 'ajax': ajax&#125; 内容有方式、学号、密码还有其他，其中密码不是明文，这密码应该是加密的，在所有js文件中搜索password，发现有一处函数，验证了是base64加密方式。（注意：以下操作是在在Chrome浏览器中进行的）在chrome浏览器中，输入账号登录网址”http://211.85.192.115/srun_portal_pc.php?ac_id=1&amp;“，右键【审查元素】-【网络】，然后刷新于是开始着手写代码了：123456789101112131415161718def login_request(): if not is_net_ok(): print(&quot;[03]&#123;&#125; whpu-wlan is offline, request now...&quot;.format(datetime.datetime.now().strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)))# password = base64.b64encode(password.encode()).decode() #加密 try: # 发送post请求登录网页 result = requests.post(post_addr, data=post_data, headers=post_header) # z.text为str类型的json数据因此先编码成byte类型再解码成unicode类型这样就可以正常输出中文 s = result.text.encode(&apos;utf-8&apos;).decode(&apos;unicode-escape&apos;) print(s) print(&quot;login success!&quot;) except: print(&quot;[00]&#123;&#125; request error, whpu-wlan isnto connected to WIFI...&quot;.format(datetime.datetime.now().strftime(&apos;%Y-%m-%d %H:%M:%S&apos;))) else: print(&quot;[02]&#123;&#125; whpu-wlan is online, login sucesss...&quot;.format(datetime.datetime.now().strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)))login_request() 参考文献 (2016.7.18)python脚本实现自动登录校园网 (2017.10.24)python实现校园网自动登录 (2018.7.13)用python写服务器校园网上网认证脚本 (2018.11.24)基于python实现校园网自动登录]]></content>
  </entry>
  <entry>
    <title><![CDATA[证件之星软件个人加密狗完美破解心得]]></title>
    <url>%2Fzheng-jian-zhi-xing-ruan-jian-ge-ren-jia-mi-gou-wan-mei-po-jie-xin-de.html</url>
    <content type="text"><![CDATA[破解过程由于此软件程序为Delphi 所编写，并且无壳，直接载入OD，发现有狗，第一次打狗，不容易，如下图发现修改狗，轻松进入软件，但是通过使用软件后发现有以下四点问题： 点击&quot;一键完成&quot;会出现错误信息弹窗，点击确定软件关闭，如下图： 程序载入OD,右键搜索中文字符“软件遇到异常错误，即将关闭”，如下图，按图操作这个问题解决了，一键处理已经ok了但是还有以下三个问题没有处理。 由于此软件是Delphi编写的，利用其特征码给其按钮事件下断，载入OD，CTRL+G,转到00401000处,然后右键—-查找——-二进制字符串，输入我们的特征码740E8BD38B83????????FF93????????，在特征码的下面都会有一个call，我们就在call处下断点，之后寻找下一个（CTRL+L），直到显示为没有。为什么要下那么多断点，因为我们也不知道那一个会用上。那如果特征码的情况比较多怎么办，我的建议是使用脚本（脚本百度），能快速的下断点，程序运行起来，结果发现，断下来了，但是这三个按钮点击依然闪退，找不到真正的按钮事件地址，怎么办了呢？ 由于本菜鸟是新手，用了好多办法都不是很凑效，最后发现有这款软件，Delphi 事件到地址转换工具 v2.020 汉化版——可以查看Delphi 程序软件的按钮事件真实地址，实在太好了，于是就将程序载入Delphi 事件到地址转换工具中，终于找到了以OnClick 显示的按钮事件真实地址。 如下图说明：以此处理三个问题，载入OD，Ctrl+G，输入地址，点击ok来到真是地址，然后找到图中关键跳修改即可。 点击背景处理弹窗中的“处理”按钮，软件闪退，解决办法如下图。 点击照片保存弹窗中的“保存”按钮，软件闪退，解决办法如下图。 点击照片打印，软件闪退，解决办法如下图。到此完美爆破，本菜鸟还需努力学习，谢谢大家！ 下载地址源程序下载原程序下载： 官网下载 本地下载 爆破程序下载爆破程序下载： 百度网盘 链接:https://pan.baidu.com/s/1wOwV6tjtKZ5wI78dQJQKCA 提取码: kpsg 本地下载 Delphi事件到地址转换工具 v2.020Delphi 事件到地址转换工具 v2.020 汉化版下载，解压密码：RCFF Team： 百度网盘 链接:https://pan.baidu.com/s/1Y_hgYvlcCjA_dObcrWSa4A 密码:xihj 本地下载]]></content>
      <categories>
        <category>电脑</category>
        <category>破解心得</category>
      </categories>
      <tags>
        <tag>电脑; 破解心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10卸载或更改程序中不显示修改日期和类型]]></title>
    <url>%2Fwin10-xie-zai-huo-geng-gai-cheng-xu-zhong-bu-xian-shi-xiu-gai-ri-qi-he-lei-xing.html</url>
    <content type="text"><![CDATA[Windows7 卸载或更改程序 中不显示时间和大小急求！win10控制面板的卸载软件，都不显示安装日期了有个软件我安装了新版本，准备按安装日期卸旧版本。上次关机前都是正常显示安装日期的，打开就成这样了，混在一起（点击日期全都变成一天的了） 问题描述 解决方法方法1在列表的空白处右键，在弹出的菜单中选择：“分组依据”，“更多”，勾选“安装时间”，最后点击“确定”即可。 方法2在名称栏上右击 将“安装时间”打勾即可 如下图所示：]]></content>
      <categories>
        <category>电脑</category>
        <category>常见故障</category>
      </categories>
      <tags>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BaiduPCS-Go的安装及使用[CMD]]]></title>
    <url>%2Fbaidupcs-go-de-an-zhuang-ji-shi-yong.html</url>
    <content type="text"><![CDATA[你是否经常对百度网盘非VIP的几十K下载速度而痛恨不已？没错，百度就是无赖，VIP会员下载速度可以达到2MB/s，甚至更高，普通用户不加速也就算了，还限速！不能忍！其实，我个人对于百度的产品是十分抗拒的，因此平时基本不用百度的产品，但是也有例外的时候，比如百度网盘。因为对电影的画质要求比较高，所以一些1080P的电影动辄6、7个G，所以普通的网络下载是特别慢的，而且这些资源往往以种子的形式存在，于是经过摸索终于找到一个下载很快的方法，这里推荐给大家。当然，这个方法不仅适用于下载电影，任何保存于百度网盘的文件使用这种方法都可以达到不亚于VIP的速度，甚至顶速（具体情况取决于你使用的网络速度），关键是免费！ 注：这个教程里会涉及到一些非常简单的命令行操作，但是非程序员请不要抵触或者害怕，毕竟带来的便利是可观的，来个效果图感受一下： BaiduPCS-Go是用Go语言写的一个开源的小工具，专门用于突破百度对于非VIP用户对百度网盘下载速度的限制。其项目源码地址如下：https://github.com/iikira/BaiduPCS-Go 。在其readme文件中，对于软件的使用做了很详细的介绍，有兴趣的人可以阅读一下，我这里只介绍最基本的安装和使用方法。 特色 这是一个仿Linux命令行的百度网盘客户端。（是的，不限速） 注意： 文件名和目录名可用通配符补全(使用代替n个字符，n&gt;=0)。123# 示例：d /a_directory/b_directory/c_file.txt =d /a*/b*/c* 特点： 跨平台（Windows，macOS，Android等） 支持多账户。 网盘内列出文件和目录, 支持通配符匹配路径; 下载网盘内文件, 支持网盘内目录 (文件夹) 下载, 支持多个文件或目录下载, 支持断点续传和高并发高速下载。 离线下载，支持http/https/ftp/电驴/磁力链协议。 好玩，不过没有一点Linux基础，就不怎么好玩了。 软件下载及安装项目地址: iikira/BaiduPCS-Go下载地址: iikira/BaiduPCS-Go releases a. Android/macOSAndroid：需要用到 Termux 或 NeoTerm 类软件。这里只提一下。macOS：没有macOS的设备，这里属于凑字数。 b. Windows下载地址: 64位系统：BaiduPCS-Go-v3.3.3-windows-x64.zip 32位系统：BaiduPCS-Go-v3.3.3-windows-x86.zip GitHub地址： https://github.com/iikira/BaiduPCS-Go/releases 下载说明： 请按照上述说明下载对应的版本，我只测试了windows和linux的机器，其他系统暂时没有测试。 对于windows系统，确认系统类型的方法：右键点击“我的电脑” -&gt; “属性” -&gt; “系统类型” 该软件是绿色软件，下载完成后请直接解压到你的自己的软件目录即可。 软件的使用该软件的使用方法也很简单，在Linux下和Windows下的方法一模一样，只不过程序的名字有一点差异（Windows下，软件的名字叫做“BaiduPCS-Go.exe”，Linux下，软件的名字叫做“BaiduPCS-Go”）。以下以Windows系统为例讲解软件的使用。 程序运行下载后解压缩，双击 BaiduPCS-Go.exe。界面如下：123456----BaiduPCS-Go - 百度网盘客户端 for windows/amd64 ....... ....... .......BaiduPCS-Go &gt; 界面很长，这里用省略号代替。 登录/退出/切换123login # 登录logout # 退出当前账户su/chuser # 切换账户 在命令行窗口中输入 login ，再根据提示输入账号和密码，即可登录百度账号。 还有其他登录方式，如 login -bduss=。(获取bduss) logout 和 su / chuser 的用法也比较简单。 在使用前，我们首先要登录百度账号，只要不手动退出账号，以后可以直接使用，而不必每次都登录。首先，打开命令行，打开命令行的方式有两种： 菜单打开屏幕左下角“开始” -&gt; “所有程序” -&gt; “附件” -&gt; “命令提示符” 快捷键打开按下键盘上的Win（显示微软图标的那个键）+R， 在弹出的窗口输入”cmd”，然后按下回车 接下来我们需要进入刚才解压好的软件目录，比如我的路径为：C:\Users\User\Downloads\BaiduPCS-Go-v3.5.6-windows-x64，那么在刚弹出的窗口中输入，并按下回车：1cd C:\Users\User\Downloads\BaiduPCS-Go-v3.5.6-windows-x64 然后开始输入命令登录百度账户，在窗口中输入：1BaiduPCS-Go.exe login 然后按照下图操作即可登录： 在显示成功登陆后，我们就可以关掉这个窗口了。然后双击BaiduPCS-Go.exe这个文件就可以进行下一步的操作了。如果要退出账号，只需要在这个窗口中输入logout即可。 切换目录12345cd -l /Path/To/File# cd = change directory = 切换目录。# /Path/To/File = 文件路径，绝对路径或相对路径。# -l: 显示目标目录下的子文件及子目录。 例：cd /a/b/c/d 查看当前账户及已登录账户1loglist 查看文件 查看文件命令ls (list) 切换目录默认情况下，打开之后执行ls看到的文件就是你百度网盘最顶层目录，如果想切换目录的话，执行以下命令 1cd xxx 其中xxx是你想切换的文件夹名。 切换到上一级目录使用下面的命令可以切换到上一级目录：1cd .. 下载文件下载文件的命令如下:1download -p 1000 xxx 其中xxx是你要下载的文件名，如下图所示: 上传文件上传文件时，需要打开命令行来操作。上传文件的命令如下：1BaiduPCS-Go upload xxx yyy 其中xxx是你要上传的本地文件名，yyy是你百度网盘下的目录名，比如我要把我本地放在C:\Users\User\Downloads下的一个叫做Git-2.18.0-64-bit.exe的文件传到百度网盘的/Softwares/Tools目录下。命令如下： 1BaiduPCS-Go.exe upload C:\Users\User\Downloads\Git-2.18.0-64-bit.exe /Softwares/Tools 然后，我们就能看到如下的结果：1这里要注意的是，windows系统下，本地文件的路径名书写要用\，而百度网盘路径名书写要用/. web功能1web #启用web功能 输入上述命令后，在浏览器栏输入localhost:8080，就可以在本地查看你的网盘目录。 很不错的功能。 帮助菜单1h 输入 h ，可以看到帮助菜单。这里做成表格，以便观看1用法：输入 h ，可以看到帮助菜单。这里做成表格，以便观看 COMMANDS 含义 tool 工具箱 help / h Shows a list of commands or help for one command 其他： run 执行系统命令 sumfile / sf 获取文件的秒传信息 web 启用 web 客户端 (测试中) 百度账号： login 登录百度账号 loglist 获取当前帐号, 和所有已登录的百度帐号 logout 退出当前登录的百度帐号 su / chuser 切换已登录的百度帐号 百度网盘： cd 切换工作目录 cp 拷贝(复制) 文件/目录 download / d 下载文件或目录 ls / l / ll 列出当前工作目录内的文件和目录 或 指定目录内的文件和目录 meta 获取单个文件/目录的元信息 (详细信息) mkdir 创建目录 mv 移动/重命名 文件/目录 offlinedl/clouddl / od 离线下载 pwd 输出当前所在目录 (工作目录) quota 获取配额, 即获取网盘的总储存空间, 和已使用的储存空间 rapidupload / ru 手动秒传文件 rm 删除 单个/多个 文件/目录 upload / u 上传文件或目录 配置: config 显示和修改程序配置项 GLOBAL OPTIONS: 全局选项 –verbose 启用调试 [%BAIDUPCS_GO_VERBOSE%] –help / -h 查看帮助 –version / -v 查看版本号 常见问题 关于BaiduPCS-Go不能用问题解决，报错【获取目录下的文件列表 遇到错误, 远端服务器返回错误】BaiduPCS-Go不能使用报错：获取目录下的文件列表 遇到错误, 远端服务器返回错误, 代码: 4, 消息: No permissionto do this operation, 路径: /设置新的appid!：目前已知可用APP id：266719 在软件输入 config set -appid=266719。 注意以下问题：错误代码403/31066 造成无法下载问题 下载路径和文件命名不带空格符号。 检查文件路径是否过于复杂(如果实在不行就直接下载整个文件夹) 如果还是不行，那就换别的账号登录吧 参考文献 [教程]BaiduPCS-Go BaiduPCS-Go的安装及使用]]></content>
      <categories>
        <category>电脑</category>
        <category>百度云下载</category>
        <category>BaiduPCS-Go[CMD]</category>
      </categories>
      <tags>
        <tag>电脑;百度网盘;BaiduPCS-Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于BaiduPCS-Go打造的Web界面]]></title>
    <url>%2Fji-yu-baidupcs-go-da-zao-de-web-jie-mian.html</url>
    <content type="text"><![CDATA[使用说明github地址: https://github.com/liuzhuoling2011/baidupcs-web 腾讯视频: 地址 下载链接: https://github.com/liuzhuoling2011/baidupcs-web/releases 3.6.5 版本发布: 修复由于密码访问带来的安全隐患，强烈建议更新 增加只可以本地访问的模式 后台增加配置下载设置的API（页面暂时还没加入） 修复无法上传文件名含有 [ 字符的文件 新增公众号二维码， 更方便及时交流 3.6.4 版本发布: 支持设置工作目录，之后可以规避错误 “无法下载 返回“遇到错误, 远端服务器返回错误, 代码: 31326, 消息: user is not authorized, hitcode:123” 参考链接 https://github.com/iikira/BaiduPCS-Go/issues/460 3.6.3 版本发布: 修复了移动端页面显示异常的bug 3.6.2 版本发布: 页面针对包含大量文件的文件夹处理，上传下载列表进行优化，解决页面卡死问题，打开速度提升十倍左右 文件列表排序选项写入localstorage，长久有效 在Windows和Mac下程序打开时调用默认浏览器打开链接 http://localhost:5299 3.6.1 版本发布: 添加移动端UI 新增支持BDUSS登录 3.5.9 版本发布: 根据baidupcs-go的后台项目更新升级优化了下载, 上传 新增回收站操作(暂时回收站删除单个文件会出问题, 不过不影响) 新增重命名操作, 添加了列视图下文件删除和下载的快捷操作 修复下载文件夹时文件夹任务残留的bug 修复appid错误设置之后打开设置界面为空白的问题 修复了设置界面输入特殊符号无效的问题 附加说明： 之前如果使用过BaiduPcs软件, 可能会出现登录后加载文件列表会卡死，或者遇到错误, 远端服务器返回错误, 代码: 4, 消息:&quot;No permission to do this operation&quot; 可以试试, 在右上角的设置里面把 PCS应用ID 设置为 266719 就可以正常使用了 &quot;目前百度是针对账号进行限速，当一个非会员账号下载量达到一定阈值就会触发限速。账号被限速之后容易出现下载错误、掉连接数等问题，需要过几天或者开通会员才会恢复。&quot; 这个是需要百度云账号登陆的， 不用太担心安全什么的，相信开源的力量吧~ 参考文献 [WEB]基于BaiduPCS-Go打造的Web界面, 让你高效的使用百度云]]></content>
      <categories>
        <category>电脑</category>
        <category>百度云下载</category>
        <category>BaiduPCS-Go</category>
      </categories>
      <tags>
        <tag>电脑;百度网盘;BaiduPCS-Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爱字幕app破解VIP教程]]></title>
    <url>%2Fai-zi-mu-app-po-jie-vip-jiao-cheng.html</url>
    <content type="text"><![CDATA[这次是破解爱字母app的VIP教程破解教程仅供研究参考，请勿用于商业用途 软件有360加固哦 自行脱壳本次只会给出核心破解版 此次教程是用小米商店下载的最新版爱字幕1.5 完美没水印 解锁VIP 破解前 破解后 破解步骤 首先脱壳得到dex 把除了软件的类全部删除 搜索isVip 全部赋值1 重命名为classes2添加进apk保存返回安装看效果 下载地址成品下载地址：http://t.cn/EIu1EDR成品本地下载地址2 参考文献【简单】爱字幕app破解VIP教程https://www.52pojie.cn/thread-889438-1-1.html(出处: 吾爱破解论坛)]]></content>
      <categories>
        <category>Android</category>
        <category>逆向分析</category>
      </categories>
      <tags>
        <tag>Android; 逆向分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爱奇艺万能播放器 绿色修改 & 高速百度云下载]]></title>
    <url>%2Fai-qi-yi-wan-neng-bo-fang-qi-lu-se-xiu-gai-gao-su-bai-du-yun-xia-zai.html</url>
    <content type="text"><![CDATA[目前爱奇艺属于百度旗下，有细心的网友发现爱奇艺万能播放器竟然能不限速的下载百度网盘的资源！启动播放器，右上角可以看见百度网盘的标志。2018/11/18 17:00 之前下载的网友重新下载即可解决问题。百度云高速下载速度不错： 测试效果百度云下载功能可用，2018/11/18 17:00测试效果： 绿化步骤使用须知，绿化步骤： 百度云功能启用百度云功能启用： 设置图片关联功能设置图片关联功能：（可设置为默认查看器，菜单里找）设置步骤:修改特点： # 去除强制更新。 # 去除多余菜单，界面搜索框。 # 禁止生成无用文件及文件夹。 # 批处理进行 绿化、卸载。 使用指南目前爱奇艺属于百度旗下，有细心的网友发现爱奇艺万能播放器竟然能不限速的下载百度网盘的资源！启动播放器，右上角可以看见百度网盘的标志。然后登录你的百度网盘账号找到你要下载的资源即可下载啦！这个功能目前不支持同时下载多个任务，不支持下载文件夹；如果界面右上角没出现百度网盘功能的按钮，请重启播放器！ 更新日志：5.1.55.4941 (2018.12.05)更新日志:1、新增暂停时逐帧快进功能2、优化程序启动速度3、优化播放器定时功能4、优化看图模式图片缩放效率5、其他各种问题修正及细节优化 官方下载地址官方下载地址：爱奇艺万能播放器 v5.1.55.4941http://mbdapp.iqiyi.com/j/ot/GeePlayerSetup_onlineupdate_201812051603.exe 更新介绍及下载2018/11/18 更新特点：没有集成 百度云下载浏览器 ，点击 百度云图标 会自动下载！！！ 2019/11/19 使用说明：1、有网友反映，在线播放 打开URL 功能不能使用，经体验，官方版也不能使用。2、百度云功能正常能用，不能使用的网友，确定下载的是18号更新的版本，首次打开需等待百度云加载完成。3、看到有网友反馈限速，这个问题我解决不了。因为我这里的下载速度大家也看到了，网络是 电信网络。对于限速的网友，可以尝试换一下网络环境试试，或者 换一个 百度云账号看一下。另外，这款软件只能单个下载，无法批量下载。 2018/11/19 15:00 做最后更新：1、修复由于之前修改过度，导致 点击 设置关联 的时候不能设置关联以及闪退问题。2、精简一些更新有关的文件（软件已禁止更新）。 2018/11/23 14:00 完善批处理:1、完善卸载批处理，卸载后几乎不留痕迹。 爱奇艺万能播放器 3.2.49.4280 下载链接：(2018/11/23 14:00 更新)链接: https://pan.baidu.com/s/1pNt3Cr6WmZrukN8P-tbdpg 提取码: pyev 2018/11/18 更新到最新版：爱奇艺万能播放器 5.1.53.4745 下载链接：(2018/11/23 14:00 更新)链接: https://pan.baidu.com/s/1WC5pCtnmTo8F8AouufNU1Q 提取码:dqv5 两个版本比较：1、从百度云下载来看没啥区别2、新版貌似多着爱奇艺视频助手 功能，但是这个功能开启后，安装体积100M多点。3、没啥特殊需求的话，两个版本随意用。 当然，想必有网友问，有没有爱奇艺视频助手 提取版，答案是：必须有。 爱奇艺视频助手爱奇艺视频助手 7.5.3.15 修改特点： 禁止检查更新 去除反馈等菜单 爱奇艺视频助手 软件特色：图片MV ，功能： 转码功能转码功能：支持各种视频转换为mp4\mkv\flv格式 2018/11/23 14:00 完善批处理:1、完善卸载批处理，卸载后几乎不留痕迹。爱奇艺视频助手 7.5.3.15 下载链接：(2018/11/23 14:00 更新)链接:https://pan.baidu.com/s/1Cj37KNN_w-WqlG9vqpDbOw 提取码: 8yki 问答 对于不显示 百度云 图标的网友，尝试重新启动软件3次左右，每次在联网状态下停留1分钟左右。 参考文献 参考111/23（完善批处理）修复百度云 爱奇艺万能播放器 绿色修改 &amp; 高速百度云下载https://www.52pojie.cn/thread-823826-1-1.html 参考2爱奇艺万能播放器 v5.1.55.4941 支持度盘高速下载https://www.52pojie.cn/thread-834449-1-1.html]]></content>
      <categories>
        <category>电脑</category>
        <category>百度云下载</category>
        <category>爱奇艺万能播放器</category>
      </categories>
      <tags>
        <tag>电脑; 百度云下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome打开开发者工具（F12）之后看不到请求头信息]]></title>
    <url>%2Fchrome-da-kai-kai-fa-zhe-gong-ju-f12-zhi-hou-kan-bu-dao-qing-qiu-tou-xin-xi.html</url>
    <content type="text"><![CDATA[开发者工具的network窗口能够查看浏览器的请求头以及响应头信息，但是有时打开开发者工具会遇到像下图这种情况 找不到请求信息，无法查看请求头，或响应头。 解决方法：1.点击“Filter”按钮，也就是图中的漏斗形状的按钮。下面会多出一系列选项，如图 2.点击“All”，然后按“F5”刷新网页。之后在下方会出现资源文件，如图 3.然后在name一栏中点击资源文件，在右侧会弹出显示框，点击“Header”，就可以看见请求头了 参考文献： chrome打开开发者工具（F12）之后看不到请求头信息(CSDN; Blog)]]></content>
      <categories>
        <category>Chrome</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Cookies登录网站]]></title>
    <url>%2Fshi-yong-cookies-deng-lu-wang-zhan.html</url>
    <content type="text"><![CDATA[推荐两个谷歌浏览器的插件:123https://chrome.google.com/webstore/detail/simple-extension/ofhbnimjijmnaigdfhhmhegnlmcbilba https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg 效果说明： 第一个可以自定义浏览器的信息，我多个百度云账号可以实现点击即切换。 第二个是很早之前用的一个插件，可以导入、导出、锁定cookies。 插件介绍： 第一个插件的开源网址 1https://github.com/gzlock/simple-extension 第二个：EditThisCookie，是一个国外写的， 12http://www.cnplugins.com/devtool/editthiscookie/detail.htmlhttp://chromecj.com/productivity/2014-07/8/download.html]]></content>
      <categories>
        <category>Chrome</category>
      </categories>
      <tags>
        <tag>Chrome; Cookies</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离线下载Chrome crx扩展插件]]></title>
    <url>%2Fchi-xian-xia-zai-chrome-crx-kuo-zhan-cha-jian.html</url>
    <content type="text"><![CDATA[使用 Chrome 应用商店下载 crx 插件时，一下载完，Chrome 浏览器就会默认进行安装。如果是我们自己浏览器要安装这个插件，那么直接安装没啥问题，只是有时候我们想把这个插件下载下来分享给其它朋友，所以就会出现，如果我们使用 Chrome 浏览器直接下载 crx 拓展程序，一下载就是直接提示安装，而没办法将这个拓展程序导出来。 那么如何解决?下面以下载 Chrome 下有道词典Chrome划词插件为例，介绍如何离线下载 crx 。 Crx离线下载方法一首先访问你要下载的插件，比如这里我要下载有道词典 Chrome 划词插件这个拓展，插件地址： https://chrome.google.com/webstore/detail/有道词典chrome划词插件/eopjamdnofihpioajgfdikhhbobonhbb 然后复制出最后 32 位的应用id，这个地址的应用id为: eopjamdnofihpioajgfdikhhbobonhbb 如下截图(如果图片模糊，点击图片查看大图)： 然后使用其它浏览器（非Chrome浏览器）访问 :https://coderschool.cn/crx/dowcrx.html 访问界面如下： 把上面的 32 位应用 id 复制到上图的文本框里面，然后点击生成，接着鼠标在步骤二的蓝色文字那边右键另存为即可：然后就可以顺利的下载和保存插件了。 上面下载方法解析其实真正的下载地址是这个地址： https://clients2.google.com/service/update2/crx?response=redirect&amp;x=id%3D要下载的应用id%26uc 把上面链接的要下载的应用id替换成32位应用id，然后访问这个链接即可进行下载。上面的操作都需要你能正常访问谷歌 Chrome 应用商店。 Crx离线下载方法二方法和上面方法一介绍的基本一致。 首先访问 https://chrome-extension-downloader.com/，访问后站点截图： 然后把你要下载插件的整个 url 地址或者应用 id 复制到文本框即可进行下载，如下： Crx离线下载方法三http://yurl.sinaapp.com/crx.php 参考文献离线下载 Chrome crx 拓展插件 本文来源于：技术拉近你我！。原文标题: 离线下载 Chrome crx 拓展插件原文链接：https://coderschool.cn/2396.html]]></content>
      <categories>
        <category>Chrome</category>
      </categories>
      <tags>
        <tag>Chrome; crx插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逆向一个易语言DLL的加密算法]]></title>
    <url>%2Fni-xiang-yi-ge-yi-yu-yan-dll-de-jia-mi-suan-fa.html</url>
    <content type="text"><![CDATA[逆向一个易语言DLL的加密算法。关键算法逆出来了，但是没有看懂属于什么算法。 前言这个DLL是一个授权的，填入激活码之后会访问自己的后台验证。我也是闲来无事，拿来学习学习。如果作者看到了，并非恶意。先来说下我如何进行逆向算法的。因为易语言的DLL脱离不了核心支持库。核心支持库是有特征码的，不管程序怎么加密，核心支持库是加密不了的。我们可以用核心支持库的特征 来进行逆向算法。 程序和DLL的核心支持库都是独立存在的。 DLL用的是DLL编译时的核心支持库。 过程其中的链接 我就拿 http://www.baidu.com 为例 前面的加密算法 是通过核心支持库逆出来的。 程序启动后是不会直接加载DLL的。需要使用过DLL的函数后 才会加载。等加载完DLL之后 我们进入DLL层 双击这个DLL 就可以进入DLL层了。 然后就是通过特征码来在DLL层下断点。这里我用的的工具 知道edebug找到核心支持库的地址。 再通过这个特征 查找特征的位置的位置。在这里下断。 RC4和DES加密处也要下断，为了就是看看 没有没有使用易语言自带的加密 为了就是看看有没有使用核心支持库里的加密算法。做的动作有，先获取13位时间戳 然后把最后3位改成000 解密出来的是我填入进去的激活码。 继续运行。运行几次之后 会出来二次解密。 我写着写着 居然忘了二次解密的密码是怎么找到的了。。。 密码是 sdvscv;lwe,r[xc/bvfsd/w 再用自己的激活码进行二次解密。 密文|密文的MD5|X 分割 取密文 再将解密出来的密文 进行MD5和程序计算的MD5是否一致， 关键 关键来了。 密文的长度位128位 其实这个密文是16进制的， 转到字节集后 进行倒序。1a2b3c4d5f -&gt; 5f4d3c2b1a 然后就是进行3次解密， 因为解密出来的是128位 就是 moc.udiab.www//:ptth +字节 {0}+ 后面的107长度字节集 （后面107位不知道是做什么用的。） 关键解密 的特征码为↓↓↓ 55 8B EC 81 EC 24 00 00 00 C7 45 FC 00 00 00 00 C7 45 F8 00 00 00 00 68 05 00 00 80 6A 00 8B 5D0C 8B 03 85 C0 最后再将倒序的链接 矫正之后 就是真正的后台授权地址。（http://www.baidu.com） 参考文献逆向一个易语言DLL的加密算法。关键算法逆出来了，但是没有看懂属于什么算法。https://www.52pojie.cn/thread-886009-1-1.html(出处: 吾爱破解论坛) 相关下载链接: Desktop.rar https://pan.baidu.com/s/1RjcYrrmNnXyY0zsR9ml_Gw 提取码: 4n7g]]></content>
      <categories>
        <category>电脑</category>
        <category>逆向分析</category>
      </categories>
      <tags>
        <tag>电脑；逆向分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安卓逆向分析笔记---002]]></title>
    <url>%2Fan-zhuo-ni-xiang-fen-xi-bi-ji-002.html</url>
    <content type="text"><![CDATA[这次与上次的软件很像，不过这次里面是输入的注册码~ 【所需工具】Androidkiller、Unicode工具 【用例下载链接】链接：(简单2)https://pan.baidu.com/s/1wsoKyF4HvTdfq3N2zWIlIw 提取码：bnp1 【开始操作】 与上次一致，将APK拖到AndroidKiller里面，反编译完成后点击“工程管理器”，与上次分析一样，双击打开AndroidManifest.xml，如下图 根据路径找到第一启动类，点开Java源码看代码，如下图 实际上我们已经可以看到注册码就是“写个CM都很难啊”,当然可以直接输入这个注册成功，但是我希望的是什么都不输入就能注册成功，所以我打算去改判断的内容，我在MainActivity.smali里面没有看到相关的Unicode码，所以只能搜索，我先打开Unicode工具将以下几项设置好，输入“写个CM都很难啊”，如下图 将Unicode码区的内容复制，重新返回AndroidKiller，点击左侧工程搜索，在字符框粘贴复制的内容，下面设置成如下图所示，点击搜索 emmmmmm…，换个字符串试试 搜索到一行数据，点开打开看看 可以看到左下角有解码后的翻译，向上翻，发现了这行Unicode码 为什么之前没有搜索到？emmmmmmm，比较一下 原来是“CM”没有被翻译过来，所以才会搜索不到，那就直接看这里 简单的翻译一下const-string v1, “\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a” #将“写个CM都很难啊”的字符串赋值给V1 invoke-virtual {v0, v1}, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Z #调用equals方法比较V0与V1的值是否相等 move-result v0 #返回一个布尔值传给V0（V0、V1相等就等于1，不相等就等于0） if-eqz v0, :cond_0 #如果V0等于0，就跳转到cond_0（cond_0后面的内容是注册失败） 【修改方法一】1234567const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v0, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0if-eqz v0, :cond_0 修改为：123456789const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v0, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0const\4 v0,0x1if-eqz v0, :cond_0 【修改方法二】1234567const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v0, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0if-eqz v0, :cond_0 修改为：1234567const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v0, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0if-nez v0, :cond_0 【修改方法三】（分析认为可以这样修改，目前没有试过）1234567const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v0, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0if-eqz v0, :cond_0 修改为：1234567const-string v1, "\u5199\u4e2aCM\u90fd\u5f88\u96be\u554a"invoke-virtual &#123;v1, v1&#125;, Ljava/lang/String;-&gt;equals(Ljava/lang/Object;)Zmove-result v0if-eqz v0, :cond_0 参考文献：安卓逆向分析笔记—-002https://www.52pojie.cn/thread-886046-1-1.html(出处: 吾爱破解论坛) 问答： androidkiller搜索字符串下的“搜索”按钮后的小三角点开可以直接转unicode。]]></content>
      <categories>
        <category>Android</category>
        <category>安卓逆向分析</category>
      </categories>
      <tags>
        <tag>Android，安卓逆向分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取腾讯视频真实地址（涉及清晰度选择）]]></title>
    <url>%2Fhuo-qu-teng-xun-shi-pin-zhen-shi-di-zhi-she-ji-qing-xi-du-xuan-ze.html</url>
    <content type="text"><![CDATA[前言 由于需要在小程序插入腾讯视频，于是在网上找了诸多方法，但基本都只能抓到低清版本的；偶然发现大神能抓到超清版（720p）的,但由于自己的视频并不都含有超清的版本，因而只能退而求其次使用480p的；这对于移动端（包括全屏）的清晰度已经足够了。毕竟也不是专门做视频的小程序。 环境：linux 语言：python 本文原创地址：https://blog.csdn.net/Szu_IT_Man/article/details/80449751 步骤主要api http://vv.video.qq.com/getinfo http://vv.video.qq.com/getkey 腾讯视频的vids 当你打开一个腾讯视频的时候，比如https://v.qq.com/x/page/x0164ytbgov.html，vids=x0164ytbgov。这个将作为参数让我们去访问api. 访问getinfo 当我们得到vids后，我们就可以访问api：http://vv.video.qq.com/getinfo?vids=x0164ytbgov&amp;platform=101001&amp;charge=0&amp;otype=json&amp;defn=shd（如果需要xml格式，就让参数中otype=xml) 获取到 json1： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869QZOutputJson=&#123; "dltype":1, "exem":0, "fl":&#123; "cnt":2, "fi":[ &#123;"id":100701,"name":"msd","lmt":0,"sb":1,"cname":"标清;(270P)","br":28,"profile":2,"drm":0,"video":1,"audio":1,"fs":16546732,"sl":1&#125;, &#123;"id":2,"name":"mp4","lmt":0,"sb":1,"cname":"高清;(480P)","br":33,"profile":1,"drm":0,"video":1,"audio":1,"fs":34433483,"sl":0&#125; ] &#125;, "hs":0,"ip":"120.36.254.197","ls":0,"preview":555,"s":"o","sfl":&#123;"cnt":0&#125;,"tm":1527217153, "vl":&#123; "cnt":1, "vi":[ &#123; "br":28, "ch":0, "cl":&#123;"fc":0,"keyid":"x0164ytbgov.100701"&#125;, "ct":21600, "drm":0, "dsb":0, "fmd5":"99fb72a48f2a392ec88b5c17bac4ff81", "fn":"x0164ytbgov.m701.mp4", "fs":16546732, "fst":5, "fvkey":"1E327E4FE26E8B8222C226B0873DBF94C094934198E382AE2910361C29E5396237CCF814A49976A5A485F2E806CBC4C21602406034413F9896BF0972F7678D8263845FD123BA9B3604419489CD2BF4254EFD5F87D62A6F88FFF2A4E38F57C9898DD1F7C38C1F548C", "head":0, "hevc":0, "iflag":0, "level":0, "lnk":"x0164ytbgov", "logo":1, "mst":8, "pl":null, "share":1, "sp":0, "st":2, "tail":0, "td":"555.00", "ti":"World Builder (high quality)", "tie":0, "type":3, "ul":&#123; "ui":[ &#123; "url":"http://ugcws.video.gtimg.com/", "vt":106, "dtc":0, "dt":2 &#125;, &#123;"url":"http://122.228.238.157/vhot2.qqvideo.tc.qq.com/AcSbzObMTmKX0kJqo8qTGhY0OAwzkw14O81CvShBXrlI/","vt":200,"dtc":0,"dt":2&#125;, &#123;"url":"http://ugcdl.video.gtimg.com/","vt":116,"dtc":0,"dt":2&#125;, &#123;"url":"http://video.dispatch.tc.qq.com/","vt":0,"dtc":0,"dt":2&#125; ] &#125;, "vh":256, "vid":"x0164ytbgov", "videotype":0, "vr":0, "vst":2, "vw":480, "wh":1.875, "wl":&#123;"wi":[]&#125; &#125; ] &#125;&#125;; 拼接低清版本的视频地址 到这一步，我们就可以获取低清版本的视频源地址了。 我们把获取到字符串去掉QZOutputJson=和最后的分号，变成正式的json 1234realVideoURLRequestPath="http://vv.video.qq.com/getinfo?vids=x0164ytbgov&amp;platform=101001&amp;charge=0&amp;otype=json&amp;defn=shd"videoInfo = requests.get(realVideoURLRequestPath)videoInfo_json = videoInfo.text[len('QZOutputJson='):-1]tempStr = json.loads(videoInfo_json) 这样我们就可以获取到 1234tempStr['vl']['vi'][0]['ul']['ui'][0]['url']+tempStr['vl']['vi'][0]['fn']+"?vkey="+tempStr['vl']['vi'][0]['fvkey'] 即： 1http://ugcws.video.gtimg.com/x0164ytbgov.m701.mp4?vkey=1E327E4FE26E8B8222C226B0873DBF94C094934198E382AE2910361C29E5396237CCF814A49976A5A485F2E806CBC4C21602406034413F9896BF0972F7678D8263845FD123BA9B3604419489CD2BF4254EFD5F87D62A6F88FFF2A4E38F57C9898DD1F7C38C1F548C http://ugcws.video.gtimg.com/x0164ytbgov.m701.mp4?vkey=1E327E4FE26E8B8222C226B0873DBF94C094934198E382AE2910361C29E5396237CCF814A49976A5A485F2E806CBC4C21602406034413F9896BF0972F7678D8263845FD123BA9B3604419489CD2BF4254EFD5F87D62A6F88FFF2A4E38F57C9898DD1F7C38C1F548C 拼接高清视频地址 因为我们这个视最清晰只有高清，所以json中的fi字段 1234"fi":[ &#123;"id":100701,"name":"msd","lmt":0,"sb":1,"cname":"标清;(270P)","br":28,"profile":2,"drm":0,"video":1,"audio":1,"fs":16546732,"sl":1&#125;, &#123;"id":2,"name":"mp4","lmt":0,"sb":1,"cname":"高清;(480P)","br":33,"profile":1,"drm":0,"video":1,"audio":1,"fs":34433483,"sl":0&#125; ] 如果有超清清的话，一般会多一个id为10701的数组元素，如： 12345678910111213 "id":10701, "name":"shd", "lmt":0, "sb":1, "cname":"超清;(720P)", "br":39, "profile":1, "drm":0, "video":1, "audio":1, "fs":41560039, "sl":0&#125; 这时，我们就需要用到 http://vv.video.qq.com/getkey api 首先我们先构造filename=[vids]+.mp4 即 filename=x0164ytbgov.mp4 然后访问： 12http://vv.video.qq.com/getkey?format=2 &amp;otype=json&amp;vt=150&amp;vid=x0164ytbgov&amp;ran=0\%2E9477521511726081\\&amp;charge=0&amp;filename=x0164ytbgov.mp4&amp;platform=11 http://vv.video.qq.com/getkey?format=2&amp;otype=json&amp;vt=150&amp;vid=x0164ytbgov&amp;ran=0\%2E9477521511726081\\&amp;charge=0&amp;filename=x0164ytbgov.mp4&amp;platform=11 返回json2 1234567891011121314QZOutputJson=&#123; "br":62042.312, "ct":21600, "filename":"x0164ytbgov.mp4", "ip":"120.36.254.197", "key":"A76112B8282FBD47AE5408A46CBE73D6C3B087688864DAE1C5076726E901E76673A9884A141F398F08A0EB6E9A2C684DA23F92D9A6D0B2787CB108EF75FCD8D7A631A2D40AF2C020392BEE038A662EF1434BF157CAC87218E87A26B65375D06C", "keyid":"x0164ytbgov.2", "level":0, "levelvalid":1, "s":"o", "sp":0, "sr":0&#125;; 根据这两个json，我们就构造新的连接：tempStr[‘vl’][‘vi’][0][‘ul’][‘ui’][0][‘url’]+filename+?vkey=key (key为json2的key),即： 1http://ugcws.video.gtimg.com/x0164ytbgov.mp4?vkey=A76112B8282FBD47AE5408A46CBE73D6C3B087688864DAE1C5076726E901E76673A9884A141F398F08A0EB6E9A2C684DA23F92D9A6D0B2787CB108EF75FCD8D7A631A2D40AF2C020392BEE038A662EF1434BF157CAC87218E87A26B65375D06C http://ugcws.video.gtimg.com/x0164ytbgov.mp4?vkey=A76112B8282FBD47AE5408A46CBE73D6C3B087688864DAE1C5076726E901E76673A9884A141F398F08A0EB6E9A2C684DA23F92D9A6D0B2787CB108EF75FCD8D7A631A2D40AF2C020392BEE038A662EF1434BF157CAC87218E87A26B65375D06C 这样，我们就获得了高清的视频地址了 拼接超清视频地址 前面说过了额，如果有超清清的话，一般会多一个id为10701的数组元素，如： 1234567891011121314&#123; "id":10701, "name":"shd", "lmt":0, "sb":1, "cname":"超清;(720P)", "br":39, "profile":1, "drm":0, "video":1, "audio":1, "fs":41560039, "sl":0&#125; 这时我们构造的filename=x0164ytbgov.p701.1.mp4 此时访问getkey的api就要改为：12http://vv.video.qq.com/getkey?format=10701 &amp;otype=json&amp;vt=150&amp;vid=x0164ytbgov&amp;ran=0\%2E9477521511726081\\&amp;charge=0&amp;filename=x0164ytbgov.p701.1.mp4&amp;platform=11 &lt;http://vv.video.qq.com/getkey?format=10701 &amp;otype=json&amp;vt=150&amp;vid=x0164ytbgov&amp;ran=0\%2E9477521511726081\\&amp;charge=0&amp;filename=x0164ytbgov.p701.1.mp4&amp;platform=11&gt; 聪明的你可能发现了：format={id} 到最后构成真实视频的url:tempStr[‘vl’][‘vi’][0][‘ul’][‘ui’][0][‘url’]+filename+?vkey=key (key为json2的key,filename=x0164ytbgov.p701.1.mp4) 由于测试用的视频地址无超清源，所以超清这块的测试需要使用的超清视频做测试。提示已测试可行。 参考文献 腾讯视频爬虫 腾讯视频的地址解析下载 获取腾讯视频真实地址_20180525 答疑 问题：怎么知道defn为清晰度标识并且知道可选值有sd（标清）、hd（高清）、shd（超清）、fhd（1080P）？回答：getinfo接口的返回结果中有这个对应关系。此外，也可以推测出来，defn是definition的简写；sd、hd、shd、fhd分别是standard definition、high definition、super high definition、full high difinition的简写。]]></content>
      <categories>
        <category>电脑</category>
        <category>视频解析</category>
      </categories>
      <tags>
        <tag>电脑</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯视频的地址解析下载]]></title>
    <url>%2Fteng-xun-shi-pin-de-di-zhi-jie-xi-xia-zai.html</url>
    <content type="text"><![CDATA[以腾讯视频播放页地址http://v.qq.com/x/cover/rz4mhb6494f12co.html为例，说说如何解析得到视频的真实地址。 提取视频ID在播放页源码中，可以找到如下视频信息：123456789101112131415161718var VIDEO_INFO = &#123; title: "咱们相爱吧 第1集", duration: "2746", vid: "y00221a60w7", piantou:"0", pianwei:"0", showMark:"1", showBullet:true, showImageBullet:false, openBulletDefault:true, isNeedPay: false, isTrailer: 0, singlePrice: "undefined", vipPrice: "undefined", tryTime: "1306029", isTrailer : 0, type: "2"&#125;; 其中，vid的值y00221a60w7即为视频ID. 请求getinfo接口使用Chrome浏览器的开发者工具监控网络请求，发现getinfo接口的请求地址如下：1http://h5vv.video.qq.com/getinfo?callback=txplayerJsonpCallBack_getinfo_591513&amp;isHLS=false&amp;charge=0&amp;vid=y00221a60w7&amp;defn=hd&amp;defnpayver=1&amp;otype=json&amp;guid=29a06bf3852fbe2ea6eb53829c3878fa&amp;platform=10901&amp;sdtfrom=v1010&amp;host=v.qq.com&amp;_rnd=1479010822&amp;fhdswitch=0&amp;show1080p=1&amp;_qv_rmt=sNk0sWZTA17002uQa%3D&amp;_qv_rmt2=0Qs65I9%2B149182HOQ%3D&amp;_=1479010820769 http://h5vv.video.qq.com/getinfo?callback=txplayerJsonpCallBack_getinfo_591513&amp;isHLS=false&amp;charge=0&amp;vid=y00221a60w7&amp;defn=hd&amp;defnpayver=1&amp;otype=json&amp;guid=29a06bf3852fbe2ea6eb53829c3878fa&amp;platform=10901&amp;sdtfrom=v1010&amp;host=v.qq.com&amp;_rnd=1479010822&amp;fhdswitch=0&amp;show1080p=1&amp;_qv_rmt=sNk0sWZTA17002uQa%3D&amp;_qv_rmt2=0Qs65I9%2B149182HOQ%3D&amp;_=1479010820769我们尝试构造各个请求参数，然后看看请求结果是什么样子的。12345678910111213params = &#123; 'isHLS': False, 'charge': 0, 'vid': 'y00221a60w7', 'defn': 'shd', 'defnpayver': 1, 'otype': 'json', 'platform': 10901, 'sdtfrom': 'v1010', 'host': 'v.qq.com', 'fhdswitch': 0, 'show1080p': 1,&#125; 参数中的defn为清晰度标识，可选值有sd（标清）、hd（高清）、shd（超清）、fhd（1080P）。请求结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214&#123; "dltype": 1, "exem": 0, "fl": &#123; "cnt": 4, "fi": [ &#123; "br": 20000, "cname": "高清;(480P)", "fs": 239935872, "id": 10412, "lmt": 0, "name": "hd", "sb": 1, "sl": 0 &#125;, &#123; "br": 500000, "cname": "蓝光;(1080P)", "fs": 1064110139, "id": 10409, "lmt": 1, "name": "fhd", "sb": 1, "sl": 0 &#125;, &#123; "br": 64, "cname": "标清;(270P)", "fs": 114567499, "id": 10403, "lmt": 0, "name": "sd", "sb": 1, "sl": 0 &#125;, &#123; "br": 500000, "cname": "超清;(720P)", "fs": 468471925, "id": 10401, "lmt": 0, "name": "shd", "sb": 1, "sl": 1 &#125; ] &#125;, "hs": 0, "ls": 0, "preview": 2746, "s": "o", "sfl": &#123; "cnt": 0 &#125;, "tm": 1479014650, "vl": &#123; "cnt": 1, "vi": [ &#123; "br": 166, "ch": 0, "cl": &#123; "ci": [ &#123; "cd": "300.032", "cmd5": "285464ac33c56b14caa63c466d4b4ed3", "cs": 56516869, "idx": 1, "keyid": "y00221a60w7.10401.1" &#125;, &#123; "cd": "299.988", "cmd5": "6a9b0dedeb2ed7c73e5ad31418a9a6bc", "cs": 56364414, "idx": 2, "keyid": "y00221a60w7.10401.2" &#125;, &#123; "cd": "299.988", "cmd5": "2181237427075dc219de8bdaa37ae64d", "cs": 50213590, "idx": 3, "keyid": "y00221a60w7.10401.3" &#125;, &#123; "cd": "299.988", "cmd5": "ac17519a98640a1926f54bc48f5637fb", "cs": 37593039, "idx": 4, "keyid": "y00221a60w7.10401.4" &#125;, &#123; "cd": "300.032", "cmd5": "546963ed0dcb3544373e8642eb2bb166", "cs": 54082931, "idx": 5, "keyid": "y00221a60w7.10401.5" &#125;, &#123; "cd": "299.988", "cmd5": "f33ed88e9b94d4cb1c9c0fb5d3730f76", "cs": 43686678, "idx": 6, "keyid": "y00221a60w7.10401.6" &#125;, &#123; "cd": "299.988", "cmd5": "a6a298f6122de6c09080ae78639c139c", "cs": 40800986, "idx": 7, "keyid": "y00221a60w7.10401.7" &#125;, &#123; "cd": "299.988", "cmd5": "9454b637e3c913ebd1c880ac5e6cf973", "cs": 62436368, "idx": 8, "keyid": "y00221a60w7.10401.8" &#125;, &#123; "cd": "346.837", "cmd5": "0fe50a2165b1ef7860627dbaf672d493", "cs": 68478324, "idx": 9, "keyid": "y00221a60w7.10401.9" &#125; ], "fc": 9 &#125;, "ct": 21600, "drm": 0, "dsb": 0, "fclip": 1, "fmd5": "18f937a9790f22a843e618fb5fd02583", "fn": "y00221a60w7.p401.mp4", "fs": 468471925, "fst": 5, "fvkey": "88980E6001844B3B7D98195D7077A1EA62310E41BD70C43F57AC6E80AFEB8E9EFD71201CBBB054F2CBFEC5EA8269BB141DB6933322FAA43AD3BB6AA62AD9FE2C2B2A321A639497AA6DF23492CF3B622E41A1C5C75F2DDEC7", "hevc": 0, "iflag": 0, "level": 0, "lnk": "y00221a60w7", "logo": 1, "pl": [ &#123; "cnt": 2, "pd": [ &#123; "c": 10, "cd": 10, "fmt": 40001, "fn": "q1", "h": 45, "r": 10, "url": "http://video.qpic.cn/video_caps/0/", "w": 80 &#125;, &#123; "c": 5, "cd": 10, "fmt": 40002, "fn": "q2", "h": 90, "r": 5, "url": "http://video.qpic.cn/video_caps/0/", "w": 160 &#125; ] &#125; ], "share": 1, "sp": 0, "st": 2, "td": "2746.84", "ti": "咱们相爱吧_01", "type": 1136, "ul": &#123; "ui": [ &#123; "dt": 2, "dtc": 10, "url": "http://124.193.165.208/vlive.qqvideo.tc.qq.com/", "vt": 203 &#125;, &#123; "dt": 2, "dtc": 10, "url": "http://124.193.165.209/vlive.qqvideo.tc.qq.com/", "vt": 203 &#125;, &#123; "dt": 2, "dtc": 10, "url": "http://124.193.165.210/vlive.qqvideo.tc.qq.com/", "vt": 203 &#125;, &#123; "dt": 2, "dtc": 10, "url": "http://video.dispatch.tc.qq.com/27099043/", "vt": 0 &#125; ] &#125;, "vh": 720, "vid": "y00221a60w7", "videotype": 2, "vst": 2, "vw": 1280 &#125; ] &#125;&#125; 我们感兴趣的有三个列表：fi列表、ci列表、ui列表。 ui列表中的url是视频真实地址的前缀，选择ui列表第一个元素中的url即可。 fi列表列出了各个视频码流，每一项中的id是码流的编号，这个参数在后面会用到。 ci列表列出了各个分段的相关信息，各个分段信息中的keyid在后面会用到。 请求getkey接口同样，在网络请求监测中发现getkey接口的请求是这样的：1http://h5vv.video.qq.com/getkey?callback=txplayerJsonpCallBack_getkey_931033&amp;&amp;otype=json&amp;vid=y00221a60w7&amp;format=10401&amp;filename=y00221a60w7.p401.4.mp4&amp;platform=10901&amp;vt=203&amp;charge=0&amp;_rnd=1479015483148&amp;_=1479015476641 http://h5vv.video.qq.com/getkey?callback=txplayerJsonpCallBack_getkey_931033&amp;&amp;otype=json&amp;vid=y00221a60w7&amp;format=10401&amp;filename=y00221a60w7.p401.4.mp4&amp;platform=10901&amp;vt=203&amp;charge=0&amp;_rnd=1479015483148&amp;_=1479015476641 构造参数如下：123456789params = &#123; &apos;otype&apos;: &apos;json&apos;, &apos;vid&apos;: &apos;y00221a60w7&apos;, &apos;format&apos;: 10401, &apos;filename&apos;: &apos;y00221a60w7.p401.4.mp4&apos;, &apos;platform&apos;: 10901, &apos;vt&apos;: 217, &apos;charge&apos;: 0,&#125; 参数中的vid为视频ID，format为码流的编号，filename根据分段信息中的keyid得来。以第四个分段为例，说说如何构造filename参数。第四个分段信息中的keyid为y00221a60w7.10401.4，我们将keyid中间的.10替换为.p，然后在末尾加上.mp4即可。 getkey接口的请求结果示例如下：123456789101112&#123; "br": 125315.14, "ct": 21600, "filename": "y00221a60w7.p401.mp4", "key": "AE6DC4B022C2A29C59D1A8E942787A2A54F64A5ABFCA7C99B51BD6539703D4F745DD2D77C3433ACAF1961BBFB6D84C1C717D23BEDCAF4DABC0D8BA2229F6C3464F59F0A10C5ED4CC25E355D9171DC65D411D6834BCA2DECD", "keyid": "y00221a60w7.10401.4", "level": 0, "levelvalid": 1, "s": "o", "sp": 0, "sr": 0&#125; 看！key粗来了。 构造视频真实地址通过网络监测发现视频真实地址是像这样的：1http://60.206.195.15/vlive.qqvideo.tc.qq.com/y00221a60w7.p401.4.mp4?sdtfrom=v1010&amp;guid=29a06bf3852fbe2ea6eb53829c3878fa&amp;vkey=6C5D63159598E6D217BA73F6F3335C0B80CECE2A4E889AFCDD8A391DB9C648059731AE9AD47BDA03B016F76E8B59D9DF0C2373CA32A4ADB935AEE0FE73ECE1C460DC01F6B733A5F38035AC289C44721F606C1ADF5AED4267 http://60.206.195.15/vlive.qqvideo.tc.qq.com/y00221a60w7.p401.4.mp4?sdtfrom=v1010&amp;guid=29a06bf3852fbe2ea6eb53829c3878fa&amp;vkey=6C5D63159598E6D217BA73F6F3335C0B80CECE2A4E889AFCDD8A391DB9C648059731AE9AD47BDA03B016F76E8B59D9DF0C2373CA32A4ADB935AEE0FE73ECE1C460DC01F6B733A5F38035AC289C44721F606C1ADF5AED4267分析该地址的构造，不难发现，地址前缀在前面的ui列表中已经得到了，filename也计算出来了，vkey参数就是上面得到的key. Python代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import requestsimport json vid = 'y00221a60w7' # replace with your vidfor definition in ('shd', 'hd', 'sd'): params = &#123; 'isHLS': False, 'charge': 0, 'vid': vid, 'defn': definition, 'defnpayver': 1, 'otype': 'json', 'platform': 10901, 'sdtfrom': 'v1010', 'host': 'v.qq.com', 'fhdswitch': 0, 'show1080p': 1, &#125; r = requests.get('http://h5vv.video.qq.com/getinfo', params=params) data = json.loads(r.content[len('QZOutputJson='):-1]) url_prefix = data['vl']['vi'][0]['ul']['ui'][0]['url'] for stream in data['fl']['fi']: if stream['name'] != definition: continue stream_id = stream['id'] urls = [] for d in data['vl']['vi'][0]['cl']['ci']: keyid = d['keyid'] filename = keyid.replace('.10', '.p', 1) + '.mp4' params = &#123; 'otype': 'json', 'vid': vid, 'format': stream_id, 'filename': filename, 'platform': 10901, 'vt': 217, 'charge': 0, &#125; r = requests.get('http://h5vv.video.qq.com/getkey', params=params) data = json.loads(r.content[len('QZOutputJson='):-1]) url = '%s/%s?sdtfrom=v1010&amp;vkey=%s' % (url_prefix, filename, data['key']) urls.append(url) print 'stream:', stream['name'] for url in urls: print url 参考文献 腾讯视频的地址解析下载（简书，CSDN）]]></content>
      <categories>
        <category>电脑</category>
        <category>视频解析</category>
      </categories>
      <tags>
        <tag>电脑</tag>
        <tag>视频解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己做腾讯视频真实地址解析分析]]></title>
    <url>%2Fzi-ji-zuo-teng-xun-shi-pin-zhen-shi-di-zhi-jie-xi-fen-xi.html</url>
    <content type="text"><![CDATA[我觉得网上的一些资料大多是直接给出处理的结果,而不注重分析过程,对我们程序员来说,其过程更重要,工作中碰到的问题是多种多样的,不可能每个碰到的问题都能从网上直接找到答案,我也觉得作为程序员直接去找答案的做法本身有问题.应该提高主动去分析解决问题的能力.下面是对这类问题的一个相对通用的分析方法. URL分析http://v.qq.com/cover/g/g9jgclyhpp5sp7p.html?vid=g00118pmaso 打开这个url的页面源码,找到下面这些内容:12345678910111213141516var COVER_INFO = &#123;title :"独家首发",id :"g9jgclyhpp5sp7p",isPrev :false,pic :"http://i.gtimg.cn/qqlive/img/jpgcache/files/qqvideo/g/g9jgclyhpp5sp7p_h.jpg",typeid:22,videoReName :false,sourceid:0,isEurope:false,useTextVideoList:false,columnid:0,hasRecommend:true,specialTemp:false,varietyDate:"",copyright:5&#125;; 在源码中搜索g9jgclyhpp5sp7p字符串，从其出现的位置基本可以判断这个字符串唯一代表了一个合集(或者叫专辑). 不管它有没有用,先大概看一下,把URL中觉得有用的东西过一遍. 腾讯视频播放地址的url有几种模式,上面是其中一种,其它的几种模式对解析其地址没什么太大关系,这里就以这种为例了.这个url中最关键的就是最后面接的一个参数vid(videoID)=g00118pmaso. 抓包这里首先要说一点,平时做web应用或者后台接口,要有习惯去抓包,并且对抓到的包进行一些简单的分析,这是一个很好的习惯,有时候会学到一些意想不到的东西. 最简单的抓包方式就是直接用浏览器自带的工具了,我平时用的chrome,F12 - Network中就有当前页面发送出去的http报文,另外还有个小技巧,因为浏览器会有缓存,按Ctrl+F5刷新页面,就会强制从服务端获取内容而不用缓存,有时候需要这样做,具体的原理可以去看看http协议中的Cache-Control头部. 用浏览器抓包还有个不太方便的地方,当页面跳转时,前面抓的包就没有了,抓的永远是当前页面相关的报文.所以我平时用的比较多的是fiddler,一个免费的抓包工具,非常方便. 回到正题, 我们从浏览器输入链接http://v.qq.com/cover/g/g9jgclyhpp5sp7p.html?vid=g00118pmaso, 抓到了很多http包,我这里给出一部分的截图 标出来的部分请求是关键请求, 我自己在分析的时候,是从下往上分析的, 1)首先找到视频的下载地址12345http://101.71.72.2/music.qqvideo.tc.qq.com/g00118pmaso.p309.1.mp4?sdtfrom=v10&amp;type=mp4&amp;vkey=95C006D54AA7217B859B15732EE04FB88B3986D36E53116ECAF1CFD43EE6615BA6AD5DDDF8CC3D7B&amp;level=3&amp;platform=1&amp;br=351&amp;fmt=fhd&amp;sp=0 这里继续分析url,g00118pmaso上面讲到了,是vid,但是后面多出了.p309.1. 继续往下看sdtfrom=v10(未知),type=mp4,这个好理解，vkey=95C006D54AA7217B859B15732EE04FB88B3986D36E53116ECAF1CFD43EE6615BA6AD5DDDF8CC3D7B,这个看起来又像是一个比较关键的参数, level=3(后面xml中对应的level), platform=1(平台，web、client、iphone…?暂时也是未知), br=351(bitrate=351?大概是码率的意思，后面xml中的br=351), fmt=fhd(大概意思是format=flvHD?实际上这里是后面id为10309对应的format name), sp=0(后面xml中对应的sp). 这里有一大堆的参数，有很多都是未知，这个时候别慌，整个链接直接下载肯定是可以下载到视频的，现在要做的是，去掉其中一些参数，看是否能下载.1http://101.71.72.2/music.qqvideo.tc.qq.com/g00118pmaso.p309.1.mp4?type=mp4&amp;vkey=95C006D54AA7217B859B15732EE04FB88B3986D36E53116ECAF1CFD43EE6615BA6AD5DDDF8CC3D7B&amp;br=351&amp;fmt=fhd 最后发现，保留这些参数，能正常下载到视频。所以其它参数暂时先别管。这里有几个关键的地方，vkey从哪里来的？br和fmt从那里获取？vid后面的p309.1哪里来的？还有就是ip地址来源未知。分析到这里，继续往上找到另一个比较关键的请求。 http://vv.video.qq.com/getkey 分析玩上面的视频地址链接的组成，很容易就找到了这个请求，getkey。它是一个post请求，查看其发送的参数format=10309&amp;otype=xml&amp;vt=210&amp;vid=g00118pmaso&amp;ran=0%2E9477521511726081&amp;charge=0&amp;filename=g00118pmaso%2Ep309%2E1%2Emp4&amp;platform=11 http response中的内容如下：12&lt;?xml version="1.0" encoding="utf-8" standalone="no" ?&gt;&lt;root&gt;&lt;br&gt;360602.1875&lt;/br&gt;&lt;ct&gt;604800&lt;/ct&gt;&lt;key&gt;95C006D54AA7217B859B15732EE04FB88B3986D36E53116ECAF1CFD43EE6615BA6AD5DDDF8CC3D7B&lt;/key&gt;&lt;level&gt;3&lt;/level&gt;&lt;levelvalid&gt;1&lt;/levelvalid&gt;&lt;s&gt;o&lt;/s&gt;&lt;sp&gt;0&lt;/sp&gt;&lt;sr&gt;0&lt;/sr&gt;&lt;/root&gt; 在这里我们找到了key，同时还发现了和上面链接对应的level和sp两个参数，上面一个链接中的一个参数，不过这个参数也非必要，暂时不管。12345URLDecode后为：format=10309（这里实际上是后面的format id）&amp;otype=xml&amp;vt=210&amp;vid=g00118pmaso&amp;ran=0.9477521511726081&amp;charge=0&amp;filename=g00118pmaso.p309.1.mp4&amp;platform=11 同样，在代码中试，看那几个参数是必要的，经试验，发现format,type,vid,filename是必要的。 继续网上分析第三个url，http://vv.video.qq.com/getinfo 也是一个POST请求，参数是：12otype=xml&amp;pid=2FAF2F6427123207101EBDA3F1523310A76216BD&amp;platform=11&amp;vids=g00118pmaso&amp;charge=0&amp;speed=1246&amp;ran=0.8439321480691433 返回的结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version="1.0" encoding="utf-8" standalone="no" ?&gt;&lt;root&gt; &lt;fl&gt; &lt;cnt&gt;5&lt;/cnt&gt; &lt;fi&gt; &lt;br&gt;64&lt;/br&gt; &lt;id&gt;1&lt;/id&gt; &lt;name&gt;flv&lt;/name&gt; &lt;sl&gt;0&lt;/sl&gt; &lt;/fi&gt; &lt;fi&gt; &lt;br&gt;550&lt;/br&gt; &lt;id&gt;10301&lt;/id&gt; &lt;name&gt;shd&lt;/name&gt; &lt;sl&gt;0&lt;/sl&gt; &lt;/fi&gt; &lt;fi&gt; &lt;br&gt;230&lt;/br&gt; &lt;id&gt;10302&lt;/id&gt; &lt;name&gt;hd&lt;/name&gt; &lt;sl&gt;0&lt;/sl&gt; &lt;/fi&gt; &lt;fi&gt; &lt;br&gt;64&lt;/br&gt; &lt;id&gt;10303&lt;/id&gt; &lt;name&gt;sd&lt;/name&gt; &lt;sl&gt;0&lt;/sl&gt; &lt;/fi&gt; &lt;fi&gt; &lt;br&gt;900&lt;/br&gt; &lt;id&gt;10309&lt;/id&gt; &lt;name&gt;fhd&lt;/name&gt; &lt;sl&gt;1&lt;/sl&gt; &lt;/fi&gt; &lt;/fl&gt; &lt;hs&gt;0&lt;/hs&gt; &lt;ls&gt;0&lt;/ls&gt; &lt;preview&gt;276&lt;/preview&gt; &lt;s&gt;o&lt;/s&gt; &lt;tm&gt;1361366724&lt;/tm&gt; &lt;vl&gt; &lt;cnt&gt;1&lt;/cnt&gt; &lt;vi&gt; &lt;br&gt;351&lt;/br&gt; &lt;ch&gt;0&lt;/ch&gt; &lt;cl&gt; &lt;ci&gt; &lt;cd&gt;276.480011&lt;/cd&gt; &lt;cmd5&gt;beace783957b52f460006604229b57cf&lt;/cmd5&gt; &lt;cs&gt;99699295&lt;/cs&gt; &lt;idx&gt;1&lt;/idx&gt; &lt;keyid&gt;g00118pmaso.10309.1&lt;/keyid&gt; &lt;/ci&gt; &lt;fc&gt;1&lt;/fc&gt; &lt;/cl&gt; &lt;fmd5&gt;4ebecd94d64f46666d4e43826c120b5f&lt;/fmd5&gt; &lt;fn&gt;g00118pmaso.p309.mp4&lt;/fn&gt; &lt;fs&gt;99699378&lt;/fs&gt; &lt;fst&gt;5&lt;/fst&gt; &lt;lnk&gt;g00118pmaso&lt;/lnk&gt; &lt;logo&gt;1&lt;/logo&gt; &lt;pl&gt; &lt;cnt&gt;2&lt;/cnt&gt; &lt;pd&gt; &lt;c&gt;10&lt;/c&gt; &lt;cd&gt;2&lt;/cd&gt; &lt;fmt&gt;40001&lt;/fmt&gt; &lt;fn&gt;q1&lt;/fn&gt; &lt;h&gt;45&lt;/h&gt; &lt;r&gt;10&lt;/r&gt; &lt;url&gt;http://video.qpic.cn/video_caps/0/&lt;/url&gt; &lt;w&gt;80&lt;/w&gt; &lt;/pd&gt; &lt;pd&gt; &lt;c&gt;5&lt;/c&gt; &lt;cd&gt;2&lt;/cd&gt; &lt;fmt&gt;40002&lt;/fmt&gt; &lt;fn&gt;q2&lt;/fn&gt; &lt;h&gt;90&lt;/h&gt; &lt;r&gt;5&lt;/r&gt; &lt;url&gt;http://video.qpic.cn/video_caps/0/&lt;/url&gt; &lt;w&gt;160&lt;/w&gt; &lt;/pd&gt; &lt;/pl&gt; &lt;share&gt;1&lt;/share&gt; &lt;st&gt;2&lt;/st&gt; &lt;td&gt;276.48&lt;/td&gt; &lt;ti&gt;因你而在&lt;/ti&gt; &lt;type&gt;3585&lt;/type&gt; &lt;ul&gt; &lt;ui&gt; &lt;dt&gt;2&lt;/dt&gt; &lt;dtc&gt;10&lt;/dtc&gt; &lt;url&gt;http://101.71.72.2/music.qqvideo.tc.qq.com/&lt;/url&gt; &lt;vt&gt;210&lt;/vt&gt; &lt;/ui&gt; &lt;ui&gt; &lt;dt&gt;2&lt;/dt&gt; &lt;dtc&gt;10&lt;/dtc&gt; &lt;url&gt;http://113.207.98.27/music.qqvideo.tc.qq.com/&lt;/url&gt; &lt;vt&gt;210&lt;/vt&gt; &lt;/ui&gt; &lt;ui&gt; &lt;dt&gt;2&lt;/dt&gt; &lt;dtc&gt;10&lt;/dtc&gt; &lt;url&gt;http://video.store.qq.com/&lt;/url&gt; &lt;vt&gt;0&lt;/vt&gt; &lt;/ui&gt; &lt;/ul&gt; &lt;vh&gt;1080&lt;/vh&gt; &lt;vid&gt;g00118pmaso&lt;/vid&gt; &lt;videotype&gt;0&lt;/videotype&gt; &lt;vw&gt;1920&lt;/vw&gt; &lt;/vi&gt; &lt;/vl&gt;&lt;/root&gt; 看到这个后，比较兴奋，想要的东西基本上在这里可以找到了。再回过来分析一下这个请求所带的参数123otype=xml&amp;pid=2FAF2F6427123207101EBDA3F1523310A76216BD&amp;platform=11&amp;vids=g00118pmaso&amp;charge=0&amp;speed=1246&amp;ran=0.8439321480691433 otype返回格式， pid看起来又像是个比较重要的参数(未知)， platform(上面是1，这里又来了个11，不明白), vid和上面一样，charge(应该是付费信息，0表示不付费，来源未知),speed(未知),ran和上面一样应该是个0-1之间的随机数。用httpclient模仿这个请求，令人兴奋的是只需要vids和otype就能返回该xml（不需要再去找pid了）。 再整理一下： 1.先通过http://vv.video.qq.com/getinfo POST请求添加参数otype，vid就能获得上面的xml。 2.根据这个xml中的内容发送http://vv.video.qq.com/getkey POST请求，参数列表为format,type,vid,filename。 其中format的id和name有个对应关系，10309-fhd、10303-sd、10302-hd。这里填写的为id。例如：format=10309 filename由几部分组成g00118pmaso.p309.1.mp4 最前面一部分是vid，如果格式id为10309，则后面加上p309.1，最后是格式mp4。例如我我要下载格式id为10303的视频，那拼出来应该是g00118pmaso.p303.1.mp4。 type为xml，vid为每个视频唯一的id，这里是g00118pmaso。 3.根据以上信息拼接视频的真实地址 首先是xml中的地址http://101.71.72.2/music.qqvideo.tc.qq.com 紧跟着拼上filename，这里例如g00118pmaso.p309.1.mp4、g00118pmaso.p303.1.mp4、g00118pmaso.p302.1.mp4、g00118pmaso.p301.1.mp4。注意filename要和vkey一一对应，不能用p309的key和g00118pmaso.p303.1.mp4拼接。 然后就是vkey，根据filename获得的vkey进行拼接。 接着type=mp4，fmt和format id对应，例如10309对应的是fhd。 对于format id为10309的视频，最后拼出来的结果是1http://101.71.72.2/music.qqvideo.tc.qq.com/g00118pmaso.p309.1.mp4?type=mp4&amp;vkey=95C006D54AA7217B859B15732EE04FB88B3986D36E53116ECAF1CFD43EE6615BA6AD5DDDF8CC3D7B&amp;fmt=fhd 注意这里把一些不必要的参数去掉了。 4.若通过抓包的方式无法分析出来真实地址，则需要反编译swf，找到拼接视频地址的代码，还原其过程。 腾讯的视频地址分析还算顺利，因为不需要反编译swf 参考文献 自己做腾讯视频真实地址解析分析_20130225]]></content>
      <categories>
        <category>电脑</category>
        <category>视频解析</category>
      </categories>
      <tags>
        <tag>视频解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安卓逆向分析笔记---001]]></title>
    <url>%2Fan-zhuo-ni-xiang-fen-xi-bi-ji-001.html</url>
    <content type="text"><![CDATA[第一次使用电脑修改APK，新人上路，很基础，不要喷我~ 【所需工具】Androidkiller 【用例下载链接】链接：https://pan.baidu.com/s/1E9W77vA3wn-u8zqsTeRW4g 提取码：zfw4 【开始操作】 先将APK拖到AndroidKiller里面进行反编译，过一会儿之后，反编译成功，点击左侧“工程管理器”，如下图 用手机打开APP，看到直接进入登录页面，根据以往修改经验说明，登录页在第一启动类里，所以我双击打开AndroidManifest.xml，查看包名还有“application”下面的第一个“activity”，如下图 看到“package=”com.mycompany.myapp””和第一个“activity”的“android:name=”.RD””，通过分析就可以知道第一启动类的路径为“smali/com/mycompany/myapp/RD.smali”，打开smali文件夹，根据上述路径找到RD.smali，双击打开，如下图 直接看smali代码头疼，点击上方工具栏里面的查看Java源码，出现Java查看页面，如下图 我放大页面看，如图 可以看到“onBtButtonClick”的方法，if判断后弹出提示“失败”和“成功”，我打开smali页面找到同样的位置，如下图 一张图截不下 【修改方法一】 12345678910111213141516.line 80const-string v0, "1"const-string v1, "1"if-ne v0, v1, :cond_0```` 修改为：```` bash .line 80 const-string v0, "0" const-string v1, "1"if-ne v0, v1, :cond_0 【修改方法二】123456.line 80 const-string v0, &quot;1&quot; const-string v1, &quot;1&quot;if-ne v0, v1, :cond_0 修改为：123456.line 80 const-string v0, &quot;1&quot; const-string v1, &quot;1&quot; if-eq v0, v1, :cond_0 【修改方法三】（需要Unicode工具）修改文字显示，不推荐使用这种修改方法，治标不治本。 改完后闪退，可能是手机安卓版本太高吧！没有测试成，很遗憾！ 疑问：你的方法一改的和没改的一样啊回答：是那个上面两个1，改成一个1和一个0，当时写草稿纸上复制粘贴也没看清楚 安卓逆向分析笔记—-001https://www.52pojie.cn/thread-884957-1-1.html(出处: 吾爱破解论坛)]]></content>
      <categories>
        <category>Android</category>
        <category>安卓逆向分析</category>
      </categories>
      <tags>
        <tag>Android，安卓逆向分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 10文件资源管理器中删除快速访问]]></title>
    <url>%2Fwindows-10-wen-jian-zi-yuan-guan-li-qi-zhong-shan-chu-kuai-su-fang-wen.html</url>
    <content type="text"><![CDATA[建议备份好您的注册表。要在Windows 10文件资源管理器中删除快速访问， [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer]“HubMode”=dword:00000001 万一又想折腾回去，请打开注册表编辑器，然后将HKEY_LOCAL_MACHINE SOFTWARE Microsoft Windows CurrentVersion Explorer \ 右键单击HubMode并选择“删除”。 Windows 10文件资源管理器中删除快速访问https://www.52pojie.cn/thread-884830-1-1.html(出处: 吾爱破解论坛)]]></content>
      <categories>
        <category>电脑</category>
        <category>常见故障</category>
      </categories>
      <tags>
        <tag>注册表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PC监控小工具]]></title>
    <url>%2Fpc-jian-kong-xiao-gong-ju.html</url>
    <content type="text"><![CDATA[我一直喜欢用的监控软件分享 先下载用来加载工具的应用Gadgets Revived: https://gadgetsrevived.com/download-sidebar/ 小工具官网: http://addgadgets.com 链接: https://pan.baidu.com/s/1AjmHPJPzQmBgdcx7HSSGRA 提取码: 1kss PC监控小工具https://www.52pojie.cn/thread-884752-1-1.html(出处: 吾爱破解论坛)]]></content>
      <categories>
        <category>电脑</category>
        <category>软件</category>
      </categories>
      <tags>
        <tag>电脑</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDM可以直接捕捉、下载各种格式的网页在线视频]]></title>
    <url>%2Fidm-ke-yi-zhi-jie-bu-zhuo-xia-zai-ge-chong-ge-shi-de-wang-ye-zai-xian-shi-pin.html</url>
    <content type="text"><![CDATA[本人亲测、实力见证！新版 IDM不仅可以全速、无死角下载百度云里的一切资源，还可以下载网络上各种敏感、限制级的资源，最重要的是可以直接嗅探、捕捉、下载各种格式的网页在线视频！包括 m3u8 代码形式的在线视频，而不需要使用下载速度比较慢且不能批量下载的m3u8 downloader了。 完全可以代替迅雷、百度网盘实现满速、全速下载。真正做到所见即所得！（划重点） STEP 1→如何安装？1.下载新版IDM文件夹,先运行软件安装程序:IDM v6.xx.x.exe,安装完成后关闭注册提醒对话框。（新版IDM下载链接在文末） 2.运行压缩包 Crack.zip 中的 IDM_6.2x_Crack.exe 进行破解，破解完成。 3.全部过程完成，您需要重启一下电脑，再打开 IDM.，点击菜单—&gt;关于 IDM，确认一下是否破解成功。 STEP 2→必要插件？1.在浏览器装tampermonkey（油猴）插件，本人用360浏览器，浏览器收藏栏右上角有个：扩展—&gt;扩展中心—&gt;直接在搜索应用名称中输入tampermonkey，点击安装即可。 2.在浏览器中安装脚本，打开https://greasyfork.org/zh-CN/scripts，安装“百度网盘直接下载助手 直链加速版”脚本。 3.在浏览器装IDM插件:扩展—&gt;扩展中心—&gt;直接在搜索应用名称中输入IDM，点击安装即可。 4.安装完成后重启浏览器。 STEP 3→如何使用？1.下载百度云资源：打开IDM，打开一个需要下载的文件，在页面的上部就会出现一个“下载助手”的按钮，点击按钮会出现：下载、显示链接、批量链接等选项，点击下载，IDM会自动嗅探到资源，弹出一个下载文件信息框，点击开始下载，享受全速下载的乐趣吧！ 2.下载网页在线视频：打开一个需要下载的在线视频，点击播放，IDM会自动嗅探到资源，同时在播放框的右上角会出现如下按钮：，点击下载该视频，弹出一个下载文件信息框，点击开始下载，享受全速下载的乐趣吧！ STEP 4→什么是IDM？IDM 全名Internet Download Manager 是一款国外的多线程下载神器（简称IDM）Internet Download Manager 支持多媒体下载、自动捕获链接、自动识别文件名、静默下载、批量下载、计划下载任务、站点抓取、队列等等是一款国外的老牌下载工具。互联网下载管理器 （IDM）是一种提高下载速度5倍，恢复和下载时间表的工具。全面的错误恢复和恢复功能将重新启动由于连接丢失，网络问题，计算机关机或意外断电而导致的下载或中断下载。简单的图形用户界面，使IDM用户友好，易于使用。下载管理器有一个智能下载逻辑加速器，具有智能动态文件分割和安全的多部分下载技术，加快您的下载。与其他下载管理器和加速器不同，Internet Download Manager在下载过程中动态地分段下载文件，并重复使用可用的连接，无需额外的连接和登录阶段即可实现最佳加速性能。 下载地址链接: https://pan.baidu.com/s/1m89whBLzvYoc7uZS6l9ZBQ 提取码: qcjp]]></content>
      <categories>
        <category>Idm</category>
      </categories>
      <tags>
        <tag>Idm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博文加密]]></title>
    <url>%2Fhexo-bo-wen-jia-mi.html</url>
    <content type="text"><![CDATA[介绍一种给博客文章加密的方法。 很多时候当我们写了一篇博客，但并不想所有人能够访问它。对于WordPress这很容易做到，但是对于hexo，由于是静态网页，并不能做到完全的加密。 在GitHub上发现了有个人做了一个加密的插件，还挺好用，推荐给大家。 安装在你的hexo根目录的package.json文件夹中添加：1“hexo-blog-encrypt”: “2.0.*” 然后在命令行中输入：1npm install 这样这个插件就安装好了。 找到根目录下的_config.yml文件，添加如下：1234# Security##encrypt: enable: true 这样就可以使用插件了。 使用在你要加密的文章头部写入password，例如：1234567---title: Hello Worlddate: 2016-03-30 21:18:02password: abc123abstract: Welcome to my blog, enter password to read.message: Welcome to my blog, enter password to read.--- 这样就可以需要输入密码访问了。 Bugs 对于hexo-blog-encrypt2.0之前的版本，无法触发渲染mathjax的函数，需要进行升级。 如果想对TOC进行加密，以next主题为例，将next/layout/_macro/sidebar.swig的文件替换为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197&#123;% macro render(is_post) %&#125;&lt;div class="sidebar-toggle"&gt; &lt;div class="sidebar-toggle-line-wrap"&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-first"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-middle"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-last"&gt;&lt;/span&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside id="sidebar" class="sidebar"&gt; &lt;div class="sidebar-inner"&gt; &#123;% set display_toc = is_post and theme.toc.enable %&#125; &#123;% if page.encrypt == true %&#125; &#123;% if display_toc and toc(page.origin).length &gt; 1 %&#125; &lt;ul class="sidebar-nav motion-element"&gt; &lt;li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" &gt; &#123;&#123; __('sidebar.toc') &#125;&#125; &lt;/li&gt; &lt;li class="sidebar-nav-overview" data-target="site-overview"&gt; &#123;&#123; __('sidebar.overview') &#125;&#125; &lt;/li&gt; &lt;/ul&gt; &#123;% endif %&#125; &#123;% else %&#125; &#123;% if display_toc and toc(page.content).length &gt; 1 %&#125; &lt;ul class="sidebar-nav motion-element"&gt; &lt;li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" &gt; &#123;&#123; __('sidebar.toc') &#125;&#125; &lt;/li&gt; &lt;li class="sidebar-nav-overview" data-target="site-overview"&gt; &#123;&#123; __('sidebar.overview') &#125;&#125; &lt;/li&gt; &lt;/ul&gt; &#123;% endif %&#125; &#123;% endif %&#125; &lt;section class="site-overview sidebar-panel&#123;% if not display_toc or toc(page.content).length &lt;= 1 %&#125; sidebar-panel-active&#123;% endif %&#125;"&gt; &lt;div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"&gt; &lt;img class="site-author-image" itemprop="image" src="&#123;&#123; url_for( theme.avatar | default(theme.images + '/avatar.gif') ) &#125;&#125;" alt="&#123;&#123; theme.author &#125;&#125;" /&gt; &lt;p class="site-author-name" itemprop="name"&gt;&#123;&#123; theme.author &#125;&#125;&lt;/p&gt; &#123;% if theme.seo %&#125; &lt;p class="site-description motion-element" itemprop="description"&gt;&#123;&#123; theme.signature &#125;&#125;&lt;/p&gt; &#123;% else %&#125; &lt;p class="site-description motion-element" itemprop="description"&gt;&#123;&#123; theme.description &#125;&#125;&lt;/p&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;nav class="site-state motion-element"&gt; &#123;% if config.archive_dir != '/' %&#125; &lt;div class="site-state-item site-state-posts"&gt; &lt;a href="&#123;&#123; url_for(theme.menu.archives) &#125;&#125;"&gt; &lt;span class="site-state-item-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/span&gt; &lt;span class="site-state-item-name"&gt;&#123;&#123; __('state.posts') &#125;&#125;&lt;/span&gt; &lt;/a&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;% if site.categories.length &gt; 0 %&#125; &#123;% set categoriesPageQuery = site.pages.find(&#123;type: 'categories'&#125;, &#123;lean: true&#125;) %&#125; &#123;% set hasCategoriesPage = categoriesPageQuery.length &gt; 0 %&#125; &lt;div class="site-state-item site-state-categories"&gt; &#123;% if hasCategoriesPage %&#125;&lt;a href="&#123;&#123; url_for(categoriesPageQuery[0].path) &#125;&#125;"&gt;&#123;% endif %&#125; &lt;span class="site-state-item-count"&gt;&#123;&#123; site.categories.length &#125;&#125;&lt;/span&gt; &lt;span class="site-state-item-name"&gt;&#123;&#123; __('state.categories') &#125;&#125;&lt;/span&gt; &#123;% if hasCategoriesPage %&#125;&lt;/a&gt;&#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &#123;% if site.tags.length &gt; 0 %&#125; &#123;% set tagsPageQuery = site.pages.find(&#123;type: 'tags'&#125;, &#123;lean: true&#125;) %&#125; &#123;% set hasTagsPage = tagsPageQuery.length &gt; 0 %&#125; &lt;div class="site-state-item site-state-tags"&gt; &#123;% if hasTagsPage %&#125;&lt;a href="&#123;&#123; url_for(tagsPageQuery[0].path) &#125;&#125;"&gt;&#123;% endif %&#125; &lt;span class="site-state-item-count"&gt;&#123;&#123; site.tags.length &#125;&#125;&lt;/span&gt; &lt;span class="site-state-item-name"&gt;&#123;&#123; __('state.tags') &#125;&#125;&lt;/span&gt; &#123;% if hasTagsPage %&#125;&lt;/a&gt;&#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &lt;/nav&gt; &#123;% if theme.rss %&#125; &lt;div class="feed-link motion-element"&gt; &lt;a href="&#123;&#123; url_for(theme.rss) &#125;&#125;" rel="alternate"&gt; &lt;i class="fa fa-rss"&gt;&lt;/i&gt; RSS &lt;/a&gt; &lt;/div&gt; &#123;% endif %&#125; &lt;div class="links-of-author motion-element"&gt; &#123;% if theme.social %&#125; &#123;% for name, link in theme.social %&#125; &lt;span class="links-of-author-item"&gt; &lt;a href="&#123;&#123; link &#125;&#125;" target="_blank" title="&#123;&#123; name &#125;&#125;"&gt; &#123;% if theme.social_icons.enable %&#125; &lt;i class="fa fa-fw fa-&#123;&#123; theme.social_icons[name] | default('globe') | lower &#125;&#125;"&gt;&lt;/i&gt; &#123;% endif %&#125; &#123;&#123; name &#125;&#125; &lt;/a&gt; &lt;/span&gt; &#123;% endfor %&#125; &#123;% endif %&#125; &lt;/div&gt; &#123;% set cc = &#123;'by': 1, 'by-nc': 1, 'by-nc-nd': 1, 'by-nc-sa': 1, 'by-nd': 1, 'by-sa': 1, 'zero': 1&#125; %&#125; &#123;% if theme.creative_commons in cc %&#125; &lt;div class="cc-license motion-element" itemprop="license"&gt; &lt;a href="https://creativecommons.org/&#123;% if theme.creative_commons === 'zero' %&#125;publicdomain/zero/1.0&#123;% else %&#125;licenses/&#123;&#123; theme.creative_commons &#125;&#125;/4.0&#123;% endif %&#125;/" class="cc-opacity" target="_blank"&gt; &lt;img src="&#123;&#123; url_for(theme.images) &#125;&#125;/cc-&#123;&#123; theme.creative_commons &#125;&#125;.svg" alt="Creative Commons" /&gt; &lt;/a&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;# Blogroll #&#125; &#123;% if theme.links %&#125; &lt;div class="links-of-blogroll motion-element &#123;&#123; "links-of-blogroll-" + theme.links_layout | default('inline') &#125;&#125;"&gt; &lt;div class="links-of-blogroll-title"&gt; &lt;i class="fa fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;"&gt;&lt;/i&gt; &#123;&#123; theme.links_title &#125;&#125; &lt;/div&gt; &lt;ul class="links-of-blogroll-list"&gt; &#123;% for name, link in theme.links %&#125; &lt;li class="links-of-blogroll-item"&gt; &lt;a href="&#123;&#123; link &#125;&#125;" title="&#123;&#123; name &#125;&#125;" target="_blank"&gt;&#123;&#123; name &#125;&#125;&lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;% include '../_custom/sidebar.swig' %&#125; &lt;/section&gt; &#123;% if page.encrypt == true %&#125; &#123;% if display_toc and toc(page.origin).length &gt; 1 %&#125; &lt;!--noindex--&gt; &lt;section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"&gt; &lt;div class="post-toc"&gt; &#123;% if page.toc_number === undefined %&#125; &#123;% set toc = toc(page.origin, &#123; "class": "nav", list_number: theme.toc.number &#125;) %&#125; &#123;% else %&#125; &#123;% set toc = toc(page.origin, &#123; "class": "nav", list_number: page.toc_number &#125;) %&#125; &#123;% endif %&#125; &#123;% if toc.length &lt;= 1 %&#125; &lt;p class="post-toc-empty"&gt;&#123;&#123; __('post.toc_empty') &#125;&#125;&lt;/p&gt; &#123;% else %&#125; &lt;div class="post-toc-content"&gt;&#123;&#123; toc &#125;&#125;&lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;/section&gt; &lt;!--/noindex--&gt; &#123;% endif %&#125; &#123;% else %&#125; &#123;% if display_toc and toc(page.content).length &gt; 1 %&#125; &lt;!--noindex--&gt; &lt;section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"&gt; &lt;div class="post-toc"&gt; &#123;% if page.toc_number === undefined %&#125; &#123;% set toc = toc(page.content, &#123; "class": "nav", list_number: theme.toc.number &#125;) %&#125; &#123;% else %&#125; &#123;% set toc = toc(page.content, &#123; "class": "nav", list_number: page.toc_number &#125;) %&#125; &#123;% endif %&#125; &#123;% if toc.length &lt;= 1 %&#125; &lt;p class="post-toc-empty"&gt;&#123;&#123; __('post.toc_empty') &#125;&#125;&lt;/p&gt; &#123;% else %&#125; &lt;div class="post-toc-content"&gt;&#123;&#123; toc &#125;&#125;&lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;/section&gt; &lt;!--/noindex--&gt; &#123;% endif %&#125; &#123;% endif %&#125; &#123;% if theme.sidebar.b2t %&#125; &lt;div class="back-to-top"&gt; &lt;i class="fa fa-arrow-up"&gt;&lt;/i&gt; &#123;% if theme.sidebar.scrollpercent %&#125; &lt;span id="scrollpercent"&gt;&lt;span&gt;0&lt;/span&gt;%&lt;/span&gt; &#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt;&lt;/aside&gt;&#123;% endmacro %&#125; 参考文献 hexo博文加密_20180721]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex安装教程]]></title>
    <url>%2Flatex-an-zhuang-jiao-cheng.html</url>
    <content type="text"><![CDATA[前言毕业论文中需要使用Ctex来写，但是之前完全没有接触过这个软件，所以就打算记录一下自己的学习过程。本来打算自己写一下相关的一些东西，但是发现大佬们已经写得特别棒了，就把一些大佬写得东西的链接写出来，希望能帮到有需要的小伙伴们。 关于LaTeX和CTeXLaTeX是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天，甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。 CTeX是TEX中的一个版本，CTeX 指的是CTEX 中文套装的简称。TEX 在不同的硬件和操作系统上有不同的实现版本。这就像C 语言，在不同的操作系统中有不同的编译系统，例如Linux 下的gcc，Windows 下的Visual C++等。有时，一种操作系统里也会有好几种的TEX 系统。常见的Unix/Linux 下的TEX 系统是teTEX，Windows 下则有MiKTEX 和fpTEX。CTeX 指的是CTeX 中文套装的简称，是把MiKTEX 和一些常用的相关工具，如GSview，WinEdt 等包装在一起制作的一个简易安装程序，并对其中的中文支持部分进行了配置，使得安装后马上就可以使用中文。 LaTeX官方网站：https://www.latex-project.org/ CTeX官方网站：http://www.ctex.org/HomePage 下载LaTex软件 点击TeXLive (Unix/Linux/Windows)，选择”download on DVD” 选择“downloading the Tex live ISO image and burning your own DVD” 选择”downloaded form nearby CTAN mirror” 选择”texlive2018.iso” 注意：可以直接单击，如下网址进行下载http://mirrors-wan.geekpie.club/CTAN/systems/texlive/Images/ 安装LaTex软件 管理员权限打开”install-tl-advanced” 选择需要安装的组件 参考文献 LaTeX的下载安装及简易使用_20180515 latex安装教程_20190114]]></content>
      <categories>
        <category>LaTex</category>
      </categories>
      <tags>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo文章中图片点击全屏查看]]></title>
    <url>%2Fhexo-wen-zhang-zhong-tu-pian-dian-ji-quan-ping-cha-kan.html</url>
    <content type="text"><![CDATA[hexo中实现类似于微信公众号中，点击图片方法全屏查看，再点击后回到原来的状态。 效果如下,点击下图的图片放大查看。123|1|2|3||--|--|--||![](http://qnfile.devzhao.com/blog/2018-10-16-155439.png)|![](http://qnfile.devzhao.com/blog/2018-10-16-155529.png)|![](http://qnfile.devzhao.com/blog/2018-10-16-155650.jpg)| 实现的效果如下： 1 2 3 1.修改 post-details.js 文件 文件目录：123devzhao.com/themes/next/source/js/src/post-details.jsdevzhao.com是我的站点根目录。 在文件最后添加：123456789101112131415161718192021222324252627//----自定义js----------------function createImgEventFullScreen() &#123; var imgs = $(".post-body").find("img"); console.log(imgs); for(var i = 0;i &lt; imgs.length;i++) &#123; // $(imgs[i]).click(createCover(imgs[i])); imgs[i].onclick= function(e) &#123; var src = e.srcElement.currentSrc; createCover(src) &#125; &#125; function createCover (src) &#123; console.log(src); var cover = $("&lt;div id='fullScreenCover' class='zhao-cover-img-container'&gt;&lt;img class='zhao-cover-img' src='"+src+"'/&gt;&lt;/div&gt;"); $("#fullScreenCover").remove(); $("body").append(cover); $("body").addClass("zhao-no-scroll"); $("#fullScreenCover").click(function()&#123; $("#fullScreenCover").remove(); $("body").removeClass("zhao-no-scroll"); &#125;) &#125;&#125;setTimeout(function()&#123; createImgEventFullScreen();&#125;,1000) 2.修改custom.styl文件 文件目录： 1devzhao.com/themes/next/source/css/_custom/custom.styl 在最后添加 1234567891011121314151617181920212223.zhao-cover-img-container&#123; position: fixed; top: 0; left: 0; width: 100%; height: 100vh; z-index: 10002; text-align: center; background-color: #000; overflow-y: scroll;&#125;.zhao-cover-img&#123; width: 100%; position: absolute; top: 0; bottom: 0;&#125;.zhao-no-scroll&#123; width: 100%; height: 100vh; overflow: hidden;&#125; 3.修改blog-encrypt.js文件 如果你安装了hexo-blog-encrypt插件，需要做这一步。 文件目录：1devzhao.com/node_modules/hexo-blog-encrypt/lib/blog-encrypt.js 找到如下代码1$('#encrypt-blog').html(content); 在后面加上这行代码：1createImgEventFullScreen(); 如下图: 上面代码还有些问题： 还有点小问题，比如第三个图片，电脑上打开，头顶上和底部有一些显示不全。 在微信中打开第二个图，向上滑的时候，底下的层会跟着滑动（好像在微信里面，禁用滑动，没生效），手机Safari中也没有生效 参考文献 hexo文章中图片点击全屏查看]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 添加背景图片并自适应]]></title>
    <url>%2Fhexo-tian-jia-bei-jing-tu-pian-bing-zi-gua-ying.html</url>
    <content type="text"><![CDATA[修改过程 在站点配置文件夹source/images/放入你的背景图片 然后修改主题文件夹themes/source/css/_custom/custom.stylPS: 这个文件是存放用户自定义css样式的在custom.styl开头加入如下的代码 body { background:url(/images/background.jpg); background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%; background-size: cover; -webkit-background-size: cover; -o-background-size: cover; -moz-background-size: cover; -ms-background-size: cover; /*这是设置底部文字, 看个人需要修改*/ #footer > div > div { color:#eee; } } 参考文献 Hexo 添加背景图片并自适应_20181231]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo里面Markdown对图片进行居中、设置大小]]></title>
    <url>%2Fhexo-li-mian-markdown-dui-tu-pian-jin-xing-ju-zhong-she-zhi-da-xiao.html</url>
    <content type="text"><![CDATA[标准的markdown中提供了图片的设置大小的操作，但是在本地把图片编辑好的代码生成静态文件发现图片不能显示了。 以后的hexo版本希望能够解决这个问题。 一般的markdown语法设置图片大小标准的markdown，在图片后面加上 “=200x300” (200是宽度，300是高度，也可以之定义宽度)，就能实现下面的效果。但是此版本的hexo不行。1![](https://mir-s3-cdn-cf.behance.net/project_modules/max_1200/36aa3462013499.5b14f258e6347.jpg =200x300) 图片的宽度和高度同时修改：1&lt;img width=80% src="http://qnfile.devzhao.com/blog/2018-10-22-120649.png" &gt; 1&lt;img width=500 src="http://qnfile.devzhao.com/blog/2018-10-22-120649.png" &gt; 使用img标签，设置图片大小直接在markdown中使用HTML标签，多数markdown工具都支持。1&lt;img width=200 src="http://qnfile.devzhao.com/blog/2018-10-22-jhk-1540210533410.jpeg" &gt; 当然简单学习一下html和css，你能做出更好的效果。 图片居中markdown目前没有支持图片居中的，所有的图片要么是100%宽度，支持设置大小的markdown一般也是左对齐的，所以居中只能靠HTML标签了。123&lt;div align=center&gt;![](http://qnfile.devzhao.com/blog/2018-10-22-jhk-1540210533410.jpeg =200x)&lt;/div&gt; 效果如下： 或者：123&lt;div align=center&gt; &lt;img width=200 src="http://qnfile.devzhao.com/blog/2018-10-22-jhk-1540210533410.jpeg" &gt;&lt;/div&gt; 效果如下： 没有解决，图片的左对齐，右对齐。默认地，图片统一居中显示。123&lt;div align=left&gt;![](http://qnfile.devzhao.com/blog/2018-10-22-jhk-1540210533410.jpeg =200x)&lt;/div&gt; 效果如下： 或者：123&lt;div align=left&gt; &lt;img width=200 src="http://qnfile.devzhao.com/blog/2018-10-22-jhk-1540210533410.jpeg" &gt;&lt;/div&gt; 效果如下： 参考文献 hexo里面Markdown对图片进行居中、设置大小_20181022]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next搭建个人博客（代码块复制功能）]]></title>
    <url>%2Fhexo-next-da-jian-ge-ren-bo-ke-dai-ma-kuai-fu-zhi-gong-neng.html</url>
    <content type="text"><![CDATA[为了提高博客代码块的用户体验，仅仅代码高亮还不行，最好还能一键复制代码。故此文将讲述Hexo NexT主题博客的代码块复制功能配置。 下载 clipboard.js三方插件 clipboardjs ，相关介绍和兼容性我就不赘述了，去它主页或github上看。 下载地址： clipboard.js clipboard.min.js 推荐 保存文件clipboard.js / clipboard.min.js，目录如下：.\themes\next\source\js\src clipboardjs 使用也是在.\themes\next\source\js\src目录下，创建clipboard-use.js，文件内容如下：123456789101112131415161718 /*页面载入完成后，创建复制按钮*/ !function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; //fa fa-globe可以去字体库替换自己想要的图标copyHtml += ' &lt;i class="fa fa-clipboard"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode(); &#125;(window, document); 在.\themes\next\source\css_custom\custom.styl样式文件中添加下面代码： 123456789101112131415161718192021222324252627282930313233343536 //代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 引用在.\themes\next\layout\_layout.swig文件中，添加引用（注：在 swig 末尾或 body 结束标签（&lt;/body&gt;）之前添加）：123&lt;!-- 代码块复制功能 --&gt;&lt;script type="text/javascript" src="/js/src/clipboard.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="/js/src/clipboard-use.js"&gt;&lt;/script&gt; 补充懂代码的也可以将clipboard.min.js和clipboard-use.js合并为一个文件，再在.\themes\next\layout\_layout.swig文件中使用。当然clipboard.min.js也可以直接用三方cdn的方式引入也行。 参考文献 Hexo-Next搭建个人博客（代码块复制功能）]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字数统计和阅读时长(网站底部/文章内)]]></title>
    <url>%2Fzi-shu-tong-ji-he-yue-du-shi-chang-wang-zhan-di-bu-wen-zhang-nei.html</url>
    <content type="text"><![CDATA[字数统计和阅读时长（旧版本新版本） 插件地址: hexo-symbols-count-time 安装插件: 1npm install hexo-symbols-count-time --save 修改 站点配置文件 1234567symbols_count_time: #文章内是否显示 symbols: true time: true # 网页底部是否显示 total_symbols: true total_time: true 修改 主题配置文件 123456789101112# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: true #文章中的显示是否显示文字（本文字数|阅读时长） item_text_post: true #网页底部的显示是否显示文字（站点总字数|站点阅读时长） item_text_total: false # Average Word Length (chars count in word) awl: 4 # Words Per Minute wpm: 275 参考文献 字数统计和阅读时长]]></content>
  </entry>
  <entry>
    <title><![CDATA[在Hexo中添加LaTex公式]]></title>
    <url>%2Fzai-hexo-zhong-tian-jia-latex-gong-shi.html</url>
    <content type="text"><![CDATA[在Hexo中渲染MathJax数学公式在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 代码编辑器，强烈推荐使用微软的 VS code，相比Atom开启迅速，使用方便，扩展丰富 原因Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线&#39;_&#39;代表斜体，会被渲染引擎处理为标签。 因为类Latex格式书写的数学公式下划线&#39;_&#39;表示下标，有特殊的含义，如果被强制转换为标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，$x_i$在开始被渲染的时候，处理为$xi$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。 类似的语义冲突的符号还包括&#39;*&#39;,&#39;{&#39;, &#39;}&#39;,&#39;\&#39;等。 解决方法解决方案有很多，可以网上搜下，为了节省大家的时间，这里只提供亲身测试过的最靠谱的方法。 第一步： 安装Kramed更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 第二步：更改文件配置打开/node_modules/hexo-renderer-kramed/lib/renderer.js，更改为，直接返回text： 改动前： 12345// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125; 改动后： 1234// Change inline math rulefunction formatText(text) &#123; return text;&#125; 第三步: 停止使用 hexo-math，并安装mathjax包卸载hexo-math1npm uninstall hexo-math --save 安装 hexo-renderer-mathjax 包1npm install hexo-renderer-mathjax --save 第四步: 更新 Mathjax 的 配置文件打开/node_modules/hexo-renderer-mathjax/mathjax.html如图所示更改&lt;script&gt;为：即注释掉&lt;script&gt;代码，并把以下代码复制到对应位置 更改前： 1&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; 更改后： 1&lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 整体效果：12&lt;!-- &lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; --&gt;&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"&gt;&lt;/script&gt; 第五步: 更改默认转义规则因为LaTeX与markdown语法有语义冲突，所以hexo默认的转义规则会将一些字符进行转义，所以我们需要对默认的规则进行修改.打开/node_modules\kramed\lib\rules\inline.js 更改前： 1escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改后： 1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 2. 更改前： 1em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改后： 1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 第六步: 开启mathjax打开/themes/yilia主题目录下的config.yml文件因为我用的yilia主题，所以路径是/themes/yilia 我们需要在_config.yml文件 中开启 Mathjax， 找到 mathjax 字段添加如下代码：(不同的主题配置方法略微有区别)12mathjax: enable: true 或者1mathjax: true 此外，还需要，更改当前文件下math: true 更改前： 12math: enable: false 更改后： 12math: enable: true 需要注意的是：无论是配置文件还是博客文件，配置项跟配置参数均有有一个空格，否则会配置失败例如：123mathjax: true（mathjax:空格true）而不是mathjax:true（mathjax:true） 写博客文件时，要开启 Mathjax选项，， 添加以下内容：1mathjax: true 例如：1234title: 特征提取——局部特征date: 2018-07-16 09:39:40tags: [GitHub, Mysql]mathjax: true 结束语通过以上步骤，我们就可以在 hexo 中使用 Mathjax 来书写数学公式。 参考文献 使用LaTex添加公式到Hexo博客里_20180803 在Hexo中渲染MathJax数学公式_20161229]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>LaTex</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在markdown中插入数学公式]]></title>
    <url>%2Fzai-markdown-zhong-cha-ru-shu-xue-gong-shi.html</url>
    <content type="text"><![CDATA[一些扩展的Markdown语法支持采用LaTex语法写数学公式，而在网页中使用Mathjax插件来显示数学公式。 本教程介绍如何在Markdown中书写数学公式。 插入数学公式在Markdown中插入数学公式的语法是$数学公式$和$$数学公式$$。 行内公式是可以让公式在文中与文字或其他东西混编，不独占一行。 实例 1质能方程$E = mc^2$ 显示质能方程 $E = mc^2$]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决Chrome浏览器中Flash Player无法播放问题]]></title>
    <url>%2Fjie-jue-chrome-liu-lan-qi-zhong-flash-player-wu-fa-bo-fang-wen-ti.html</url>
    <content type="text"><![CDATA[问题原因：因为Flash相对于HTML5，有着运算效率低、资源占用大、安全性不高等缺点，随着HTML5越来越普及，Adobe已宣布2020年正式停止支持Flash这项技术。 但Adobe公司为了利益最大化，从Flash Player 30开始，就与国内某公司合作，推出中国特供版Flash Player，根据特供版用户协议，Flash Player将捆绑应用、广告等推广业务，同时会获取用户信息并披露给第三方，用户不能追究数据泄漏的法律责任，相当霸王的条款。如果中国大陆用户安装的是国际版的Flash Player，那打开Flash时会提示“此Flash Player与您的地区不相容，请重新安装Flash”并拒绝播放Flash，一定要下载并安装特供版才予以正常播放（不知这是否是侵犯消费者的权益）。 解决办法： 卸载本机所有Flash Player 29.0.0.171及之后的版本，推荐使用“uninstall_flash_player”一次性搞定； 安装Flash Player 29.0.0.140（安装时记得断网，不然还是会自动跳转到最新版的下载地址，真是无。。。！不同浏览器安装包不同，如果网上找不到请联系KP君获取全浏览器版） 在安装最后一个界面，选择“从不检查更新”，或是在“控制面板”-“Flash Player 设备管理器”－“更新”页面，选择“从不检查更新”。 特别值得一提的是，Flash Player 29.0.0.171安装过程已取消最后一个界面，当全部更新为29.0.0.171后，“更新”页面也消失了，可理解为Flash Player 29.0.0.171屏蔽了用户对更新的选择权，不再有“从不检查更新”了，也就是可能会强制更新到特供版。 本以为这样安装完后就可以顺利使用，结果还是提示错误，差点吐血，只好再找找攻略，还好有发现： Chrome地址栏输入：chrome://version/，可以看到Flash的信息 1Flash 32.0.0.114 C:\Users\KPlayer\AppData\Local\Google\Chrome\User Data\PepperFlash\32.0.0.114\pepflashplayer.dll 找到之前安装的29版本的pepflashplayer.dll文件，通常在： 1C:\Windows\System32\Macromed\Flash\pepflashplayer64_29_0_0_140.dll 关闭Chrome，将pepflashplayer64_29_0_0_140.dll替换之前的pepflashplayer.dll，并重命名 重新打开Chrome，即可正常浏览Flash。 结论：Flash Player 29.0.0.140百度网盘下载链接:https://pan.baidu.com/s/1WKYEUemCfRP_b1KPcXohag 密码: r4i5 这或许是我们还会安装的最后一个版本！期待早日告别Flash这项落后的技术，期待Flash Player不尊重消费者的行为早点被唾弃！ 附录：V30前最后的几个版本号（发布日期：2018/5/8）Flash Player 29.0.0.171 (422 MB)（发布日期：2018/4/10）Flash Player 29.0.0.140 (422 MB)（发布日期：2018/3/13）Flash Player 29.0.0.113 (405 MB)（发布日期：2018/2/6）Flash Player 28.0.0.161 (408 MB)（发布日期：2018/1/9）Flash Player 28.0.0.137 (406 MB)（发布日期：2017/12/12）Flash Player 28.0.0.126 (405 MB)（发布日期：2017/11/14）Flash Player 27.0.0.187 (403 MB)（发布日期：2017/10/25）Flash Player 27.0.0.183 (404 MB)（发布日期：2017/10/16）Flash Player 27.0.0.170 (404 MB)（发布日期：2017/10/10）Flash Player 27.0.0.159 (405 MB)（发布日期：2017/9/12）Flash Player 27.0.0.130 (405 MB)（发布日期：2017/8/8）Flash Player 26.0.0.151 (404 MB)（发布日期：2017/7/11）Flash Player 26.0.0.137 (404 MB)（发布日期：2017/6/16）Flash Player 26.0.0.131 (404 MB)（发布日期：2017/6/13）Flash Player 26.0.0.126 (404 MB) 参考地址： 【教程】解决Chrome浏览器“此Flash Player与您的地区不相容，请重新安装Flash”报错 提示“此Flash Player与您的地区不相容，请重新安装Flash”的解决办法]]></content>
      <categories>
        <category>Chrome</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
        <tag>Flash Player</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加字数统计、阅读时长]]></title>
    <url>%2Fhexo-tian-jia-zi-shu-tong-ji-yue-du-shi-chang.html</url>
    <content type="text"><![CDATA[打开hexo目录，运行git bush，安装插件 1$ npm install hexo-symbols-count-time --save 修改博客网站配置文件，添加以下代码 12345symbols_count_time: symbols: true time: true total_symbols: true total_time: true 修改主题配置文件，搜索symbols_count_time，快速定位，修改成以下代码 123456symbols_count_time: separated_meta: true item_text_post: true item_text_total: false awl: 4 wpm: 275 重启服务 123hexo cleanhexo ghexo s]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2Fmarkdown-yu-fa.html</url>
    <content type="text"><![CDATA[主要内容： Markdown是什么？谁创造了它？为什么使用它？谁在用它?怎么使用它？ 正文1. Markdown是什么?Markdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。Markdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。 Markdown是一种纯文本格式的标记语言。通过简单的标记语法，它可以使普通文本内容具有一定的格式。相比WYSIWYG编辑器● 优点：1、因为是纯文本，所以只要支持Markdown的地方都能获得一样的编辑效果，可以让作者摆脱排版的困扰，专心写作。2、操作简单。比如:WYSIWYG编辑时标记个标题，先选中内容，再点击导航栏的标题按钮，选择几级标题。要三个步骤。而Markdown只需要在标题内容前加#即可● 缺点：1、需要记一些语法（当然，是很简单。五分钟学会）。2、有些平台不支持Markdown编辑模式。 还好，简书是支持Markdown编辑模式的。1开启方式：设置-&gt;默认编辑器-&gt;Markdown编辑器 2.谁创造了它？它由Aaron Swartz和John Gruber共同设计，Aaron Swartz就是那位于去年（2013年1月11日）自杀,有着开挂一般人生经历的程序员。维基百科对他的介绍是：软件工程师、作家、政治组织者、互联网活动家、维基百科人。 14岁参与RSS 1.0规格标准的制订。 2004年入读斯坦福，之后退学。 2005年创建Infogami，之后与Reddit合并成为其合伙人。 2010年创立求进会（Demand Progress），积极参与禁止网络盗版法案（SOPA）活动，最终该提案被撤回。 2011年7月19日，因被控从MIT和JSTOR下载480万篇学术论文并以免费形式上传于网络被捕。 2013年1月自杀身亡。 3.为什么要用它？ 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。 兼容HTML，可以转换为HTML格式发布。 跨平台使用。 越来越多的网站支持Markdown。 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail） 摆脱Word（我不是认真的）。 4. 谁在用它？Markdown的使用者： GitHub 简书 Stack Overflow Apollo Moodle Reddit 等等 5.怎么使用它？如果不算扩展，Markdown的语法绝对简单到让你爱不释手。Markdown语法主要分为如下几大部分：标题，段落，区块引用，代码区块，强调，分割线，链接，图片，反斜杠\，符号~等。 一、标题两种方式：● 方式1使用=和-标记一级和二级标题。1234一级标题=======二级标题------- 运行结果，如下： 一级标题二级标题● 方式2：使用#，可表示1-6级标题。 在想要设置为标题的文字前面加#来表示一个#是一级标题，二个#是二级标题，以此类推。支持六级标题。注：标准语法一般在#后跟个空格再写文字，貌似简书不加空格也行。示例：12345678```# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题``` 运行结果，如下：123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。段落：以一个空行开始，以一个空行结束，中间的就是一个段落。 二、字体● 加粗要加粗的文字左右分别用两个*号包起来1**加粗的字体** 运行结果，如下：加粗的字体● 斜体要倾斜的文字左右分别用一个*号包起来1*要倾斜的字体* 运行结果，如下：要倾斜的文字● 斜体加粗要倾斜和加粗的文字左右分别用三个*号包起来1***倾斜加粗的字体*** 运行结果，如下：倾斜加粗的字体● 删除线要加删除线的文字左右分别用两个~~号包起来1~~删除线的使用~~ 运行结果，如下：删除线的使用 三、引用在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;，n个···貌似可以一直加下去，但没神马卵用12345&gt;这是引用的内容1&gt;&gt;这是引用的内容2&gt;&gt;&gt;这是引用的内容3&gt;&gt;&gt;&gt;这是引用的内容4&gt;&gt;&gt;&gt;&gt;这是引用的内容5 运行结果，如下： 这是引用的内容1 这是引用的内容2 这是引用的内容3 这是引用的内容4 这是引用的内容5 四、分割线三个或者三个以上的 - 或者 * 都可以。1234-------******* 运行结果，如下： 五、图片1![图片alt](图片地址 &quot;图片title&quot;) 其中，图片alt就是显示图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加也可不加。示例：1![宠物](http://baiducdn.pig66.com/uploadfile/2017/0511/20170511074935785.jpg &quot;宠物狗狗&quot;) 运行结果，如下： 六、超链接语法：1[超链接名字](超链接地址 &quot;超链接title&quot;) 其中，”超链接title”可加也可不加。示例：12[简书](http://jianshu.com &quot;访问简书主页&quot;)[百度](http://www.baidu.com &quot;访问百度主页&quot;) 运行结果，如下：简书百度注：Markdown本身语法不支持链接在新页面中打开，貌似简书做了处理，是可以的。别的平台可能就不行了，如果想要在新页面中打开的话可以用html语言的a标签代替。1&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt; 示例1&lt;a href=&quot;https://www.jianshu.com/u/1f5ac0cf6a8b&quot; target=&quot;_blank&quot;&gt;简书&lt;/a&gt; 七、列表● 有序列表有序列表：使用 数字 加一个英文句点.123451. 有序列表2. 有序列表3. 有序列表4. 有序列表5. 有序列表 运行结果，如下： 有序列表 有序列表 有序列表 有序列表 有序列表 ● 无序列表语法：无序列表用&quot; - + *&quot;任何一种都可以123- 列表内容1+ 列表内容2* 列表内容3 注意：- + * 跟内容之间都要有一个空格，且结束列表时，需要中间空一行。运行结果，如下： 列表内容1 列表内容2 列表内容3 ● 列表嵌套上一级和下一级之间敲三个空格即可1234- 一级无序列表内容1 - 二级无序列表内容2 - 二级无序列表内容2 - 二级无序列表内容2 运行结果，如下： 一级无序列表内容1 二级无序列表内容2 二级无序列表内容2 二级无序列表内容2 1234+ 一级无序列表内容1 + 二级无序列表内容2 + 二级无序列表内容2 + 二级无序列表内容2 运行结果，如下： 一级无序列表内容1 二级无序列表内容2 二级无序列表内容2 二级无序列表内容2 1234* 一级无序列表内容1 * 二级无序列表内容2 * 二级无序列表内容2 * 二级无序列表内容2 运行结果，如下： 以及无序列表内容1 二级无序列表内容2 二级无序列表内容2 二级无序列表内容2 特别注意，如下这种情况： 一级无序列表内容1 二级无序列表内容2 二级无序列表内容2 二级无序列表内容2 三级无序列表内容3 三级无序列表内容3 三级无序列表内容3 二级无序列表内容2 一级无序列表内容1（不知道为什么？加入字体高亮度显示时，会出错） 运行结果，如下： 一级无序列表内容1 二级无序列表内容2 二级无序列表内容2 二级无序列表内容2 三级无序列表内容3 三级无序列表内容3 三级无序列表内容3 二级无序列表内容2 一级无序列表内容1 注意：这里想说明的是不同级别之间要有换行，如果不换行，默认还是上一级别。但是，这里实验可以发现这个问题已经不存在啦。 八、表格语法：1234表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容 说明：第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左■两边加：表示文字居中■右边加：表示文字居右注：原生的语法两边都要用 | 包起来。此处省略示例：12345姓名|技能|排行----|:--:|--:刘备|哭|大哥关羽|打|二哥张飞|骂|二哥 运行结果，如下：（表语与上文要空一行，否则会错乱） 姓名 技能 排行 刘备 哭 大哥 关羽 打 二哥 张飞 骂 二哥 九、代码语法：● 单行代码：代码之间分别用一个反引号包起来1`代码内容` ● 代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行12345``` 代码... 代码... 代码...``` 示例：单行代码1`create database hero;` 运行结果，如下：create database hero;代码块123456``` function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;; &#125; fun();``` 运行结果，如下：1234function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;;&#125;fun(); 十、流程图123456789```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp;``` 运行结果，如下：12345678st=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; (注意，这里并未打印出流程图，不知道什么原因？) 十一、显示网址● 写法1：123超链接：[连接名称](网址 , 标题)[我是链接名](http://www.izhangbo.cn, "我是标题")[&lt;i class="icon-refresh"&gt;&lt;/i&gt; 点我刷新](/sonfilename/) ● 写法2：1234另一种超链接写法：[链接名][链接代号][here][3]然后在别的地方定义 3 这个详细链接信息，[3]: http://www.izhangbo.cn "聚牛团队" 直接展示链接的写法：1&lt;http://www.izhangbo.cn&gt; 十二、反引号code格式：反引号Use the printf() function.1``There is a literal backtick (`) here.针对在代码区段内插入反引号的情况`` There is a literal backtick (`) here.针对在代码区段内插入反引号的情况 十三、缩进markdown段落中如何产生缩进？因为markdown的段落定义是由一个或多个连续的文本组成，中间的多个空格和tab会被认为是一个空格。但是有时候确实需要这样的空格或tab缩进格式时怎么办? 方法1：（有效，注意后面的分号）直接写半方大的空白&ensp;或&#8194;全方大的空白&emsp;或&#8195;不断行的空白格&nbsp;或&#160; 方法2：（无效）输入法切换到全角，双击空格键~搜狗输入法切换全角与半角的快捷键：Shift + 空格，即可切换半角/全角。按住Shift，点击空格一次，即可互相切换。这个方法我遇到一个问题：首段不能缩进！ 方法3：（无效） 1&gt; your text 这个是引用的样式 方法4：（无效）1&lt;a&gt; test&lt;/a&gt; 参考文献Markdown基本语法 网址：https://www.jianshu.com/p/191d1e21f7edyounghz/Markdown 网址：https://github.com/younghz/Markdown掌握这几种 Markdown 语法Markdown 语法说明 (简体中文版)]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中插入图片的方法]]></title>
    <url>%2Fhexo-zhong-cha-ru-tu-pian-de-fang-fa.html</url>
    <content type="text"><![CDATA[在写文章时，常常有配图说明的需求。Hexo有多种图片插入方式，可以将图片存放在本地引用或者将图片放在CDN上引用。 插入本地图片绝对路径本地图片统一放在source/images文件夹中。 使用Markdown语法1231 ![](/images/test.jpg)2 helllsfl3 fslfml 注意： ●images的名字可以改变，比如images0,images1,等 ●images前面的”/“，必须要有，且不可写source，格式固定。 使用Html语法1&lt;img src="/images/test.jpg"&gt; 相对路径使用插件1、把根目录下的配置文件_config.yml里的post_asset_folder选项设置为true；1234567891011121314151617181920212223242526272829301 # 文章配置2 # 这个地方一般直接取默认值不用修改3 # Writing4 new_post_name: :title.md # File name of new posts # 新文章的文件名称5 default_layout: post # 预设布局6 titlecase: false # Transform title into titlecase # 把标题转换为 titlecase（titlecase指的是将每个单词首字母转换成大写）7 external_link: true # Open external links in new tab # 在新标签中打开链接8 filename_case: 0 # 把文件名称转换为 (1) 小写或 (2) 大写, 0表示不变9 render_drafts: false # 显示草稿10 post_asset_folder: true # 启动 Asset 文件夹11 relative_link: false # 启动 Asset 文件夹12 future: true # 显示未来的文章13 highlight: # 代码块的设置14 enable: true15 line_number: true # 是否显示行号16 auto_detect: false # 是否自动监测17 tab_replace: # 将 tab 替换成其他字符串``` 2、在你的hexo目录下执行这句话npm install hexo-asset-image --save，这是下载安装一个可以上传本地图片的插件：``` bashhuangsengolndeMacBook-Pro:hexo-blog huangaengoln$ npm install hexo-asset-image --save(node:59464) fs: re-evaluating native module sources is not supported. If you are using the graceful-fs module, please update it to a more recent version.hexo-site@0.0.0 /Users/huangaengoln/Documents/hexo-blog└─┬ hexo-asset-image@0.0.3 └─┬ cheerio@0.19.0 ├─┬ css-select@1.0.0 │ ├── css-what@1.0.0 │ └── domutils@1.4.3 └── lodash@3.10.1 huangsengolndeMacBook-Pro:hexo-blog huangaengoln$ 3、下载完成后，下次再执行hexo n “1How-to-change-App-theme-color”来生成md博文时，/source/_posts文件夹内除了1How-to-change-App-theme-color.md文件，还有一个与之同名的文件夹；4、最后在1How-to-change-App-theme-color.md中想引入图片时，先把图片复制到md对应的文件夹1How-to-change-App-theme-color中，然后只需要1How-to-change-App-theme-color.md中按照markdown的格式引入图片：11 ![你想输入的替代文字](xxxx/图片名.jpg) 注意：xxxx是这个md文件的名字，也是同名文件夹的名字，你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。5、最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是插入网络图片使用Markdown语法1![](网络图片地址) 使用Html语法1&lt;img src="网络图片地址"&gt; 使用图床可以把你的图片上传到 七牛之类的服务器，然后直接按照markdown的方式使用。例如可以使用github作为图床， CDN引用除了在本地存储图片，还可以将图片上传到一些免费的CDN服务中。比如Cloudinary提供的图片CDN服务，在Cloudinary中上传图片后，会生成对应的url地址，将地址直接拿来引用即可。(来源：Yan Yinhong)]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github与Hexo搭建个人博客]]></title>
    <url>%2Fshi-yong-github-yu-hexo-da-jian-ge-ren-bo-ke.html</url>
    <content type="text"><![CDATA[摘要：这是一篇有关如何使用Github Pages和Hexo搭建自己独立博客的详尽教程，里面介绍了如何使用和配置Hexo框架，如何将Hexo部署到自己的Github项目中，域名注册，以及域名的绑定，还有我在搭建自己博客过程中所遇到的各种困难。 前言一名安卓入门开发者，但，我只是入门！而且我对网站开发以及前端的知识几乎是零基础，所以在自己刚接触这个东西的时候，我像很多人一样，都是上网找教程，但是要知道，那都是程序员的教程。所以对于我这个网站技术小白来说，真是很难受，所以藉此机会写一篇让小白看得懂的教程。如果你是一个小白而且又想做自己的博客，可以，请跟着我的脚步，我会带你真真正正做一个属于你自己的博客。 入门门槛 必须耐得住折腾。 刻苦的学习精神和耐心 关于Github一、Github的优点 GitHub是基于git实现的代码托管。git可能是目前最好用的版本控制系统了，非常受欢迎。 GitHub可以免费使用，并且快速稳定。 Github上面的世界很精彩，用久了你的眼界会开阔很多。 二、什么是Github PagesGithub Pages可以被认为是用户编写的、托管在github上的静态网页。 三、为什么要使用Github Pages 可以绑定你的域名(但暂时貌似只能绑定一个)。 简单快捷，使用Github Pages可以为你提供一个免费的服务器，免去了自己搭建服务器和写数据库的麻烦。 安装Node.js在 Windows 环境下安装 Node.js 非常简单，仅须到官网下载安装文件并执行即可完成安装。像我的是Windows 64位，直接下载安装，无脑下一步就行了，不需要配置环境变量。 安装Git去Git官网根据你的电脑参数，下载对应版本。 下载完成，通过在命令行输入git version查看是否安装成功，有输出版本号说明安装成功。 鼠标邮件菜单里就多了Git GUI Here和Git Bash Here两个按钮，一个是图形界面的Git操作，一个是命令行，我们选择Git Bash Here。 安装教程：如何在windows下安装Git Git入门教程: Pro Git(中文版) Git基本操作： HexoHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 一、Hexo安装桌面右键鼠标，点击Git Bash Here，输入npm命令即可安装12npm install hexo-cli -gnpm install hexo-deployer-git --save 第一句是安装hexo，第二句是安装hexo部署到git page的deployer，两个都需要安装。如下图即安装完成。 二、Hexo初始化配置创建Hexo文件夹安装完成后，根据自己喜好建立目录（如F:\Blog\Hexo），直接进入F:\Blog\Hexo文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，执行以下操作。1$ hexo init 安装 Hexo 完成后，Hexo 将会在指定文件夹中新建所需要的文件。Hexo文件夹下的目录如下： 本地查看效果执行下面语句，执行完即可登录localhost:4000查看效果12hexo generatehexo server 登录localhost:4000，即可看到本地的效果如下： 将博客部署到Github Pages上那么现在本地的博客已经搭建起来了，但是我们只可以通过本地连接查看我们的博客。那么我们现在需要做的就是把本地的博客发布到服务器上，让别人也可以连接我们的博客，而Github Pages就帮我完成了这件事情。但是Github Pages的代码就是寄存在Github上面的。那么接下来我们需要在Github上面创建一个新的项目。 一、注册Github账户 访问Github首页 点击右上角的Sign Up，注册自己的账户 二、创建项目代码库 注册完登陆后，我们就创建一个我们自己的Github Pages项目。点击New repository。 创建要点如下： 三、配置SSH密钥配置Github的SSH密钥可以让本地git项目与远程的github建立联系，让我们在本地写了代码之后直接通过git操作就可以实现本地代码库与Github代码库同步。操作如下： 第一步、看看是否存在SSH密钥(keys)首先，我们需要看看是否看看本机是否存在SSH keys,打开Git Bash,并运行:1$ cd ~/. ssh 检查你本机用户home目录下是否存在.ssh目录 如果，不存在此目录，则进行第二步操作，否则，你本机已经存在ssh公钥和私钥，可以略过第二步，直接进入第三步操作。 第二步、创建一对新的SSH密钥(keys)1234$ssh-keygen -t rsa -C "your_email@example.com"#这将按照你提供的邮箱地址，创建一对密钥Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter] 直接回车，则将密钥按默认文件进行存储。此时也可以输入特定的文件名，比如/c/Users/you/.ssh/github_rsa 接着，根据提示，你需要输入密码和确认密码（说到这里，如果你很放心，其实可以不用密码，就是到输密码的地方，都直接回车，所以每次push就只管回车就行了。所谓的最安全的密码，就是没有密码 哈哈）。相关提示如下： 12Enter passphrase (empty for no passphrase): [Type a passphrase]Enter same passphrase again: [Type passphrase again] 输入完成之后，屏幕会显示如下信息： 1234Your identification has been saved in /c/Users/you/.ssh/id_rsa.Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub.The key fingerprint is:01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com 第三步、在GitHub账户中添加你的公钥运行如下命令，将公钥的内容复制到系统粘贴板(clipboard)中。1clip &lt; ~/.ssh/id_rsa.pub 接着： 登陆GitHub,进入你的Account Settings. 选择SSH Keys 粘贴密钥，添加即可 第四步、测试可以输入下面的命令，看看设置是否成功，git@github.com的部分不要修改：1$ ssh -T git@github.com 如果是下面的反馈： 123The authenticity of host 'github.com (207.97.227.239)' can't be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? 不要紧张，输入yes就好，然后会看到： 第五步、设置用户信息现在你已经可以通过SSH链接到GitHub了，还有一些个人信息需要完善的。Git会根据用户的名字和邮箱来记录提交。GitHub也是用这些信息来做权限的处理，输入下面的代码进行个人信息的设置，把名称和邮箱替换成你自己的，名字根据自己的喜好自己取，而不是GitHub的昵称。 12$ git config --global user.name "ryanlijianchang"//用户名$ git config --global user.email "liji.anchang@163.com"//填写自己的邮箱 第六步、SSH Key配置成功本机已成功连接到github。 四、将本地的Hexo文件更新到Github的库中第一步、登录Github打开自己的项目 username.github.io 第二步、打开之后，点击SSH，选择SSH类型地址 第三步、复制地址 第四步、打开你一开始创建的Hexo文件夹（如F:\Blog\Hexo），用记事本打开刚文件夹下的_config.yml文件 第五步、在配置文件里作如下修改，保存 第六步、在Hexo文件夹下执行：12hexo ghexo d 或者直接执行1hexo g -d 执行完之后会让你输入github的账号和密码，输入完后就可以登录我们自己的部署在Github Pages服务器上的博客了。对应的地址是 username.github.io(我的是：ryanlijianchang.github.io)。 假如这时候，报错 ERROR Deployer not found: git，那么就是你的deployer没有安装成功，你需要执行如下命令再安装一次： 1npm install hexo-deployer-git --saved 这样，你再执行hexo g -d，你的博客就部署到Github上了。 第七步、在浏览器上输入自己的主页地址 在浏览器上输入Github Pager为我们生成的外链（例如我的是：ryanlijianchang.github.io/，而你的只需要把你的github用户名替换掉这个链接中的ryanlijianchang，因为我的用户名是这个，那么你自己的专属博客地址就是：https://[您的用户名].github.io/）即可看到自己的博客了。 当然，每一个人都可以通过这个地址访问到你的博客了。 美化自己博客那么现在我们的博客已经挂在了Github服务器上面，别人已经可以通过地址来登陆我们的博客了，但是我们这时就有了新的需求，就是自己的博客并不好看，那怎么办的？这很简单，要知道很多前端开发者在Hexo框架下开发了很多的主题给我们使用，我们只需要把他们的主题克隆过来，然后通过修改配置文件即可达到我们所需要的效果。 那么我们应该怎么修改呢？ 一、进入Hexo的官网主题专栏 二、挑选我们喜欢的主题可以看到有很多主题给我们选，我们只要选择喜欢的主题点击进去，然后进入到它的github地址，我们只要把这个地址复制下来(例如我是选择：hexo-theme-next这个主题) 三、克隆主题再打开Hexo文件夹下的themes目录（F:\Blog\hexo\themes），右键Git Bash，在命令行输入: 1git clone https://github.com/iissnan/hexo-theme-next(此处地址替换成你需要使用的主题的地址) 下载中，等待下载完成： 四、修改Hexo配置文件下载完成后，打开Hexo文件夹下的配置文件_config.yml修改参数为：theme: hexo-theme-next 五、部署主题，本地查看效果返回Hexo目录，右键Git Bash，输入 12hexo ghexo s 打开浏览器，输入http://localhost:4000/ 即可看见我们的主题已经更换了。 六、如果效果满意，将它部署到Github上打开Hexo文件夹，右键Git Bash，输入 12hexo clean (必须要，不然有时因为缓存问题，服务器更新不了主题)hexo g -d 七、打开自己的主页，即可看到修改后的效果更多修改效果请查看对应主题的说明文档，点击此查看本主题(Next)对应的说明文档。 在博客写文章一、用hexo发表新文章1$ hexo n "文章标题" 其中 我的家 为文章标题，执行命令hexo n“我的家” 后，会在项目\Hexo\source_posts中生成我的家.md文件，用编辑器打开编写即可。 当然，也可以直接在\Hexo\source_posts中新建一个md文件，我就是这么做的。写完后，推送到服务器上，执行以下命令即可在我们的站点看到新的文章。 12$ hexo g #生成$ hexo d #部署 # 可与hexo g合并为 hexo d -g 二、用Markdown写文章我们注意到在 \Hexo\source_posts 文件夹下存放着我们的文章，它们的格式都是以.md格式结尾的，没错，Hexo也是支持Markdown语法的，所以当我们需要写具有格式化的文章时，我们可以使用支持Markdown语法的编辑器进行文章编译，然后保存文件到 \Hexo\source_posts 文件夹下即可。 复制进去之后，只要执行 1$ hexo d -g 推送到我们的Github仓库即可。 那么什么是Markdown？ Markdown 是一种轻量级的「标记语言」，它的优点很多，目前也被越来越多的写作爱好者，撰稿者广泛使用。看到这里请不要被「标记」、「语言」所迷惑，Markdown 的语法十分简单。常用的标记符号也不超过十个，这种相对于更为复杂的HTML 标记语言来说，Markdown 可谓是十分轻量的，学习成本也不需要太多，且一旦熟悉这种语法规则，会有一劳永逸的效果。 Markdown有什么优点？ 专注你的文字内容而不是排版样式。 轻松的导出 HTML、PDF 和本身的 .md 文件。 纯文本内容，兼容所有的文本编辑器与字处理软件。 可读，直观。适合所有人的写作语言。 我该用什么工具？ Windows下可以使用 MarkdownPad2。 在 Mac OS X 上，我建议你用 Mou 这款免费且十分好用的 Markdown 编辑器。 Web 端上，我强烈推荐 简书 这款产品。 关于Markdown的更多资料可以查看如下： 认识与入门 Markdown Markdown入门指南 将自己的域名关联到Github Pages上很多朋友创建了自己的博客之后会选择买一个属于自己的域名，然后将自己域名绑定到自己的Github Pages博客上，其实这也并不难，只要你有个域名。 一、购买域名如果你不是很有钱，在阿里云上，你只要几块钱就可以买到一个域名。 选择你喜欢的域名，然后购买即可 二、配置CNAME文件在 \hexo\source 文件夹下创建文件 CNAME （新建记事本文件命名CNAME，然后打开） 内容为你的域名，例如我的域名是：ryane.top 在Hexo文件夹提交 1hexo g -d 三、修改DNS 如果你是在阿里云购买域名的话，请登录阿里云网站。打开个人中心，点击域名 选择管理 修改DNS为12f1g1ns2.dnspod.net f1g1ns1.dnspod.net 四、域名解析 打开DNSPOD，注册一个账户 点击添加域名，把你的域名添加进去，如无意外，添加完之后就是以下这个状态 此时点击添加记录，添加两个记录，一个主机记录为@， 一个为www，而记录值都是填同一个，填你的博客主页对应的ip，添加完后如下。 但是如何获取ip值呢？打开运行，输入cmd，打开命令窗口输入 ping 主页地址 ， 红色部分即为你的ip值 将IP输入过去，然后会提示你到域名注册的地方修改DNS。等待生效，最迟72小时生效。即可通过你的域名浏览你的博客主页。 结语当你完成了你的博客之后，相信你的心情跟我刚做完的心情是一样的，即便很累，但是当自己的博客成型之后，自己还是有一个成就感的，那么完成后，以后的路还很长，真正想要自己博客能够积累人气，还得靠好的内容，所以认真写文章吧，相信你会受益于此的。 如果大家有什么问题的话，可以在我留言板下留言，我看到评论时会第一时间回答大家。 参考资料： Markdown入门手册中文版 Markdown–入门指南 如何搭建一个独立博客——简明 Github Pages与 jekyll 教程 by cnfeat Hexo搭建Github静态博客 by 金石开 使用SSH密钥连接Github【图文教程】 by 轩枫 搭建自己的博客 Github: 参考视频《Hexo博客+Next主题教程》(第二版)-建站]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Markdown</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
